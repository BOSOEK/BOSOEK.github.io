{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/generate/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>Decoding Strategy (디코딩 전략)</h1>\n<p>이번 포스팅에서는 자연어처리 모델의 디코딩 전략에 관해서 다뤄보려고 합니다. 디코딩이란 말처럼 디코딩은 디코더에서\n수행하는 작업입니다. 즉, BERT와 같은 인코더 모델에서 사용하는게 아니라 GPT와 같은 디코더 모델 혹은 인코더-디코더를 모두\n가지고 있는 Seq2seq 모델의 디코더에서 수행됩니다. 같은 모델이더라도 이 디코딩 전략을 어떻게 가져가냐에 따라서\n디코딩의 퀄리티와 수행 시간들이 천차만별이므로, 자연어처리를 공부하시는 분들이라면 한 번은 꼭 짚고 넘어가야 하는 개념입니다.</p>\n<h2>Greedy Search</h2>\n<p>가장 기본적인 디코딩 전략입니다. 단순하게 매 타임스텝마다 가장 높은 확률을 가지는 토큰을 다음 토큰으로 선택하는 전략입니다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/149542712-7c3db121-5bb1-4a26-bfd2-9e32135346ec.png\" width=\"400\">\n<p>Greedy Search는 가장 기본적이면서도 직관적입니다. 시간복잡도 면에서는 훌륭한 방법이지만, 최종 정확도 관점에서는 꽤나\n아쉬운 방법입니다. 특정 시점 t에서 확률 분포 상에서 1등과 2등의 확률 차이가 매우 작더라도 Greedy Search는 1등만 선택할 뿐입니다.\n여기서 문제점은 디코딩이라는 과정은 특정 t 시점에서 끝나는게 아니라, 시퀀스 길이 N만큼의 시점 t가 있다는 점입니다.\n단 한 번이라도 정답 토큰이 아닌 다른 토큰으로 예측하게 되면 뒤의 디코딩에도 영향을 미치기 때문에 정확도 면에서는 많이 아쉬운 전략입니다.</p>\n<h2>Beam Search</h2>\n<p>Greedy Search 방법에서 시간복잡도를 조금 포기하고 정확도를 높이기 위해 제안된 방법입니다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/149543597-d3d4c079-7938-468b-aae9-ef9774ac4d6c.png\" width=\"400\">\n<p>가장 좋은 디코딩 방법은 가능한 모든 경우의 수를 고려해서 누적 확률이 가장 높은 경우를 선택하는 것이겠지만 이는 시간복잡도 면에서\n사실상 불가능한 방법입니다. 빔서치는 이러한 Greedy Search와 모든 경우의 수를 고려하는 방법의 타협점입니다. 해당 시점에서\n유망하다고 판단되는 빔 K개를 골라서 진행하는 방식입다. Greedy Search가 놓칠 수 있는 시퀀스를 찾을 수 있다는 장점이 있지만,\n시간복잡도 면에서는 더 느리다는 단점도 가지고 있습니다. 또한 빔 개수 K를 몇 으로 설정하냐에 따라서도 결과와 수행시간이 달라지기 때문에\n적절한 K를 찾아야 합니다.</p>\n<h2>N-gram Penalty</h2>\n<p>언어모델의 고질적인 문제점 중 하나는 동일한 말을 계속 반복한다는 것입니다. 이러한 현상을 줄여주기 위해 n-gram 패널티를 줄 수 있습니다.\nn-gram 단위의 시퀀스가 두 번 이상 등장할 일이 없도록 확률을 0으로 만드는 전략입니다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/149546126-59292e15-cfe9-4ed4-855c-f0a0f54e4fa0.png\" width=\"450\">\n<p>이 전략을 사용하게 되면 동일한 말을 반복하는 현상을 줄일 수 있지만, n-gram으로 설정한 시퀀스가 두 번 이상 등장할 수 없기 때문에\n주의해서 사용해야 합니다.</p>\n<h2>Beam search is boring !!</h2>\n<p>최근 연구 결과들에서 빔서치의 단점이 부각되고 있습니다. 기계번역이나 요약 같이 어느 정도 정답이 정해져 있는 태스크에서는\n빔서치가 효과적이지만, dialogue/story generation과 같은 open-ended generation 태스크에서는 적절치 않다는 연구 결과들이 있습니다.</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1808.10006\">Correcting Length Bias in Neural Machine Translation</a></li>\n<li><a href=\"https://arxiv.org/abs/1808.09582\">Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation</a></li>\n</ul>\n<p>그리고 빔서치의 가장 큰 문제점은 less-surprising 하다는 것입니다. 빔서치는 K개의 빔에서 가장 높은 확률을 가지는 문장을\n선택하게 되는데, 결과를 보게 되면 대체적으로 가장 뻔하게 예측이 되는 문장으로 생성됩니다. 이는 대화/스토리 생성와 같은\nopen ended 태스크에서는 치명적인 단점입니다. 예측 가능하고 뻔한 문장이 생성된다는 것은 재미라는 점에서 많은 마이너스 포인트를 가져가게 됩니다.\n아래 그래프가 이러한 경향성에 대해서 아주 잘 보여주고 있습니다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/149557583-d636cdf1-5711-4fcb-bb2c-9ae5e844e6b1.png\" width=\"500\">\n<p>그래서 이러한 <strong>지루한</strong> 디코딩을 조금이나마 재밌게 만들기 위해서는 어느 정도의 <strong>랜덤성</strong>이 추가되면 개선이 될 수도 있습니다.</p>\n<h2>Sampling</h2>\n<p>디코딩 방법에 랜덤성을 추가하는 대표적인 디코딩 전략입니다. 이를 non-deterministic 하다고 표현하는데, 방법은 간단합니다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/149557912-9a6d52fb-3829-4606-983d-39d067c42757.png\" width=\"400\">\n<p>위의 그림을 예로 설명하겠습니다. 네모 상자 위에 적힌 숫자는 각 토큰의 해당 시점 t의 확률입니다.\nnice는 0.5, dog은 0.4, car는 0.1 입니다. greedy search라면 바로 nice를 선택하고 이어나가겠지만 sampling은\n이 확률을 그대로 선택될 확률로 사용합니다. 즉, nice라는 토큰이 선택될 확률을 0.5로 줌으로써 다른 토큰들(dog, car)이\n선택될 수 있는 랜덤성을 부여하는 방법입니다.</p>\n<h2>Top-k Sampling</h2>\n<p>Top-k Sampling은 Sampling 방법을 약간 개조한 전략입니다. 다음 토큰 선택시, 확률이 높은 K개의 토큰들만으로 한정해서 Sampling을 진행하는 방식입니다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/149559141-aca96ef5-58a0-4a7b-979d-3972ca234bd9.png\" width=\"500\">\n<p>GPT-2 논문에서 이 전략으로 스토리 생성에서 큰 효과를 봤으나, 이 방법은 모델의 창의성을 저하할 수 있다는 단점을 가지고 있습니다.</p>\n<h2>Top-p Sampling (Nucleus Sampling)</h2>\n<p>Top-k Sampling의 문제점을 개선하기 위해 제안된 방법으로, 확률이 높은 K개의 토큰으로부터 샘플링을 하지만,\n<strong>누적 확률이 p 이상이 되는 최소한의 집합</strong>으로부터 샘플링을 하게 하는 전략입니다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/149561365-545447c8-bed5-4495-ba99-d4e5e85d801a.png\" width=\"500\">\n<p>이론 상으로는 top-p 샘플링이 top-k보다 좋아보이지만, 항상 그렇듯 경우에 따라 더 좋을 때도 아닐때도 있습니다.\n두 전략 모두 꽤 잘 작동하므로 두 전략 모두 사용해보면서 결과를 비교해보는게 가장 좋습니다.</p>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Decoding Strategy (디코딩 전략)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이번 포스팅에서는 자연어처리 모델의 디코딩 전략에 관해서 다뤄보려고 합니다. 디코딩이란 말처럼 디코딩은 디코더에서\n수행하는 작업입니다. 즉, BERT와 같은 인코더 모델에서 사용하는게 아니라 GPT와 같은 디코더 모델 혹은 인코더-디코더를 모두\n가지고 있는 Seq2seq 모델의 디코더에서 수행됩니다. 같은 모델이더라도 이 디코딩 전략을 어떻게 가져가냐에 따라서\n디코딩의 퀄리티와 수행 시간들이 천차만별이므로, 자연어처리를 공부하시는 분들이라면 한 번은 꼭 짚고 넘어가야 하는 개념입니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Greedy Search"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"가장 기본적인 디코딩 전략입니다. 단순하게 매 타임스텝마다 가장 높은 확률을 가지는 토큰을 다음 토큰으로 선택하는 전략입니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/149542712-7c3db121-5bb1-4a26-bfd2-9e32135346ec.png","width":400},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Greedy Search는 가장 기본적이면서도 직관적입니다. 시간복잡도 면에서는 훌륭한 방법이지만, 최종 정확도 관점에서는 꽤나\n아쉬운 방법입니다. 특정 시점 t에서 확률 분포 상에서 1등과 2등의 확률 차이가 매우 작더라도 Greedy Search는 1등만 선택할 뿐입니다.\n여기서 문제점은 디코딩이라는 과정은 특정 t 시점에서 끝나는게 아니라, 시퀀스 길이 N만큼의 시점 t가 있다는 점입니다.\n단 한 번이라도 정답 토큰이 아닌 다른 토큰으로 예측하게 되면 뒤의 디코딩에도 영향을 미치기 때문에 정확도 면에서는 많이 아쉬운 전략입니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Beam Search"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Greedy Search 방법에서 시간복잡도를 조금 포기하고 정확도를 높이기 위해 제안된 방법입니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/149543597-d3d4c079-7938-468b-aae9-ef9774ac4d6c.png","width":400},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"가장 좋은 디코딩 방법은 가능한 모든 경우의 수를 고려해서 누적 확률이 가장 높은 경우를 선택하는 것이겠지만 이는 시간복잡도 면에서\n사실상 불가능한 방법입니다. 빔서치는 이러한 Greedy Search와 모든 경우의 수를 고려하는 방법의 타협점입니다. 해당 시점에서\n유망하다고 판단되는 빔 K개를 골라서 진행하는 방식입다. Greedy Search가 놓칠 수 있는 시퀀스를 찾을 수 있다는 장점이 있지만,\n시간복잡도 면에서는 더 느리다는 단점도 가지고 있습니다. 또한 빔 개수 K를 몇 으로 설정하냐에 따라서도 결과와 수행시간이 달라지기 때문에\n적절한 K를 찾아야 합니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"N-gram Penalty"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"언어모델의 고질적인 문제점 중 하나는 동일한 말을 계속 반복한다는 것입니다. 이러한 현상을 줄여주기 위해 n-gram 패널티를 줄 수 있습니다.\nn-gram 단위의 시퀀스가 두 번 이상 등장할 일이 없도록 확률을 0으로 만드는 전략입니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/149546126-59292e15-cfe9-4ed4-855c-f0a0f54e4fa0.png","width":450},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이 전략을 사용하게 되면 동일한 말을 반복하는 현상을 줄일 수 있지만, n-gram으로 설정한 시퀀스가 두 번 이상 등장할 수 없기 때문에\n주의해서 사용해야 합니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Beam search is boring !!"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"최근 연구 결과들에서 빔서치의 단점이 부각되고 있습니다. 기계번역이나 요약 같이 어느 정도 정답이 정해져 있는 태스크에서는\n빔서치가 효과적이지만, dialogue/story generation과 같은 open-ended generation 태스크에서는 적절치 않다는 연구 결과들이 있습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1808.10006"},"children":[{"type":"text","value":"Correcting Length Bias in Neural Machine Translation"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1808.09582"},"children":[{"type":"text","value":"Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation"}]}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 빔서치의 가장 큰 문제점은 less-surprising 하다는 것입니다. 빔서치는 K개의 빔에서 가장 높은 확률을 가지는 문장을\n선택하게 되는데, 결과를 보게 되면 대체적으로 가장 뻔하게 예측이 되는 문장으로 생성됩니다. 이는 대화/스토리 생성와 같은\nopen ended 태스크에서는 치명적인 단점입니다. 예측 가능하고 뻔한 문장이 생성된다는 것은 재미라는 점에서 많은 마이너스 포인트를 가져가게 됩니다.\n아래 그래프가 이러한 경향성에 대해서 아주 잘 보여주고 있습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/149557583-d636cdf1-5711-4fcb-bb2c-9ae5e844e6b1.png","width":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그래서 이러한 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"지루한"}]},{"type":"text","value":" 디코딩을 조금이나마 재밌게 만들기 위해서는 어느 정도의 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"랜덤성"}]},{"type":"text","value":"이 추가되면 개선이 될 수도 있습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Sampling"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"디코딩 방법에 랜덤성을 추가하는 대표적인 디코딩 전략입니다. 이를 non-deterministic 하다고 표현하는데, 방법은 간단합니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/149557912-9a6d52fb-3829-4606-983d-39d067c42757.png","width":400},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 그림을 예로 설명하겠습니다. 네모 상자 위에 적힌 숫자는 각 토큰의 해당 시점 t의 확률입니다.\nnice는 0.5, dog은 0.4, car는 0.1 입니다. greedy search라면 바로 nice를 선택하고 이어나가겠지만 sampling은\n이 확률을 그대로 선택될 확률로 사용합니다. 즉, nice라는 토큰이 선택될 확률을 0.5로 줌으로써 다른 토큰들(dog, car)이\n선택될 수 있는 랜덤성을 부여하는 방법입니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Top-k Sampling"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Top-k Sampling은 Sampling 방법을 약간 개조한 전략입니다. 다음 토큰 선택시, 확률이 높은 K개의 토큰들만으로 한정해서 Sampling을 진행하는 방식입니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/149559141-aca96ef5-58a0-4a7b-979d-3972ca234bd9.png","width":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"GPT-2 논문에서 이 전략으로 스토리 생성에서 큰 효과를 봤으나, 이 방법은 모델의 창의성을 저하할 수 있다는 단점을 가지고 있습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Top-p Sampling (Nucleus Sampling)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Top-k Sampling의 문제점을 개선하기 위해 제안된 방법으로, 확률이 높은 K개의 토큰으로부터 샘플링을 하지만,\n"},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"누적 확률이 p 이상이 되는 최소한의 집합"}]},{"type":"text","value":"으로부터 샘플링을 하게 하는 전략입니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/149561365-545447c8-bed5-4495-ba99-d4e5e85d801a.png","width":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이론 상으로는 top-p 샘플링이 top-k보다 좋아보이지만, 항상 그렇듯 경우에 따라 더 좋을 때도 아닐때도 있습니다.\n두 전략 모두 꽤 잘 작동하므로 두 전략 모두 사용해보면서 결과를 비교해보는게 가장 좋습니다."}]}],"data":{"quirksMode":false}},"excerpt":"Decoding Strategy (디코딩 전략) 이번 포스팅에서는 자연어처리 모델의 디코딩 전략에 관해서 다뤄보려고 합니다. 디코딩이란 말처럼 디코딩은 디코더에서\n수행하는 작업입니다. 즉, BERT와 같은 인코더 모델에서 사용하는게 아니라 GPT…","fields":{"readingTime":{"text":"9 min read"}},"frontmatter":{"title":"Decoding Strategy (디코딩 전략)","userDate":"15 January 2022","date":"2022-01-15T10:00:00.000Z","tags":["nlp"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/43d0a11cfbaf2b0bf2571f5678e1ced5/68f14/decoding.png","srcSet":"/static/43d0a11cfbaf2b0bf2571f5678e1ced5/d0df2/decoding.png 750w,\n/static/43d0a11cfbaf2b0bf2571f5678e1ced5/fa96f/decoding.png 1080w,\n/static/43d0a11cfbaf2b0bf2571f5678e1ced5/ad399/decoding.png 1366w,\n/static/43d0a11cfbaf2b0bf2571f5678e1ced5/68f14/decoding.png 1400w","sizes":"100vw"},"sources":[{"srcSet":"/static/43d0a11cfbaf2b0bf2571f5678e1ced5/b9516/decoding.webp 750w,\n/static/43d0a11cfbaf2b0bf2571f5678e1ced5/c4814/decoding.webp 1080w,\n/static/43d0a11cfbaf2b0bf2571f5678e1ced5/2a401/decoding.webp 1366w,\n/static/43d0a11cfbaf2b0bf2571f5678e1ced5/db739/decoding.webp 1400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5614285714285715}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}]}}]}},"relatedPosts":{"totalCount":30,"edges":[{"node":{"id":"06ac0e32-0688-50f0-810d-134ef8b168ab","excerpt":"Decoding Strategy (디코딩 전략) 이번 포스팅에서는 자연어처리 모델의 디코딩 전략에 관해서 다뤄보려고 합니다. 디코딩이란 말처럼 디코딩은 디코더에서\n수행하는 작업입니다. 즉, BERT와 같은 인코더 모델에서 사용하는게 아니라 GPT…","frontmatter":{"title":"Decoding Strategy (디코딩 전략)","date":"2022-01-15T10:00:00.000Z"},"fields":{"readingTime":{"text":"9 min read"},"slug":"/generate/"}}},{"node":{"id":"db36f120-4fb0-5bf7-af53-16447fe6cdd4","excerpt":"Generation with Retrieval 이번에 딥마인드에서 RETRO(Retrieval-Enhanced Transformer) 라는 모델을 내놓았습니다. 문서 retrieval + GPT 기반 모델인데,\n7B 모델임에도 불구하고 2…","frontmatter":{"title":"Generation with Retrieval","date":"2022-01-04T23:00:00.000Z"},"fields":{"readingTime":{"text":"6 min read"},"slug":"/fid_and_rag/"}}},{"node":{"id":"3b4040eb-d53d-5064-beec-cfbf7a7a0fe2","excerpt":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems Paper Review Paper: https://aclanthology.org/2021.naacl-main.12…","frontmatter":{"title":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems Paper Review","date":"2021-12-18T10:00:00.000Z"},"fields":{"readingTime":{"text":"2 min read"},"slug":"/bert_fp/"}}},{"node":{"id":"78976688-33d9-53c4-8489-5099082b9972","excerpt":"GPT (Generative Pre-trained Transformer) 1 gpt1 먼저 알아보고, gpt2에 대해 알아보겠습니다. GPT1 Improving Language Understanding by Generative Pre-Training…","frontmatter":{"title":"GPT (Generative Pre-trained Transformer)","date":"2021-11-23T11:00:00.000Z"},"fields":{"readingTime":{"text":"13 min read"},"slug":"/gpt/"}}},{"node":{"id":"ad5b0c9b-8199-5f10-bfc9-6bb05942e164","excerpt":"Large Scale LM (2) Distributed Programming (작성중) 이 자료는 [해당 link…","frontmatter":{"title":"Large Scale LM (2) Distributed Programming","date":"2021-11-22T11:00:00.000Z"},"fields":{"readingTime":{"text":"17 min read"},"slug":"/big-model2/"}}}]}},"pageContext":{"slug":"/generate/","prev":{"excerpt":"…","frontmatter":{"title":"광주소프트웨어마이스터고등학교 학생들 튜닙 방문","tags":["record"],"date":"2022-01-11T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAADV0lEQVQozwXB+1MSBgDAcf4ImznAO08ElSQeSaWmQiL4mKIlPkBeXoqiGZgkig90KiJb4pDh47THmrvyct3p1g+V7TaX2utqt6u71e1uXX/Hd5+PoLjKT1PHKLbucazeSSouDDHx/U/8+d8HXv/7jpWfd+m5tsDz9294/vkjR58+EIzdIJK4zdE/f/Hy00c2H+/R6plh//1bBOdMPhQaB5VFdranBhho7iYVHGInMY3FMoLTPsbt6Wlmg1E6e2N0XP4WW1cEu2eW+GSMK75ZPKE43pEFlnZ2EZjMAygUNpwGJ2/Xx3k4F2R/KcR+coxui5/JvhEONyLcjYSZ8IXx9X/NQCBCeDTC/WSMxdk5xuaXCMVS/PLsNwSV9VdRKq3Yde08jQ6ycaWX3x8EuP8kyaXNH0jF47y6t8TrnTU2V5dJJFIEgzMsRud5uH6dGwsxoksrBGdTPPv7JQJjwyAqlRXz2TZWer180z9C8qafy49u4rq+RGI4xHZsile7t5gLTDEf/Y6e7hDD/lFWx8OkojHW7twiFEmx9+YQQVXjINpTdso1behPWzEb3KhLrNTZfCT9wyz6rrIamWHU3c92IkKgb5SQb4yN+AIed4CKyn4m5peZii3z6MUfCGqaApzWtFOqbqbibDtp6dVkHKsiW1iDTN6CtqiT0vN9aKX1hIfGcDf20GDoxGv1I1e5OC5xoKsbJhxN8uvhUwSmpmuolVZys74iJ7uBPFkz4gwT8jwLIlE1SpUNdbGHUzIzpSVuCgraEIsbUCkdnNR2kX3Chfa8j3B8jScHewhsziFycsxki6uQSRvJz2tGKjIiFNYgTDegKfZQWNZHoaweicyCUuXgy4xaCks8qApdZEpaKDP5iK7d4e7sNIIJey9+XR1urYlc2QUkUgsnsoxkCo18cUxHvsbFSfkF1LJahFIrnRc96LWtFOl7KD1jQ5Jrxeed4MXjH9lZiSFY7XKx5ahnprqKrKw6JJJGzuRWoyswIjpeTr6ijbQ0HeJ0HfmKVrqaOqk1Oak0dWE2OpEqXNicYxw8WObdwRaCe816thrLSNUbEIlrKM+rxqoyopfoyc/Uo5Y3IRcZqDhno7XFy8VKB9WlVkxlDi61dKLUOOhwD3G0HuLz/gL/Azr0GHEpMjtZAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/0236317ab6d0ade50e943efd96ed64ee/e38af/soma.png","srcSet":"/static/0236317ab6d0ade50e943efd96ed64ee/e9604/soma.png 750w,\n/static/0236317ab6d0ade50e943efd96ed64ee/e38af/soma.png 847w","sizes":"100vw"},"sources":[{"srcSet":"/static/0236317ab6d0ade50e943efd96ed64ee/64486/soma.webp 750w,\n/static/0236317ab6d0ade50e943efd96ed64ee/ed812/soma.webp 847w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.6068476977567887}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"2 min read"},"layout":"","slug":"/soma/"}},"next":null,"primaryTag":"nlp"}},
    "staticQueryHashes": ["3170763342","3229353822"]}