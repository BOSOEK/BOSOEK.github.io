{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/luna/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>Luna: Linear Unified Nested Attention</h1>\n<ul>\n<li>USC + CMU + Facebook AI</li>\n<li>2021.06</li>\n<li><a href=\"https://github.com/XuezheMax/fairseq-apollo\">code</a></li>\n</ul>\n<h2>Abstract</h2>\n<ul>\n<li>\n<p>트랜스포머의 Multi Headed Self Attention은 시퀀스가 길어질수록 메모리, 시간복잡도가 quadratic함</p>\n</li>\n<li>\n<p>이를 linear하게 바꾸기 위한 시도로 Luna (Linear Unified Nested Attention) 을 제안</p>\n</li>\n<li>\n<p>핵심은 인풋을 고정 길이로 변환한다는 점과 attention을 2개로 분리한다는 점.</p>\n</li>\n<li>\n<p>Positional Embedding을 별도의 고정 길이 query로 뺌.</p>\n</li>\n<li>\n<p>속도는 Performer랑 비슷한데 시퀀스가 길어질수록 좀 더 효과적이고 메모리도 적게 쓴다고 함.</p>\n</li>\n<li>\n<p>정확도 성능은 경쟁력이 높은 편.</p>\n</li>\n</ul>\n<h2>Attention</h2>\n<h3>Traditional attention mechanism:</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/127510622-5af08d8d-771c-4bb9-8e9e-53bb3f4cf85e.png\" height=\"70\">\n<ul>\n<li>X: Query sequence, C: Context sequence</li>\n<li>ω: activation function (usually softmax)</li>\n<li>Q = XW<sub>Q</sub>, K = CW<sub>K</sub>, V = CW<sub>V</sub></li>\n<li>Self attention에서는 X == C</li>\n<li>공간 &#x26; 시간복잡도: O(nm) (n: X의 시퀀스 길이, m: C의 시퀀스 길이)</li>\n</ul>\n<h2>Linear Unified Nested Attention (LUNA)</h2>\n<img src=\"https://user-images.githubusercontent.com/42150335/127514004-0ca02332-8ef5-4563-b9b4-bd9bf6bb72b9.png\" height=\"400\">\n<ul>\n<li>Goal: Attention mechanism’s complexity <strong>quadratic => linear</strong></li>\n</ul>\n<h3>Luna (Pack and Unpack Attention)</h3>\n<ul>\n<li>이 어텐션의 핵심은 어텐션을 2개로 쪼개는 것.</li>\n<li>O(ln) (l은 P의 길이)</li>\n<li>Pack Attention:\n<ul>\n<li>Query를 learnable paramter인 P로 대체 (고정 길이, 길이에 대한 실험 있음)</li>\n<li>상대적으로 더 짧은 Query를 놓음으로써 complexity를 줄임</li>\n<li>P-contextual: 첫 레이어의 P는 learnable parameter이며 다음 레이어로 전달</li>\n<li>P-non-contextual: 각 레이어마다 P를 학습하고 다음 레이어로 넘기지 않음</li>\n<li>contextual &#x26; non-contextual에 대한 실험은 뒤에 있음</li>\n</ul>\n</li>\n</ul>\n<img src=\"https://user-images.githubusercontent.com/42150335/127603299-44234ff8-1735-4dc2-95b0-bc8f66bfbbde.png\" height=\"60\">\n<ul>\n<li>Unpack Attention:\n<ul>\n<li>Pack Attention의 결과인 <code class=\"language-text\">packed context</code>를 Key와 Value로 사용. Query는 기존 query.</li>\n</ul>\n</li>\n</ul>\n<img src=\"https://user-images.githubusercontent.com/42150335/127603352-dfb741df-ec40-4b49-8bdf-b158e55b771e.png\" height=\"70\">\n<ul>\n<li>Luna Attention:</li>\n</ul>\n<img src=\"https://user-images.githubusercontent.com/42150335/127603409-554fa551-e89d-46ee-aec4-9a471a5cdc8f.png\" height=\"70\">\n<ul>\n<li>Luna Layer</li>\n</ul>\n<img src=\"https://user-images.githubusercontent.com/42150335/127603246-192380b0-fde6-4941-b060-1bf639d3b8e7.png\" height=\"100\">\n<h2>Discussion</h2>\n<h3>Relation to Linformer</h3>\n<ul>\n<li>Linformer와 비슷한 포지션. 그럼 뭐가 더 나은가?\n<ul>\n<li>Linformer는 인풋 시퀀스가 모두 고정 길이를 가져야하는데, Luna는 various length가 가능 (projection matrix 때문에 linformer는 길이가 고정이여야함)</li>\n<li>결정적으로 Linformer보다 성능이 좋음</li>\n</ul>\n</li>\n</ul>\n<h2>Experiment</h2>\n<h3>Long-Context Sequence Modeling</h3>\n<h4><strong>Score</strong></h4>\n<img src=\"https://user-images.githubusercontent.com/42150335/127603947-40c6b9f0-63d7-475c-8b6d-cefdfbe5ab59.png\" height=\"500\">\n<h4><strong>Training Speed &#x26; Memory</strong></h4>\n<img src=\"https://user-images.githubusercontent.com/42150335/127604072-79facf8c-7f84-4e9d-bd1e-38b3322458d0.png\" height=\"500\">\n<ul>\n<li>인풋 길이가 길어지면 Luna가 Linformer보다 더 빠름</li>\n<li>메모리 사용량에서 Luna가 Linformer를 포함한 다른 모델들보다 경쟁력이 있음</li>\n</ul>\n<h3>NLU Task (Masked Language Modeling for Large-Scale Pretraining)</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/127604344-7074edf7-ede2-4140-a6e8-320ed711d5f3.png\" height=\"300\">\n<ul>\n<li>BERT 방식으로 pre-training 후 파인튜닝 했을 때 성능 비교</li>\n<li>RoBERTa와도 비견될만큼 좋은 성능을 보임</li>\n</ul>\n<h3>Machine Translation</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/127604608-822e1fd6-00a1-472b-801e-24cda16efa5f.png\" height=\"250\">\n<h3>Abbrebiation Study (contextual &#x3C;-> non-contextual)</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/127604743-ae8289cc-d420-41b0-9ff1-5604ef770b4c.png\" height=\"150\">\n<ul>\n<li>P를 각 레이어의 파라미터로 둘지, 위층으로 넘김으로써 context 정보를 넘겨줄지에 대한 실험</li>\n<li>결론: 다음 층으로 넘겨주는게 좋더라.</li>\n</ul>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Luna: Linear Unified Nested Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"USC + CMU + Facebook AI"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"2021.06"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://github.com/XuezheMax/fairseq-apollo"},"children":[{"type":"text","value":"code"}]}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Abstract"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"트랜스포머의 Multi Headed Self Attention은 시퀀스가 길어질수록 메모리, 시간복잡도가 quadratic함"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이를 linear하게 바꾸기 위한 시도로 Luna (Linear Unified Nested Attention) 을 제안"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"핵심은 인풋을 고정 길이로 변환한다는 점과 attention을 2개로 분리한다는 점."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Positional Embedding을 별도의 고정 길이 query로 뺌."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"속도는 Performer랑 비슷한데 시퀀스가 길어질수록 좀 더 효과적이고 메모리도 적게 쓴다고 함."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"정확도 성능은 경쟁력이 높은 편."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Traditional attention mechanism:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127510622-5af08d8d-771c-4bb9-8e9e-53bb3f4cf85e.png","height":70},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"X: Query sequence, C: Context sequence"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"ω: activation function (usually softmax)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Q = XW"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"Q"}]},{"type":"text","value":", K = CW"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"K"}]},{"type":"text","value":", V = CW"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"V"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Self attention에서는 X == C"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"공간 & 시간복잡도: O(nm) (n: X의 시퀀스 길이, m: C의 시퀀스 길이)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Linear Unified Nested Attention (LUNA)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127514004-0ca02332-8ef5-4563-b9b4-bd9bf6bb72b9.png","height":400},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Goal: Attention mechanism’s complexity "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"quadratic => linear"}]}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Luna (Pack and Unpack Attention)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이 어텐션의 핵심은 어텐션을 2개로 쪼개는 것."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"O(ln) (l은 P의 길이)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Pack Attention:\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Query를 learnable paramter인 P로 대체 (고정 길이, 길이에 대한 실험 있음)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"상대적으로 더 짧은 Query를 놓음으로써 complexity를 줄임"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"P-contextual: 첫 레이어의 P는 learnable parameter이며 다음 레이어로 전달"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"P-non-contextual: 각 레이어마다 P를 학습하고 다음 레이어로 넘기지 않음"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"contextual & non-contextual에 대한 실험은 뒤에 있음"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127603299-44234ff8-1735-4dc2-95b0-bc8f66bfbbde.png","height":60},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Unpack Attention:\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Pack Attention의 결과인 "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"packed context"}]},{"type":"text","value":"를 Key와 Value로 사용. Query는 기존 query."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127603352-dfb741df-ec40-4b49-8bdf-b158e55b771e.png","height":70},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Luna Attention:"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127603409-554fa551-e89d-46ee-aec4-9a471a5cdc8f.png","height":70},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Luna Layer"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127603246-192380b0-fde6-4941-b060-1bf639d3b8e7.png","height":100},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Discussion"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Relation to Linformer"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Linformer와 비슷한 포지션. 그럼 뭐가 더 나은가?\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Linformer는 인풋 시퀀스가 모두 고정 길이를 가져야하는데, Luna는 various length가 가능 (projection matrix 때문에 linformer는 길이가 고정이여야함)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"결정적으로 Linformer보다 성능이 좋음"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Experiment"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Long-Context Sequence Modeling"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h4","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Score"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127603947-40c6b9f0-63d7-475c-8b6d-cefdfbe5ab59.png","height":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h4","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Training Speed & Memory"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127604072-79facf8c-7f84-4e9d-bd1e-38b3322458d0.png","height":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"인풋 길이가 길어지면 Luna가 Linformer보다 더 빠름"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"메모리 사용량에서 Luna가 Linformer를 포함한 다른 모델들보다 경쟁력이 있음"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"NLU Task (Masked Language Modeling for Large-Scale Pretraining)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127604344-7074edf7-ede2-4140-a6e8-320ed711d5f3.png","height":300},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"BERT 방식으로 pre-training 후 파인튜닝 했을 때 성능 비교"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"RoBERTa와도 비견될만큼 좋은 성능을 보임"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Machine Translation"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127604608-822e1fd6-00a1-472b-801e-24cda16efa5f.png","height":250},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Abbrebiation Study (contextual <-> non-contextual)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127604743-ae8289cc-d420-41b0-9ff1-5604ef770b4c.png","height":150},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"P를 각 레이어의 파라미터로 둘지, 위층으로 넘김으로써 context 정보를 넘겨줄지에 대한 실험"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"결론: 다음 층으로 넘겨주는게 좋더라."}]},{"type":"text","value":"\n"}]}],"data":{"quirksMode":false}},"excerpt":"Luna: Linear Unified Nested Attention USC + CMU + Facebook AI 2021.06 code Abstract 트랜스포머의 Multi Headed Self Attention…","fields":{"readingTime":{"text":"4 min read"}},"frontmatter":{"title":"Luna: Linear Unified Nested Attention","userDate":"3 July 2021","date":"2021-07-03T23:46:37.121Z","tags":["nlp","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/540ac0a1a4457065fef9e262f9962c83/af6c5/luna.png","srcSet":"/static/540ac0a1a4457065fef9e262f9962c83/a5b02/luna.png 750w,\n/static/540ac0a1a4457065fef9e262f9962c83/ace3b/luna.png 1080w,\n/static/540ac0a1a4457065fef9e262f9962c83/58836/luna.png 1366w,\n/static/540ac0a1a4457065fef9e262f9962c83/af6c5/luna.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/540ac0a1a4457065fef9e262f9962c83/6f4ad/luna.webp 750w,\n/static/540ac0a1a4457065fef9e262f9962c83/9ec3b/luna.webp 1080w,\n/static/540ac0a1a4457065fef9e262f9962c83/51e4d/luna.webp 1366w,\n/static/540ac0a1a4457065fef9e262f9962c83/3c39a/luna.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5192708333333333}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}]}}]}},"relatedPosts":{"totalCount":30,"edges":[{"node":{"id":"06ac0e32-0688-50f0-810d-134ef8b168ab","excerpt":"Decoding Strategy (디코딩 전략) 이번 포스팅에서는 자연어처리 모델의 디코딩 전략에 관해서 다뤄보려고 합니다. 디코딩이란 말처럼 디코딩은 디코더에서\n수행하는 작업입니다. 즉, BERT와 같은 인코더 모델에서 사용하는게 아니라 GPT…","frontmatter":{"title":"Decoding Strategy (디코딩 전략)","date":"2022-01-15T10:00:00.000Z"},"fields":{"readingTime":{"text":"9 min read"},"slug":"/generate/"}}},{"node":{"id":"db36f120-4fb0-5bf7-af53-16447fe6cdd4","excerpt":"Generation with Retrieval 이번에 딥마인드에서 RETRO(Retrieval-Enhanced Transformer) 라는 모델을 내놓았습니다. 문서 retrieval + GPT 기반 모델인데,\n7B 모델임에도 불구하고 2…","frontmatter":{"title":"Generation with Retrieval","date":"2022-01-04T23:00:00.000Z"},"fields":{"readingTime":{"text":"6 min read"},"slug":"/fid_and_rag/"}}},{"node":{"id":"3b4040eb-d53d-5064-beec-cfbf7a7a0fe2","excerpt":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems Paper Review Paper: https://aclanthology.org/2021.naacl-main.12…","frontmatter":{"title":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems Paper Review","date":"2021-12-18T10:00:00.000Z"},"fields":{"readingTime":{"text":"2 min read"},"slug":"/bert_fp/"}}},{"node":{"id":"78976688-33d9-53c4-8489-5099082b9972","excerpt":"GPT (Generative Pre-trained Transformer) 1 gpt1 먼저 알아보고, gpt2에 대해 알아보겠습니다. GPT1 Improving Language Understanding by Generative Pre-Training…","frontmatter":{"title":"GPT (Generative Pre-trained Transformer)","date":"2021-11-23T11:00:00.000Z"},"fields":{"readingTime":{"text":"13 min read"},"slug":"/gpt/"}}},{"node":{"id":"ad5b0c9b-8199-5f10-bfc9-6bb05942e164","excerpt":"Large Scale LM (2) Distributed Programming (작성중) 이 자료는 [해당 link…","frontmatter":{"title":"Large Scale LM (2) Distributed Programming","date":"2021-11-22T11:00:00.000Z"},"fields":{"readingTime":{"text":"17 min read"},"slug":"/big-model2/"}}}]}},"pageContext":{"slug":"/luna/","prev":{"excerpt":"Ray: multi-processing library…","frontmatter":{"title":"Ray: multi-processing library","tags":["toolkit"],"date":"2021-07-03T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+0lEQVQY042RsUoDURBF7zNqrGysrBSVTTIzSyRvNouBqIWkEwSbxUYE/8AfsPIn7PJWO639HfuwI/kAV0zWkCAYLxyY6nAvA/wkGBAKIK8ItoXcNvFW4haAj2MoNaFCU5gX+J2Z0Nz0tjOEIqs9vqOd9jd8HG8rNVZUqK5Ca8q8rsy1JcKJtI5g1wiWYTh6wMvnTTvpJUrNSy+SeKErFeqp0ECZ3fKGoVhFsCMEu8BwdI/X8qRz2ImUWqkXPvbCpyqsKkTVfKdM/5o8QG7nePoAyhLdaA/pwQ480wyd4w/h5CGukgPPY7eb3SFp7LukFX2LXFf6C2I/N/kLonRoV2yUhgcAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/60ac3ec5a068a489c5ce524ee4097082/5e0b4/ray.png","srcSet":"/static/60ac3ec5a068a489c5ce524ee4097082/87157/ray.png 750w,\n/static/60ac3ec5a068a489c5ce524ee4097082/0107c/ray.png 1080w,\n/static/60ac3ec5a068a489c5ce524ee4097082/92c3b/ray.png 1366w,\n/static/60ac3ec5a068a489c5ce524ee4097082/5e0b4/ray.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/60ac3ec5a068a489c5ce524ee4097082/6ae24/ray.webp 750w,\n/static/60ac3ec5a068a489c5ce524ee4097082/02909/ray.webp 1080w,\n/static/60ac3ec5a068a489c5ce524ee4097082/5981b/ray.webp 1366w,\n/static/60ac3ec5a068a489c5ce524ee4097082/707e4/ray.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.3125}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"3 min read"},"layout":"","slug":"/ray/"}},"next":{"excerpt":"[REVIEW] 크래프톤웨이 (Krafton Way) 배틀그라운드, 테라 등의 게임개발사로 유명한 KRAFTON…","frontmatter":{"title":"[REVIEW] 크래프톤웨이 (Krafton Way)","tags":["book","review"],"date":"2021-07-10T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/6281efa490feec135db893f41708f052/b3219/krafton.png","srcSet":"/static/6281efa490feec135db893f41708f052/b3219/krafton.png 690w","sizes":"100vw"},"sources":[{"srcSet":"/static/6281efa490feec135db893f41708f052/6dfae/krafton.webp 690w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.4840579710144928}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"2 min read"},"layout":"","slug":"/krafton/"}},"primaryTag":"nlp"}},
    "staticQueryHashes": ["3170763342","3229353822"]}