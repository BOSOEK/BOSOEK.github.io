{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/deepspeech/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>Deep Speech: Scaling up end-to-end speech recognition</h1>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006014-e0ea7aa9-fa25-4b31-90fe-c1144cd68d40.png\" alt=\"title\"></p>\n<p><a href=\"https://arxiv.org/pdf/1412.5567.pdf\">https://arxiv.org/pdf/1412.5567.pdf</a> (Awni Hannun et al. 2014)</p>\n<h2>Abstract</h2>\n<p>본 논문은 2014년도에 나온 논문이다. 논문을 읽어본 결과, 당시에는 음성인식에 End-to-End 방식의 딥러닝을 적용한 사례가 없던 모양이다. (또는 주목할만한 성과가 나오지 않았던 모양이다.) 본 논문에서는 End-to-End 방식으로 Switchboard Hub5’00 데이터셋에서 16.0%의 Error Rate를 기록하며 <strong>State-Of-The-Art</strong> (SOTA) 를 달성한 성과를 밝히고 있다. 기존 음성인식의 traditional한 방식은 전처리 과정이 상당히 많이 필요했지만, 이러한 과정 없이 데이터와 레이블만을 이용한 End-to-End 방식으로 이러한 성과를 냈음을 거듭 강조하고 있다. 그리고 이전 방식으로는 노이즈가 있는 환경에서 급격히 떨어졌는데 반하여, 본 논문 방식으로는 노이즈가 있는 환경에서도 좋은 성능을 기록했다고 한다. 즉, 기존 방식보다 더 간단하면서도 좋은 성능을 기록한 End-to-End 방식의 Speech-Recognition 모델을 소개하는 논문이다.</p>\n<h2>Introduction</h2>\n<p>Abstract에서 강조한 내용을 다시 한번 강조한다. 기존 traditional한 방식은 전문가들의 손이 많이 가야했다. (노이즈 필터링 등..) 하지만 그러한 노력에도 불구하고, 실제 노이즈가 낀 상황에서의 인식률은 좋지 못했다. 하지만 End-to-End 방식으로 이러한 2가지의 단점을 개선할 수 있다고 주장한다.</p>\n<p>물론 End-to-End 방식으로 가기 위해서는 몇가지 고려해야할 사항들이 있었지만, 본 논문에서는 기존 연구 결과들을 참고하여 End-to-End 방식을 시도할 수 있었다고 말한다. 그리고, 본 논문에서는 RNN 모델을 사용했다. 뒤에 더 자세히 설명하겠지만 본 논문에서는 학습 시간을 단축시키기 위해 많은 고려를 한 것으로 보인다.</p>\n<h2>RNN Training Setup</h2>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006049-0cdcb969-0383-4722-b675-87b9f3587488.png\" alt=\"model\"></p>\n<p>해당 챕터에서는 자신들이 어떤 식으로 모델을 구성했는지에 대해 설명한다.<br>\n모델의 핵심은 RNN으로 구성되어 있으며, 트레이닝 셋은 ${(x_1, y_1), (x_2, y_2), … (x_t, y_t)}$ 와 같은 딕셔너리 형식으로 구성했다고 한다. (x는 스펙트로그램, y는 문자로 구성 )</p>\n<p>모델은 총 5개의 히든 레이어로 구성했다고 한다.\n논문을 읽으면서 모델 아키텍쳐가 상당히 특이하다고 생각했다.</p>\n<p>현재 내가 알고있는 방식과는 사뭇 다른 방식이였는데, 히든 레이어 중 1, 2, 3번째 레이어는 병렬적 (Parallel) 하게 처리하기 위해 서로 독립적으로 포워딩 된다고 한다. RNN 아키텍쳐를 사용하게 되면 이전 셀의 아웃풋이 필요하기 때문에 어쩔 수 없이 병렬처리의 한계점이 있기 때문에 학습 속도 개선을 위해 각 인풋을 독립적으로 처리했다고 한다.<br>\n여기서도 나는 본 논문이 학습 시간을 단축시키기 위해 상당히 노력했다는 인상을 받았다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006132-c4eef972-3f32-4839-953f-8950ddc08b47.png\" alt=\"forward\"></p>\n<p>위의 수식을 통해 포워딩이 진행되는데, 여기서 g는 최소 0, 최대 20의 값을 가지는 ReLU 함수이다.<br>\n그리고 4번째 레이어에서는 Bidirectional-RNN으로 구성했다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006150-b5ab3ce6-27a3-44fd-9a61-50e96e031071.png\" alt=\"Bi-RNN\"></p>\n<p>(f)는 정방향 (forward), (b)는 역방향 (backward)을 표현한 것이다.<br>\n이때 주의할 점으로는, forward는 t = 1 에서 t = T 방향으로 흐르고, backward는 t = T에서 t = 1 방향으로 흐른다는 점이다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006159-84108cbf-f8d5-47d7-aa57-43400eb37c1d.png\" alt=\"5-layer\"></p>\n<p>그리고 마지막 5번째 레이어는 이렇게 forward, backward의 결과에 웨이트를 주고 1, 2, 3 번째 레이어와 동일한 ReLU를 활성화 함수로 사용했다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006172-ba0e1f77-acc5-4f55-a469-57df1bb95881.png\" alt=\"softmax\"></p>\n<p>그리고 이렇게 나온 결과는 Softmax 함수에 넣어서 최종적으로 Classfication을 진행한다. 또한 loss 계산시에는 CTC loss를 사용했다.</p>\n<p>여기서 또 특이했던 점으로, LSTM이 아닌 기본 RNN을 사용했다는 점이다.<br>\n그 이유로 본 논문에서는 LSTM의 단점은 메모리가 많이 소요되고, 학습이 오래걸린다는 점을 꼽았다.</p>\n<p>총 5개의 히든 레이어 중 실제 RNN 계층은 1개 뿐이라는 점과, LSTM이 아닌 RNN을 사용했다는 점이 인상깊었다.<br>\n최근 연구에서는 기본 RNN을 사용하는 모습을 거의 볼 수 없는데, 당시에는 아직 GPU의 성능이 그리 좋지 않던 때라 그런지 학습 속도에 대해 굉장히 고려를 많이한 모습이 보였다.</p>\n<h3>Regularization</h3>\n<p>본 논문은 학습 시, 드랍아웃 비율을 5 - 10%정도를 유지했다고 한다.<br>\n그리고 Spectrogram의 프레임 길이는 10ms, 포워딩은 5ms를 사용했다.<br>\n(해당 부분은 오류가 있을수도 있습니다)</p>\n<p>통상적으로 음성 인식에서 프레임 길이는 20 - 40ms를 사용하기 때문에 프레임 길이가 상당히 짧다고 생각했다.<br>\n해당 부분은 다른 이유가 있어서 짧게 한 건지, 당시에 프레임 길이에 대한 연구가 현재보다 덜 발달해서 그런 것인지는 확인을 해봐야 할 듯 하다.</p>\n<h3>Language Model</h3>\n<p>본 논문의 모델은 성능 테스트시에, 정확히 맞추거나 그럴싸하게 틀렸다고 한다.<br>\n<img src=\"https://user-images.githubusercontent.com/42150335/134006322-cd818c2b-5b83-4875-a558-26b21cdcf2aa.png\" alt=\"performance-test\"></p>\n<p>arther => are there, n tickets => any tickets 등 꽤나 말이 되도록 틀린 것을 볼 수 있다.<br>\n본 논문은 이보다 더 정확한 인식을 위하여 N-gram Language Model을 사용했다고 한다.</p>\n<p>매우 방대한 텍스트 Corpus로 N-gram language model을 학습시켰으며, 해당 언어 모델은 다음 공식에 사용됐다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006338-ca5fd3b0-575f-4bcc-b512-1785a11b8d51.png\" alt=\"scoring\"></p>\n<p>여기서 알파, 베타는 설정 가능한 파라미터이다.<br>\n본 논문에서는 성능을 향상시키기 위해 빔서치를 사용했는데, 이때 빔 사이즈를 1,000 - 8,000으로 상당히 크게 준 것을 볼 수 있었다.<br>\n이후에 나온 논문들을 봤을 때, 빔 사이즈 단위는 기껏 해봐야 수십 정도였는데 본 논문은 상당히 큰 빔 사이즈를 사용한 것을 볼 수 있었다.</p>\n<h2>Optimizations</h2>\n<p>해당 장에서는 어떻게 최적화를 했는지에 대해 설명하고 있다.<br>\n주로 빠른 학습을 시키기 위해 어떤 노력을 했는지를 설명했다.</p>\n<h3>Data Parallelism</h3>\n<p>데이터를 효과적으로 처리하기 위해 2-level data parallelism을 사용했다고 한다.</p>\n<p>미니배치 단위로 처리를 했는데, 이때 배치의 크기를 GPU 메모리 한계까지 사용했다고 한다.</p>\n<p>또한, 학습을 빨리하기 위해 NMT와 같은 Text-NLP에서 많이 사용되는, 길이 순으로 정렬해서 비슷한 길이끼리 배치로 묶었다고 한다. 이렇게 비슷한 길이끼리 배치로 묶게 되면, 배치 안에서 Max Length를 맞추기 위해 PAD token을 최소화 할 수 있다.</p>\n<h1></h1>\n<p>본 논문은 음성 인식의 기반을 다진 논문인지라 논문에 소개된 대부분의 내용이 음성인식 튜토리얼 내용과 비슷했습니다.<br>\n해당 논문 리뷰는 여기까지만 하겠습니다.</p>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Deep Speech: Scaling up end-to-end speech recognition"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006014-e0ea7aa9-fa25-4b31-90fe-c1144cd68d40.png","alt":"title"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/pdf/1412.5567.pdf"},"children":[{"type":"text","value":"https://arxiv.org/pdf/1412.5567.pdf"}]},{"type":"text","value":" (Awni Hannun et al. 2014)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Abstract"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문은 2014년도에 나온 논문이다. 논문을 읽어본 결과, 당시에는 음성인식에 End-to-End 방식의 딥러닝을 적용한 사례가 없던 모양이다. (또는 주목할만한 성과가 나오지 않았던 모양이다.) 본 논문에서는 End-to-End 방식으로 Switchboard Hub5’00 데이터셋에서 16.0%의 Error Rate를 기록하며 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"State-Of-The-Art"}]},{"type":"text","value":" (SOTA) 를 달성한 성과를 밝히고 있다. 기존 음성인식의 traditional한 방식은 전처리 과정이 상당히 많이 필요했지만, 이러한 과정 없이 데이터와 레이블만을 이용한 End-to-End 방식으로 이러한 성과를 냈음을 거듭 강조하고 있다. 그리고 이전 방식으로는 노이즈가 있는 환경에서 급격히 떨어졌는데 반하여, 본 논문 방식으로는 노이즈가 있는 환경에서도 좋은 성능을 기록했다고 한다. 즉, 기존 방식보다 더 간단하면서도 좋은 성능을 기록한 End-to-End 방식의 Speech-Recognition 모델을 소개하는 논문이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Introduction"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Abstract에서 강조한 내용을 다시 한번 강조한다. 기존 traditional한 방식은 전문가들의 손이 많이 가야했다. (노이즈 필터링 등..) 하지만 그러한 노력에도 불구하고, 실제 노이즈가 낀 상황에서의 인식률은 좋지 못했다. 하지만 End-to-End 방식으로 이러한 2가지의 단점을 개선할 수 있다고 주장한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"물론 End-to-End 방식으로 가기 위해서는 몇가지 고려해야할 사항들이 있었지만, 본 논문에서는 기존 연구 결과들을 참고하여 End-to-End 방식을 시도할 수 있었다고 말한다. 그리고, 본 논문에서는 RNN 모델을 사용했다. 뒤에 더 자세히 설명하겠지만 본 논문에서는 학습 시간을 단축시키기 위해 많은 고려를 한 것으로 보인다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"RNN Training Setup"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006049-0cdcb969-0383-4722-b675-87b9f3587488.png","alt":"model"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"해당 챕터에서는 자신들이 어떤 식으로 모델을 구성했는지에 대해 설명한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n모델의 핵심은 RNN으로 구성되어 있으며, 트레이닝 셋은 ${(x_1, y_1), (x_2, y_2), … (x_t, y_t)}$ 와 같은 딕셔너리 형식으로 구성했다고 한다. (x는 스펙트로그램, y는 문자로 구성 )"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"모델은 총 5개의 히든 레이어로 구성했다고 한다.\n논문을 읽으면서 모델 아키텍쳐가 상당히 특이하다고 생각했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"현재 내가 알고있는 방식과는 사뭇 다른 방식이였는데, 히든 레이어 중 1, 2, 3번째 레이어는 병렬적 (Parallel) 하게 처리하기 위해 서로 독립적으로 포워딩 된다고 한다. RNN 아키텍쳐를 사용하게 되면 이전 셀의 아웃풋이 필요하기 때문에 어쩔 수 없이 병렬처리의 한계점이 있기 때문에 학습 속도 개선을 위해 각 인풋을 독립적으로 처리했다고 한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n여기서도 나는 본 논문이 학습 시간을 단축시키기 위해 상당히 노력했다는 인상을 받았다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006132-c4eef972-3f32-4839-953f-8950ddc08b47.png","alt":"forward"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 수식을 통해 포워딩이 진행되는데, 여기서 g는 최소 0, 최대 20의 값을 가지는 ReLU 함수이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그리고 4번째 레이어에서는 Bidirectional-RNN으로 구성했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006150-b5ab3ce6-27a3-44fd-9a61-50e96e031071.png","alt":"Bi-RNN"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"(f)는 정방향 (forward), (b)는 역방향 (backward)을 표현한 것이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이때 주의할 점으로는, forward는 t = 1 에서 t = T 방향으로 흐르고, backward는 t = T에서 t = 1 방향으로 흐른다는 점이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006159-84108cbf-f8d5-47d7-aa57-43400eb37c1d.png","alt":"5-layer"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 마지막 5번째 레이어는 이렇게 forward, backward의 결과에 웨이트를 주고 1, 2, 3 번째 레이어와 동일한 ReLU를 활성화 함수로 사용했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006172-ba0e1f77-acc5-4f55-a469-57df1bb95881.png","alt":"softmax"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 이렇게 나온 결과는 Softmax 함수에 넣어서 최종적으로 Classfication을 진행한다. 또한 loss 계산시에는 CTC loss를 사용했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"여기서 또 특이했던 점으로, LSTM이 아닌 기본 RNN을 사용했다는 점이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그 이유로 본 논문에서는 LSTM의 단점은 메모리가 많이 소요되고, 학습이 오래걸린다는 점을 꼽았다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"총 5개의 히든 레이어 중 실제 RNN 계층은 1개 뿐이라는 점과, LSTM이 아닌 RNN을 사용했다는 점이 인상깊었다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n최근 연구에서는 기본 RNN을 사용하는 모습을 거의 볼 수 없는데, 당시에는 아직 GPU의 성능이 그리 좋지 않던 때라 그런지 학습 속도에 대해 굉장히 고려를 많이한 모습이 보였다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Regularization"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문은 학습 시, 드랍아웃 비율을 5 - 10%정도를 유지했다고 한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그리고 Spectrogram의 프레임 길이는 10ms, 포워딩은 5ms를 사용했다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n(해당 부분은 오류가 있을수도 있습니다)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"통상적으로 음성 인식에서 프레임 길이는 20 - 40ms를 사용하기 때문에 프레임 길이가 상당히 짧다고 생각했다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n해당 부분은 다른 이유가 있어서 짧게 한 건지, 당시에 프레임 길이에 대한 연구가 현재보다 덜 발달해서 그런 것인지는 확인을 해봐야 할 듯 하다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Language Model"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문의 모델은 성능 테스트시에, 정확히 맞추거나 그럴싸하게 틀렸다고 한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006322-cd818c2b-5b83-4875-a558-26b21cdcf2aa.png","alt":"performance-test"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"arther => are there, n tickets => any tickets 등 꽤나 말이 되도록 틀린 것을 볼 수 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n본 논문은 이보다 더 정확한 인식을 위하여 N-gram Language Model을 사용했다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"매우 방대한 텍스트 Corpus로 N-gram language model을 학습시켰으며, 해당 언어 모델은 다음 공식에 사용됐다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006338-ca5fd3b0-575f-4bcc-b512-1785a11b8d51.png","alt":"scoring"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"여기서 알파, 베타는 설정 가능한 파라미터이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n본 논문에서는 성능을 향상시키기 위해 빔서치를 사용했는데, 이때 빔 사이즈를 1,000 - 8,000으로 상당히 크게 준 것을 볼 수 있었다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이후에 나온 논문들을 봤을 때, 빔 사이즈 단위는 기껏 해봐야 수십 정도였는데 본 논문은 상당히 큰 빔 사이즈를 사용한 것을 볼 수 있었다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Optimizations"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"해당 장에서는 어떻게 최적화를 했는지에 대해 설명하고 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n주로 빠른 학습을 시키기 위해 어떤 노력을 했는지를 설명했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Data Parallelism"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"데이터를 효과적으로 처리하기 위해 2-level data parallelism을 사용했다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"미니배치 단위로 처리를 했는데, 이때 배치의 크기를 GPU 메모리 한계까지 사용했다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"또한, 학습을 빨리하기 위해 NMT와 같은 Text-NLP에서 많이 사용되는, 길이 순으로 정렬해서 비슷한 길이끼리 배치로 묶었다고 한다. 이렇게 비슷한 길이끼리 배치로 묶게 되면, 배치 안에서 Max Length를 맞추기 위해 PAD token을 최소화 할 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문은 음성 인식의 기반을 다진 논문인지라 논문에 소개된 대부분의 내용이 음성인식 튜토리얼 내용과 비슷했습니다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n해당 논문 리뷰는 여기까지만 하겠습니다."}]}],"data":{"quirksMode":false}},"excerpt":"Deep Speech: Scaling up end-to-end speech recognition title https://arxiv.org/pdf/1412.5567.pdf (Awni Hannun et al. 2014) Abstract…","fields":{"readingTime":{"text":"10 min read"}},"frontmatter":{"title":"DeepSpeech Paper Review","userDate":"11 November 2019","date":"2019-11-11T10:00:00.000Z","tags":["speech","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/360f6a5bb171f9136086a6bf108b401d/2241d/deepspeech.png","srcSet":"/static/360f6a5bb171f9136086a6bf108b401d/2241d/deepspeech.png 541w","sizes":"100vw"},"sources":[{"srcSet":"/static/360f6a5bb171f9136086a6bf108b401d/1edf8/deepspeech.webp 541w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.9149722735674677}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}]}}]}},"relatedPosts":{"totalCount":20,"edges":[{"node":{"id":"64e39e81-9c08-53ad-967e-f53e0ffd1d51","excerpt":"한국어 Tacotron2 이번 포스팅에서는 Tacotron2 아키텍처로 한국어 TTS 시스템을 만드는 방법에 대해 다루겠습니다. Tacotron2 Tacotron2는 17년 12월 구글이 NATURAL TTS SYNTHESIS BY…","frontmatter":{"title":"한국어 Tacotron2","date":"2021-10-10T10:00:00.000Z"},"fields":{"readingTime":{"text":"11 min read"},"slug":"/korean_tacotron2/"}}},{"node":{"id":"8609f7b7-4942-59fe-bfaa-5f82c648649e","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP_ Generating expressive speech from raw audio/"}}},{"node":{"id":"75998e15-7d74-5d05-af5b-1112437e067d","excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","date":"2021-03-17T10:00:00.000Z"},"fields":{"readingTime":{"text":"3 min read"},"slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/"}}},{"node":{"id":"feb53e83-51f2-5350-ae0e-58157d8dfd22","excerpt":"PORORO Text-To-Speech (TTS) 얼마전에 저희 팀에서 공개한 PORORO: Platform Of neuRal mOdels for natuRal language prOcessing 라이브러리에 제가 공들여만든 TTS…","frontmatter":{"title":"PORORO Text-To-Speech (TTS)","date":"2021-02-16T10:00:00.000Z"},"fields":{"readingTime":{"text":"1 min read"},"slug":"/pororo-tts/"}}},{"node":{"id":"77bed2d4-fc96-5bff-9808-9b9cb45369f3","excerpt":"EMNLP Paper Review: Speech Adaptive Feature Selection for End-to-End Speech Translation (Biao Zhang et al) Incremental Text-to-Speech…","frontmatter":{"title":"EMNLP Paper Review: Speech","date":"2020-12-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/2020 EMNLP Speech Paper Review/"}}}]}},"pageContext":{"slug":"/deepspeech/","prev":{"excerpt":"「Listen, Attend and Spell」 Review title https://arxiv.org/abs/1508.01211  (William Chan et al. 2015)  Introduction 어텐션 기반 Seq2seq…","frontmatter":{"title":"Listen, Attend and Spell Paper Review","tags":["speech","paper"],"date":"2019-09-20T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAYCAYAAAD6S912AAAACXBIWXMAAAsTAAALEwEAmpwYAAAC9klEQVQ4y42VCU8iURCE5///HDagUcEjMSrEM4p4gDMwHIMOICCHV22+XptdjZIlqcw7q+vV634EURgqqtc1Go00HA6X4vHxUaPRWC8vL3p+fravtedz67++virY3NzU6uqqtra2lM1mlS8UVCgUlM1lxRygz/yvXxnt7+8rjhsqloq6vLxUuVJRt9tVv9838uDw8NAW53I5HRwcWHt9fd3aGxsbC6ytrWlnZ0enp6e2ud1pK0kSJd2uhqOh3t7exC84OTlRsVjU8fGxkdA/OjpatB0Q7e3tqVwu66ff+/u7AqJEUaRms6m7uzt1Oh3FcWxjvV7PfBsMBoYwDPXw8GCbUQTBvzCFkFxfX6vTSXRxcWHEoFarqdVqLUCgSqVic0sV7u7uKpPJKJ/Pm3d4Wfi4GPx0bG9v2zx2JElHNzc3ppjTIKgRx38UlkolMx1SPFpZWTGwmSAO1hAEPyFMuomm06lms6km06mljRESCaM5+tnZmUWr1+sGPz7gyMw3Go3lR26327q9vdXV1ZUBIidN09RSBHBBBPdL+Xohi0vBcLyAwC/CLwVCiADZQGCS+CdCU3h/f29RUQEZG4CP9fsDIyZtIKYEl/0Cr0sHRnvusbnZjNVqt0RgVLoFaS+1QID1CGB/8F2U+Xyu8XhsD0atWlXVrGgqjELVG3VLekubKDJC1iLAavk7wtlspslkYgppe1X8zy8gAjmFXDZChEIIiEgfpU9PT2YHfYL488Ueni1AO0Cqm+5fPy6gjVKfwy/zME3NN693xlgb+AUwgDck7ldQXuHHQwwJFwShJz8cEH8iZCF5htkshARASCAehmq1amSeaqyjwvzmjZDjkH+8NF4xLCIyhHw9EBXEPOkDIWANc4wtFBIZBdQrFQI5pARiMX1XS91HH+mCUo6KUsa4uACS8/Nziw4ZRGzyNoQEow/4H0Elb4D76WVJEKtlNgInQQltr2/6nAL4GISuki8KaQeeGp5rePr1+zmN/o4x78A6cvQ3oXzzIv4jxPMAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/c6dd3a7d5b5935928a2ffb65755ccaf6/61a36/las.png","srcSet":"/static/c6dd3a7d5b5935928a2ffb65755ccaf6/61a36/las.png 625w","sizes":"100vw"},"sources":[{"srcSet":"/static/c6dd3a7d5b5935928a2ffb65755ccaf6/09f70/las.webp 625w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.192}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"8 min read"},"layout":"","slug":"/las/"}},"next":{"excerpt":"네이버 2019 해커톤 - Speech 결선진출 네이버  2019 해커톤 - Speech 대회 예선전에서 100 팀 중 11위를 기록하며 예선전을 통과했다 !! 오늘 아침까지만 해도 10위여서 Top 1…","frontmatter":{"title":"네이버 2019 해커톤 - Speech 결선진출","tags":["record"],"date":"2019-11-21T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/9cdc30b955fd570c857fed56edaa1f86/88663/2019_hackathon.png","srcSet":"/static/9cdc30b955fd570c857fed56edaa1f86/88663/2019_hackathon.png 720w","sizes":"100vw"},"sources":[{"srcSet":"/static/9cdc30b955fd570c857fed56edaa1f86/21362/2019_hackathon.webp 720w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.3347222222222221}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"5 min read"},"layout":"","slug":"/naver_2019_hackathon/"}},"primaryTag":"speech"}},
    "staticQueryHashes": ["3170763342","3229353822"]}