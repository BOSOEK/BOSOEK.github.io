{
    "componentChunkName": "component---src-templates-tags-tsx",
    "path": "/tags/nlp/",
    "result": {"data":{"allTagYaml":{"edges":[{"node":{"id":"speeches","description":"Some of the greatest words ever spoken.","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#282818","images":{"fallback":{"src":"/static/4dd7b5f567d2699202bc626bbcda5936/7f071/speeches-cover.jpg","srcSet":"/static/4dd7b5f567d2699202bc626bbcda5936/7a1a9/speeches-cover.jpg 750w,\n/static/4dd7b5f567d2699202bc626bbcda5936/25ec5/speeches-cover.jpg 1080w,\n/static/4dd7b5f567d2699202bc626bbcda5936/28e43/speeches-cover.jpg 1366w,\n/static/4dd7b5f567d2699202bc626bbcda5936/7f071/speeches-cover.jpg 1400w","sizes":"100vw"},"sources":[{"srcSet":"/static/4dd7b5f567d2699202bc626bbcda5936/662b3/speeches-cover.webp 750w,\n/static/4dd7b5f567d2699202bc626bbcda5936/6e6c6/speeches-cover.webp 1080w,\n/static/4dd7b5f567d2699202bc626bbcda5936/e4be3/speeches-cover.webp 1366w,\n/static/4dd7b5f567d2699202bc626bbcda5936/04d29/speeches-cover.webp 1400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.4407142857142857}}}}}]},"allMarkdownRemark":{"totalCount":30,"edges":[{"node":{"excerpt":"Decoding Strategy (디코딩 전략) 이번 포스팅에서는 자연어처리 모델의 디코딩 전략에 관해서 다뤄보려고 합니다. 디코딩이란 말처럼 디코딩은 디코더에서\n수행하는 작업입니다. 즉, BERT와 같은 인코더 모델에서 사용하는게 아니라 GPT…","frontmatter":{"title":"Decoding Strategy (디코딩 전략)","excerpt":null,"tags":["nlp"],"date":"2022-01-15T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/43d0a11cfbaf2b0bf2571f5678e1ced5/68f14/decoding.png","srcSet":"/static/43d0a11cfbaf2b0bf2571f5678e1ced5/d0df2/decoding.png 750w,\n/static/43d0a11cfbaf2b0bf2571f5678e1ced5/fa96f/decoding.png 1080w,\n/static/43d0a11cfbaf2b0bf2571f5678e1ced5/ad399/decoding.png 1366w,\n/static/43d0a11cfbaf2b0bf2571f5678e1ced5/68f14/decoding.png 1400w","sizes":"100vw"},"sources":[{"srcSet":"/static/43d0a11cfbaf2b0bf2571f5678e1ced5/b9516/decoding.webp 750w,\n/static/43d0a11cfbaf2b0bf2571f5678e1ced5/c4814/decoding.webp 1080w,\n/static/43d0a11cfbaf2b0bf2571f5678e1ced5/2a401/decoding.webp 1366w,\n/static/43d0a11cfbaf2b0bf2571f5678e1ced5/db739/decoding.webp 1400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5614285714285715}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"9 min read"},"layout":"","slug":"/generate/"}}},{"node":{"excerpt":"Generation with Retrieval 이번에 딥마인드에서 RETRO(Retrieval-Enhanced Transformer) 라는 모델을 내놓았습니다. 문서 retrieval + GPT 기반 모델인데,\n7B 모델임에도 불구하고 2…","frontmatter":{"title":"Generation with Retrieval","excerpt":null,"tags":["nlp","paper"],"date":"2022-01-04T23:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/37530fc680166ef6d64a040fedb23876/7eb21/fid.png","srcSet":"/static/37530fc680166ef6d64a040fedb23876/41321/fid.png 750w,\n/static/37530fc680166ef6d64a040fedb23876/22692/fid.png 1080w,\n/static/37530fc680166ef6d64a040fedb23876/7eb21/fid.png 1344w","sizes":"100vw"},"sources":[{"srcSet":"/static/37530fc680166ef6d64a040fedb23876/3df03/fid.webp 750w,\n/static/37530fc680166ef6d64a040fedb23876/cd184/fid.webp 1080w,\n/static/37530fc680166ef6d64a040fedb23876/2e84a/fid.webp 1344w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.6607142857142857}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"6 min read"},"layout":"","slug":"/fid_and_rag/"}}},{"node":{"excerpt":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems Paper Review Paper: https://aclanthology.org/2021.naacl-main.12…","frontmatter":{"title":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems Paper Review","excerpt":null,"tags":["nlp","paper"],"date":"2021-12-18T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/ad2df79ee3a7cc5e1d4fe4abd604473d/3a1cc/bert_fp.png","srcSet":"/static/ad2df79ee3a7cc5e1d4fe4abd604473d/69607/bert_fp.png 750w,\n/static/ad2df79ee3a7cc5e1d4fe4abd604473d/27438/bert_fp.png 1080w,\n/static/ad2df79ee3a7cc5e1d4fe4abd604473d/3a1cc/bert_fp.png 1234w","sizes":"100vw"},"sources":[{"srcSet":"/static/ad2df79ee3a7cc5e1d4fe4abd604473d/8c326/bert_fp.webp 750w,\n/static/ad2df79ee3a7cc5e1d4fe4abd604473d/32767/bert_fp.webp 1080w,\n/static/ad2df79ee3a7cc5e1d4fe4abd604473d/d399b/bert_fp.webp 1234w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.526742301458671}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"2 min read"},"layout":"","slug":"/bert_fp/"}}},{"node":{"excerpt":"GPT (Generative Pre-trained Transformer) 1 gpt1 먼저 알아보고, gpt2에 대해 알아보겠습니다. GPT1 Improving Language Understanding by Generative Pre-Training…","frontmatter":{"title":"GPT (Generative Pre-trained Transformer)","excerpt":null,"tags":["nlp","parallelism","large-scale","lm"],"date":"2021-11-23T11:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/833f7784ccf61f40030aff940fe3ac06/7eac2/gpt.png","srcSet":"/static/833f7784ccf61f40030aff940fe3ac06/7eac2/gpt.png 716w","sizes":"100vw"},"sources":[{"srcSet":"/static/833f7784ccf61f40030aff940fe3ac06/4c8d7/gpt.webp 716w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.6061452513966481}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"13 min read"},"layout":"","slug":"/gpt/"}}},{"node":{"excerpt":"Large Scale LM (2) Distributed Programming (작성중) 이 자료는 [해당 link…","frontmatter":{"title":"Large Scale LM (2) Distributed Programming","excerpt":null,"tags":["nlp","parallelism","large-scale","lm"],"date":"2021-11-22T11:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/346eaa433ec4cadd1f537f0536913cd3/ca97d/big-model2.png","srcSet":"/static/346eaa433ec4cadd1f537f0536913cd3/260a8/big-model2.png 750w,\n/static/346eaa433ec4cadd1f537f0536913cd3/99085/big-model2.png 1080w,\n/static/346eaa433ec4cadd1f537f0536913cd3/cff5f/big-model2.png 1366w,\n/static/346eaa433ec4cadd1f537f0536913cd3/ca97d/big-model2.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/346eaa433ec4cadd1f537f0536913cd3/72019/big-model2.webp 750w,\n/static/346eaa433ec4cadd1f537f0536913cd3/acb5c/big-model2.webp 1080w,\n/static/346eaa433ec4cadd1f537f0536913cd3/41355/big-model2.webp 1366w,\n/static/346eaa433ec4cadd1f537f0536913cd3/a0759/big-model2.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.30885416666666665}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"17 min read"},"layout":"","slug":"/big-model2/"}}},{"node":{"excerpt":"Large Scale LM (1) Background 이 자료는 [해당 link…","frontmatter":{"title":"Large Scale LM (1) Background","excerpt":null,"tags":["nlp","parallelism","large-scale","lm"],"date":"2021-11-22T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/d6ed5ee3d7d66b561a10d5ef112636a6/31d4c/big-model1.png","srcSet":"/static/d6ed5ee3d7d66b561a10d5ef112636a6/4ec63/big-model1.png 750w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/2df19/big-model1.png 1080w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/e3ec4/big-model1.png 1366w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/31d4c/big-model1.png 1720w","sizes":"100vw"},"sources":[{"srcSet":"/static/d6ed5ee3d7d66b561a10d5ef112636a6/6c332/big-model1.webp 750w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/03806/big-model1.webp 1080w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/698e5/big-model1.webp 1366w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/e01f3/big-model1.webp 1720w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.4872093023255814}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"5 min read"},"layout":"","slug":"/big-model1/"}}},{"node":{"excerpt":"DeepSpeed Usage…","frontmatter":{"title":"DeepSpeed Usage","excerpt":null,"tags":["nlp","parallelism","large-scale"],"date":"2021-10-30T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#081868","images":{"fallback":{"src":"/static/9b0c4df957330fe4209b1e9f8e708409/67a35/deepspeed.png","srcSet":"/static/9b0c4df957330fe4209b1e9f8e708409/67a35/deepspeed.png 224w","sizes":"100vw"},"sources":[{"srcSet":"/static/9b0c4df957330fe4209b1e9f8e708409/f42a0/deepspeed.webp 224w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/deepspeed/"}}},{"node":{"excerpt":"NLP Metrics Confusion Matrix Confusion Matrix는 분류 모델을 평가할때 모델이 얼마나 정밀한지, 얼마나 실용적인 분류를 해냈는지, 얼마나 정확한 분류를 해냈는지에 대한 모든 내용을 포함하고 있습니다. Accuracy…","frontmatter":{"title":"NLP Metrics","excerpt":null,"tags":["nlp","metric"],"date":"2021-10-13T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/1edcc9c2169958556ac8c9ac51ec28de/a5b23/cm.png","srcSet":"/static/1edcc9c2169958556ac8c9ac51ec28de/ea251/cm.png 750w,\n/static/1edcc9c2169958556ac8c9ac51ec28de/a5b23/cm.png 860w","sizes":"100vw"},"sources":[{"srcSet":"/static/1edcc9c2169958556ac8c9ac51ec28de/678bf/cm.webp 750w,\n/static/1edcc9c2169958556ac8c9ac51ec28de/d78b9/cm.webp 860w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5034883720930232}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"14 min read"},"layout":"","slug":"/metric/"}}},{"node":{"excerpt":"Page Rank 구글은 무엇을 기준으로 사이트를 보여주는 순서를 정할까요?? 구글에 특정 단어를 검색하면 다음과 같이 여러 사이트 들을 보여주는 것을 알 수 있습니다. 구글은 이런 사이트들에 점수를 부여해주는데, 여기서 부여된 점수들을 Page…","frontmatter":{"title":"\\[Sooftware NLP\\] Page Rank란??","excerpt":null,"tags":["nlp","algorithm"],"date":"2021-10-08T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/48f781cfe0664d01fb0b81d9e0c56e60/5088a/page_rank.png","srcSet":"/static/48f781cfe0664d01fb0b81d9e0c56e60/744c5/page_rank.png 750w,\n/static/48f781cfe0664d01fb0b81d9e0c56e60/5088a/page_rank.png 818w","sizes":"100vw"},"sources":[{"srcSet":"/static/48f781cfe0664d01fb0b81d9e0c56e60/7c11c/page_rank.webp 750w,\n/static/48f781cfe0664d01fb0b81d9e0c56e60/1c73b/page_rank.webp 818w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.6405867970660146}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"7 min read"},"layout":"","slug":"/page_rank/"}}},{"node":{"excerpt":"Uniform Length Batching in PyTorch 전체 토큰 길이가 비슷한 인풋끼리 배치를 이루어주는 방식 그냥 랜덤하게 배치를 묶어주면 길이가 한 데이터를 제외하고는 평균 길이가 10인데 한 데이터 길이가 10…","frontmatter":{"title":"Uniform Length Batching in PyTorch","excerpt":null,"tags":["nlp"],"date":"2021-09-28T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/d35fd77fe1abce1948a61534ae0683af/d389c/ulb.png","srcSet":"/static/d35fd77fe1abce1948a61534ae0683af/57129/ulb.png 750w,\n/static/d35fd77fe1abce1948a61534ae0683af/3c97c/ulb.png 1080w,\n/static/d35fd77fe1abce1948a61534ae0683af/6ea65/ulb.png 1366w,\n/static/d35fd77fe1abce1948a61534ae0683af/d389c/ulb.png 1592w","sizes":"100vw"},"sources":[{"srcSet":"/static/d35fd77fe1abce1948a61534ae0683af/86b93/ulb.webp 750w,\n/static/d35fd77fe1abce1948a61534ae0683af/4c98a/ulb.webp 1080w,\n/static/d35fd77fe1abce1948a61534ae0683af/e8f75/ulb.webp 1366w,\n/static/d35fd77fe1abce1948a61534ae0683af/30d2a/ulb.webp 1592w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.3674623115577889}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"6 min read"},"layout":"","slug":"/uniform_length_batching/"}}},{"node":{"excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","excerpt":null,"tags":["speech","nlp","paper"],"date":"2021-09-19T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/b91fe939e42bcc0b6f0c076dca98fcc8/afa5c/gslm.png","srcSet":"/static/b91fe939e42bcc0b6f0c076dca98fcc8/0dee1/gslm.png 750w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/8beaa/gslm.png 1080w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/d079a/gslm.png 1366w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/afa5c/gslm.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/b91fe939e42bcc0b6f0c076dca98fcc8/a66aa/gslm.webp 750w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/65dd5/gslm.webp 1080w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/4fad6/gslm.webp 1366w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/c512e/gslm.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5625}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/Textledd NLP_ Generating expressive speech from raw audio/"}}},{"node":{"excerpt":"이번에 저희 튜닙에서 공들여 만든 TUNiB Electra 모델을 공개했습니다 !! 🎉 🎉 이번 공개에서는 한-영 bilingual 모델과 한국어 모델을 각각 Small/Base 사이즈로 공개했으며, HuggingFace transformers…","frontmatter":{"title":"TUNiB Electra 공개","excerpt":null,"tags":["huggingface","nlp","record"],"date":"2021-09-18T15:11:55.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/7ccdb9951c9d362c8a3548e8c2a87231/59ccb/electra.png","srcSet":"/static/7ccdb9951c9d362c8a3548e8c2a87231/c68af/electra.png 750w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/87f65/electra.png 1080w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/d464a/electra.png 1366w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/59ccb/electra.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ccdb9951c9d362c8a3548e8c2a87231/9fb02/electra.webp 750w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/cd76f/electra.webp 1080w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/b7397/electra.webp 1366w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/507b8/electra.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.4479166666666667}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"2 min read"},"layout":"post","slug":"/tunib_electra/"}}},{"node":{"excerpt":"Tokenization 문장에서 의미있는 단위로 나누는 작업을 라고 한다. 문자 단위 토큰화 문자 단위로 토큰화를 하는 것이다. 한글 음절 수는 모두 11,172개이므로 알파벳, 숫자, 기호 등을 고려한다고 해도 단어 사전의 크기는 기껏해야 1…","frontmatter":{"title":"Tokenizer","excerpt":null,"tags":["nlp"],"date":"2021-09-13T23:46:37.121Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#c8b8b8","images":{"fallback":{"src":"/static/8e92507f0141c508e4d5a30d3ca6fe53/4d1d2/writing.jpg","srcSet":"/static/8e92507f0141c508e4d5a30d3ca6fe53/6cce3/writing.jpg 750w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/fb319/writing.jpg 1080w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/6a5de/writing.jpg 1366w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/4d1d2/writing.jpg 1400w","sizes":"100vw"},"sources":[{"srcSet":"/static/8e92507f0141c508e4d5a30d3ca6fe53/d2a19/writing.webp 750w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/72f43/writing.webp 1080w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/8992a/writing.webp 1366w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/d652b/writing.webp 1400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.6678571428571428}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"10 min read"},"layout":"post","slug":"/tokenizer/"}}},{"node":{"excerpt":"정규 표현식 정규표현식(regular expression)은 일종의 문자를 표현하는 공식으로, 특정 규칙이 있는 문자열 집합을 추출할 때 자주 사용되는 기법입니다. 주로 Prograaming Language나 Text Editor…","frontmatter":{"title":"정규표현식 (regex)","excerpt":null,"tags":["nlp"],"date":"2021-09-08T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/ebc4f36cae54a4a8d9eea6079daca5bc/acc2d/regex.png","srcSet":"/static/ebc4f36cae54a4a8d9eea6079daca5bc/acc2d/regex.png 699w","sizes":"100vw"},"sources":[{"srcSet":"/static/ebc4f36cae54a4a8d9eea6079daca5bc/8a3ab/regex.webp 699w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.45064377682403434}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"30 min read"},"layout":"","slug":"/regex/"}}},{"node":{"excerpt":"최근 NLP 토크나이저를 만드는데 가장 많이 사용되는  라이브러와 실제 사용이 가장 많이 되는  라이브러리로의 변환에 대한 코드를 담고 있습니다. 해당 내용은  버젼에서 수행되었습니다. Train 아래 코드는 wordpiece, char-bpe…","frontmatter":{"title":"Hugging Face Tokenizers","excerpt":null,"tags":["huggingface","nlp"],"date":"2021-08-11T15:11:55.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8d818","images":{"fallback":{"src":"/static/ccbb378a7f6bb27d48ab79b74b1b4d28/5eb66/huggingface.png","srcSet":"/static/ccbb378a7f6bb27d48ab79b74b1b4d28/c65f6/huggingface.png 750w,\n/static/ccbb378a7f6bb27d48ab79b74b1b4d28/5eb66/huggingface.png 798w","sizes":"100vw"},"sources":[{"srcSet":"/static/ccbb378a7f6bb27d48ab79b74b1b4d28/2b9c0/huggingface.webp 750w,\n/static/ccbb378a7f6bb27d48ab79b74b1b4d28/1b08c/huggingface.webp 798w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.9511278195488723}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"post","slug":"/tokenizers/"}}},{"node":{"excerpt":"Efficient Attention: Attention with Linear Complexities Shen Zhuoran et al. Abstract Dot-product attention은 들어오는 인풋 길이에 따라 memory…","frontmatter":{"title":"Efficient Attention Paper Review","excerpt":null,"tags":["nlp","paper"],"date":"2021-07-17T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/2cdc9b7481cd5e517d106c8618db52d3/a2d8d/efficient_attention.png","srcSet":"/static/2cdc9b7481cd5e517d106c8618db52d3/a74b9/efficient_attention.png 750w,\n/static/2cdc9b7481cd5e517d106c8618db52d3/63493/efficient_attention.png 1080w,\n/static/2cdc9b7481cd5e517d106c8618db52d3/a2d8d/efficient_attention.png 1211w","sizes":"100vw"},"sources":[{"srcSet":"/static/2cdc9b7481cd5e517d106c8618db52d3/ba69e/efficient_attention.webp 750w,\n/static/2cdc9b7481cd5e517d106c8618db52d3/f8adb/efficient_attention.webp 1080w,\n/static/2cdc9b7481cd5e517d106c8618db52d3/a7f95/efficient_attention.webp 1211w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.4706853839801816}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"1 min read"},"layout":"","slug":"/efficient-attention/"}}},{"node":{"excerpt":"Luna: Linear Unified Nested Attention USC + CMU + Facebook AI 2021.06 code Abstract 트랜스포머의 Multi Headed Self Attention…","frontmatter":{"title":"Luna: Linear Unified Nested Attention","excerpt":null,"tags":["nlp","paper"],"date":"2021-07-03T23:46:37.121Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/540ac0a1a4457065fef9e262f9962c83/af6c5/luna.png","srcSet":"/static/540ac0a1a4457065fef9e262f9962c83/a5b02/luna.png 750w,\n/static/540ac0a1a4457065fef9e262f9962c83/ace3b/luna.png 1080w,\n/static/540ac0a1a4457065fef9e262f9962c83/58836/luna.png 1366w,\n/static/540ac0a1a4457065fef9e262f9962c83/af6c5/luna.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/540ac0a1a4457065fef9e262f9962c83/6f4ad/luna.webp 750w,\n/static/540ac0a1a4457065fef9e262f9962c83/9ec3b/luna.webp 1080w,\n/static/540ac0a1a4457065fef9e262f9962c83/51e4d/luna.webp 1366w,\n/static/540ac0a1a4457065fef9e262f9962c83/3c39a/luna.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5192708333333333}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"post","slug":"/luna/"}}},{"node":{"excerpt":"GPT Understands, Too Xiao Liu et al. Tsinghua University etc. arXiv pre-print Abstract GPT를 파인튜닝하는 방법은 Narural Language Understanding (NLU…","frontmatter":{"title":"P-Tuning Paper Review","excerpt":null,"tags":["nlp","paper"],"date":"2021-05-13T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/5251bff33539be461c690325c488ea29/c0e65/p_tuning.png","srcSet":"/static/5251bff33539be461c690325c488ea29/f0b2b/p_tuning.png 750w,\n/static/5251bff33539be461c690325c488ea29/3831f/p_tuning.png 1080w,\n/static/5251bff33539be461c690325c488ea29/ec348/p_tuning.png 1366w,\n/static/5251bff33539be461c690325c488ea29/c0e65/p_tuning.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/5251bff33539be461c690325c488ea29/a4cf2/p_tuning.webp 750w,\n/static/5251bff33539be461c690325c488ea29/4e6df/p_tuning.webp 1080w,\n/static/5251bff33539be461c690325c488ea29/04ab9/p_tuning.webp 1366w,\n/static/5251bff33539be461c690325c488ea29/19b4f/p_tuning.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.23489583333333336}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/p_tuning/"}}},{"node":{"excerpt":"Longformer: The Long-Document Transformer Paper Code Iz Beltagy et al. Introduction 트랜스포머는 긴 시퀀스는 처리하지 못한다는 한계를 가지고 있음 이유는 시퀀스 길이에 O(n^…","frontmatter":{"title":"Longformer Paper Review","excerpt":null,"tags":["nlp","paper"],"date":"2021-02-06T23:46:37.121Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/ef32af94bd92c05cbfebe0cb00c3966e/597e6/longformer.png","srcSet":"/static/ef32af94bd92c05cbfebe0cb00c3966e/597e6/longformer.png 512w","sizes":"100vw"},"sources":[{"srcSet":"/static/ef32af94bd92c05cbfebe0cb00c3966e/3d2a6/longformer.webp 512w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.8046875000000001}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"post","slug":"/longformer/"}}},{"node":{"excerpt":"Pororo: A Deep Learning based Multilingual Natural Language Processing Library Link: https://github.com/kakaobrain/pororo…","frontmatter":{"title":"Pororo: A Deep Learning based Multilingual Natural Language Processing Library","excerpt":null,"tags":["nlp","toolkit","record"],"date":"2021-02-02T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#080808","images":{"fallback":{"src":"/static/7ee72db9d9d5f47d514ac31fc67c7770/1c53b/pororo.png","srcSet":"/static/7ee72db9d9d5f47d514ac31fc67c7770/1c53b/pororo.png 515w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ee72db9d9d5f47d514ac31fc67c7770/92941/pororo.webp 515w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.6}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"2 min read"},"layout":"","slug":"/pororo/"}}},{"node":{"excerpt":"Fairseq’s Hydra Fairseq이 0.10.1로 버젼 업그레이드를 하면서 configuration 관리를 Hydra로 하게됨. Fairseq을 실행시키는 command line…","frontmatter":{"title":"Fairseq Hydra","excerpt":null,"tags":["toolkit","nlp"],"date":"2020-12-31T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/1b089752085416a626f169c3547274ad/de522/fairseq.png","srcSet":"/static/1b089752085416a626f169c3547274ad/eb6e7/fairseq.png 750w,\n/static/1b089752085416a626f169c3547274ad/85af9/fairseq.png 1080w,\n/static/1b089752085416a626f169c3547274ad/de522/fairseq.png 1132w","sizes":"100vw"},"sources":[{"srcSet":"/static/1b089752085416a626f169c3547274ad/8ad53/fairseq.webp 750w,\n/static/1b089752085416a626f169c3547274ad/9f48c/fairseq.webp 1080w,\n/static/1b089752085416a626f169c3547274ad/e12ec/fairseq.webp 1132w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.9946996466431096}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"3 min read"},"layout":"","slug":"/fairseq-hydra/"}}},{"node":{"excerpt":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism ​ Mohammad Shoeybi et al. 2019. NVIDIA Corp. ​ Summary…","frontmatter":{"title":"Megatron LM Paper Review","excerpt":null,"tags":["nlp","parallelism","paper"],"date":"2020-12-03T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/a4dec0b9c36035e9191658ce9647ae73/a70e6/megatron.png","srcSet":"/static/a4dec0b9c36035e9191658ce9647ae73/37b55/megatron.png 750w,\n/static/a4dec0b9c36035e9191658ce9647ae73/a70e6/megatron.png 791w","sizes":"100vw"},"sources":[{"srcSet":"/static/a4dec0b9c36035e9191658ce9647ae73/0b2ce/megatron.webp 750w,\n/static/a4dec0b9c36035e9191658ce9647ae73/c471e/megatron.webp 791w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5170670037926676}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/Megatron-lm/"}}},{"node":{"excerpt":"RoBERTa paper / code Abstract BERT를 제대로 학습시키는 법을 제안 BERT는 엄청난 모델이지만, Original BERT 논문에서 하이퍼파라미터에 대한 실험이 제대로 진행되지 않음 BERT…","frontmatter":{"title":"RoBERTa Paper Review","excerpt":null,"tags":["nlp","paper"],"date":"2020-10-11T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/26b5f2f1a6d4d031c6cf36eac285256a/1be58/roberta.png","srcSet":"/static/26b5f2f1a6d4d031c6cf36eac285256a/5dae1/roberta.png 750w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/35cd7/roberta.png 1080w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/1be58/roberta.png 1134w","sizes":"100vw"},"sources":[{"srcSet":"/static/26b5f2f1a6d4d031c6cf36eac285256a/76436/roberta.webp 750w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/ce7b4/roberta.webp 1080w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/03f03/roberta.webp 1134w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.8835978835978836}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/roberta/"}}},{"node":{"excerpt":"Below is just about everything you’ll need to style in the theme. Check the source code to see the many embedded elements within paragraphs…","frontmatter":{"title":"Electra Paper Review","excerpt":null,"tags":["nlp","paper"],"date":"2020-09-23T07:03:47.149Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/7ccdb9951c9d362c8a3548e8c2a87231/59ccb/electra.png","srcSet":"/static/7ccdb9951c9d362c8a3548e8c2a87231/c68af/electra.png 750w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/87f65/electra.png 1080w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/d464a/electra.png 1366w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/59ccb/electra.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ccdb9951c9d362c8a3548e8c2a87231/9fb02/electra.webp 750w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/cd76f/electra.webp 1080w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/b7397/electra.webp 1366w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/507b8/electra.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.4479166666666667}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"post","slug":"/electra/"}}},{"node":{"excerpt":"Beam Search (빔서치) 본 포스팅은 “빔서치”에 대한 본질적인 개념보다는 Encoder-Decoder 모델 (Seq2seq…","frontmatter":{"title":"Beam Search (빔서치)","excerpt":null,"tags":["nlp"],"date":"2020-02-14T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/2ff5ceb2a289c296c3f360461508704b/90853/beamsearch.png","srcSet":"/static/2ff5ceb2a289c296c3f360461508704b/d0f30/beamsearch.png 750w,\n/static/2ff5ceb2a289c296c3f360461508704b/90853/beamsearch.png 773w","sizes":"100vw"},"sources":[{"srcSet":"/static/2ff5ceb2a289c296c3f360461508704b/df98f/beamsearch.webp 750w,\n/static/2ff5ceb2a289c296c3f360461508704b/5b2f4/beamsearch.webp 773w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.8214747736093144}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"9 min read"},"layout":"","slug":"/beamsearch/"}}},{"node":{"excerpt":"Teacher Forcing 본 포스팅을 이해하기 위해서는 다음 글에 대한 이해가 선행되는 것이 좋습니다. RNN (Recurrent Neural Network) LSTM & GRU (Long Short Term Memory & Gated…","frontmatter":{"title":"Teacher Forcing","excerpt":null,"tags":["nlp"],"date":"2020-01-31T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/43c1417fedd352e3d13e64f0a4ceec5b/e46c1/teacher_forcing.png","srcSet":"/static/43c1417fedd352e3d13e64f0a4ceec5b/436bd/teacher_forcing.png 750w,\n/static/43c1417fedd352e3d13e64f0a4ceec5b/e46c1/teacher_forcing.png 773w","sizes":"100vw"},"sources":[{"srcSet":"/static/43c1417fedd352e3d13e64f0a4ceec5b/a51c3/teacher_forcing.webp 750w,\n/static/43c1417fedd352e3d13e64f0a4ceec5b/10f41/teacher_forcing.webp 773w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.37516170763260026}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"5 min read"},"layout":"","slug":"/teacher_forcing/"}}},{"node":{"excerpt":"Attention 본 포스팅을 이해하기 위해서는 다음 글에 대한 이해가 선행되는 것이 좋습니다. RNN (Recurrent Neural Network) LSTM & GRU (Long Short Term Memory & Gated Recurrent…","frontmatter":{"title":"Attention Mechanism (어텐션 메커니즘)","excerpt":null,"tags":["nlp"],"date":"2020-01-26T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/a9c3aa2f1074a9ebdde0b36f5881c273/e7aa3/attention.png","srcSet":"/static/a9c3aa2f1074a9ebdde0b36f5881c273/ea5bc/attention.png 750w,\n/static/a9c3aa2f1074a9ebdde0b36f5881c273/e7aa3/attention.png 773w","sizes":"100vw"},"sources":[{"srcSet":"/static/a9c3aa2f1074a9ebdde0b36f5881c273/2a863/attention.webp 750w,\n/static/a9c3aa2f1074a9ebdde0b36f5881c273/99eab/attention.webp 773w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.351875808538163}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"8 min read"},"layout":"","slug":"/attention/"}}},{"node":{"excerpt":"Seq2seq (Sequence to sequence) 본 포스팅을 이해하기 위해서는 다음 글에 대한 이해가 선행되는 것이 좋습니다. RNN (Recurrent Neural Network) LSTM & GRU (Long Short Term Memory…","frontmatter":{"title":"Seq2seq (Sequence to sequence)","excerpt":null,"tags":["nlp"],"date":"2020-01-25T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/3f604bd7e174e888d547ccba99e0fccb/a6e9f/seq2seq.png","srcSet":"/static/3f604bd7e174e888d547ccba99e0fccb/4a1e8/seq2seq.png 750w,\n/static/3f604bd7e174e888d547ccba99e0fccb/a6e9f/seq2seq.png 773w","sizes":"100vw"},"sources":[{"srcSet":"/static/3f604bd7e174e888d547ccba99e0fccb/f9860/seq2seq.webp 750w,\n/static/3f604bd7e174e888d547ccba99e0fccb/c53c5/seq2seq.webp 773w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5808538163001293}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"10 min read"},"layout":"","slug":"/seq2seq/"}}},{"node":{"excerpt":"LSTM & GRU 본 포스팅을 이해가기 위해서는 아래 글에 대한 이해가 선행되는 것이 좋습니다. RNN (Recurrent Neural Network) LSTM 등장 배경 RNN…","frontmatter":{"title":"LSTM & GRU","excerpt":null,"tags":["nlp"],"date":"2020-01-24T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/385b4880d470b853d530d77243bb4c52/b3525/lstm_gru.png","srcSet":"/static/385b4880d470b853d530d77243bb4c52/2107c/lstm_gru.png 750w,\n/static/385b4880d470b853d530d77243bb4c52/b3525/lstm_gru.png 773w","sizes":"100vw"},"sources":[{"srcSet":"/static/385b4880d470b853d530d77243bb4c52/28c13/lstm_gru.webp 750w,\n/static/385b4880d470b853d530d77243bb4c52/b6f78/lstm_gru.webp 773w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.31565329883570503}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"18 min read"},"layout":"","slug":"/lstm_gru/"}}},{"node":{"excerpt":"RNN (Recurrent Neural Network) 본 포스팅을 이해하기 위해서는 피드포워드 네트워크에 대한 이해가 선행되는 것이 좋습니다. RNN의 등장 배경 RNN에 대해 알아보기 전에 RNN…","frontmatter":{"title":"RNN (Recurrent Neural Network)","excerpt":null,"tags":["nlp"],"date":"2019-12-26T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/0ed5ff7ccf0292f6675e6399032d0148/a0c94/rnn.png","srcSet":"/static/0ed5ff7ccf0292f6675e6399032d0148/a0c94/rnn.png 579w","sizes":"100vw"},"sources":[{"srcSet":"/static/0ed5ff7ccf0292f6675e6399032d0148/671dd/rnn.webp 579w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5785837651122625}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}}}]},"fields":{"readingTime":{"text":"16 min read"},"layout":"","slug":"/rnn/"}}}]}},"pageContext":{"tag":"nlp"}},
    "staticQueryHashes": ["3170763342","3229353822"]}