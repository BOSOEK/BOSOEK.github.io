{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/loc-attention/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>Attention-Based Models for Speech Recognition Paper Review</h1>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134008551-c7b29862-1cf1-4ffc-98d0-36186a61dd39.png\" alt=\"title\"></p>\n<p><a href=\"http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf\">http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf</a></p>\n<h2>Introduction</h2>\n<p>본 논문에서는 최근 도입된 (당시는 최근이였음) 어텐션 매커니즘이 여러 분야에서 좋은 성능을 보였지만, 음성 인식 분야의 특성을 충분히 반영한 매커니즘은 없었다고 주장한다.</p>\n<p>음성 인식은 NMT 등의 task에 비해 상당히 긴 input sequence를 가진다.<br>\n단어 단위로 수개에서 수십개의 인풋을 가지는 NMT에 비해 음성 인식에서는 20 ~ 40ms로 자른 프레임들이 수백~수천개의 인풋으로 들어가게 된다</p>\n<p>본 논문은 이러한 음성 인식 분야의 특성에 맞게 새로운 어텐션 매커니즘을 제안한다.</p>\n<h1></h1>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134008561-29ab5819-3617-4fa6-8502-c0c61b2874d1.png\" alt=\"2paper\"></p>\n<p>참고로 본 논문은 2015년 당시 음성 인식 분야에서 “Listen, Attend and Spell” 논문과 함께 Innovation이라고 불릴만큼 큰 파장을 준 논문이였다.\n기존 CTC 방식이 압도적이였던 당시에, End-to-End 방식의 포문을 열어준 논문이였기 때문이다.</p>\n<h1></h1>\n<h2>General Framework</h2>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134008574-b70ac298-d51f-4588-9777-d2362600815f.png\" alt=\"base_attention\"></p>\n<p>기본적인 어텐션에 대한 큰 그림이다.<br>\n(본 논문에서는 α는 alignment, g는 glimpse라고 칭함 )</p>\n<p>어떠한 매커니즘을 거쳐서 alignment (α) 를 구하고 나면, alignment와 인코더의 아웃풋들을 곱해서 glimpse를 구한다.</p>\n<h1></h1>\n<h3>Attention의 개념</h3>\n<p>본 논문에서는 나와 있지 않지만 간단하게 개념을 정리하고 가자면, “alignment는 어떤 인코더를 고려해야 할까?”를 수치화해준 벡터이고, glimpse는 수치화 된 alignment와 인코더의 아웃풋들을 각각 곱해서 현재 디코딩에 필요한 인코더의 정보를 압축한 벡터이다. 그리고 glimpse와 디코더의 아웃풋을 고려해서 현재 스텝의 값을 예측한다.</p>\n<h1></h1>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134008583-eff08f57-d0d3-4241-b5cd-39aa3b407a3c.png\" alt=\"alignment\"></p>\n<p>그럼 alignment는 어떻게 구하지?<br>\n란 물음에 답해주는 부분이다.</p>\n<p>특정 방식으로 Score를 구한 뒤, 해당 점수를 Softmax 함수에 넣어서 전체 값을 0~1의 값으로, 전체 합을 1로 만들어 준다.<br>\n=> 각 인코더 아웃풋을 얼마씩 참고할지를 수치화하는 것이다.</p>\n<p>그럼 Score를 구하는 특정 방식은 무엇이냐??</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134008595-d555ad41-1e2d-4aa3-bf30-54a1d6ce45ed.png\" alt=\"score_func\"></p>\n<p>어텐션 스코어를 구하는 방법은 위와 같이 다양하다. 사실 위는 정말 몇 개만 뽑아온 것이다.</p>\n<p>어텐션 매커니즘의 종류는 이 스코어 함수가 무엇이냐에 따라 달라진다.</p>\n<p>그리고, 본 논문은 새로운 “스코어 함수”를 제안한 논문인 것이다.</p>\n<h1></h1>\n<p>본 논문에서는 2가지 어텐션 방식에 주목했다.</p>\n<ol>\n<li>Content-Based Attention</li>\n<li>Location-Based Attention</li>\n</ol>\n<h2>Content-Based Attention</h2>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134008607-25acb65f-6277-4cb1-868a-151c70f3ab18.png\" alt=\"content-based\"></p>\n<p>아마 어텐션을 처음 공부할 때에 대부분 Dot-Product Attention으로 배웠을 것이다.</p>\n<p>해당 스텝의 디코더의 출력과 인코더의 모든 출력들을 내적하여 어텐션 스코어를 구하는 방식이다.</p>\n<p>Content-Based Attention은 Dot-Product보다 조금 더 복잡한 수식으로 점수를 낸다.</p>\n<p>단순한 내적이 아닌, 해당 스텝의 디코더의 출력과 인코더의 모든 출력들에 웨이트를 준다.</p>\n<p>그리고 편향 및 Hyperbolic tangent를 걸어주고, 마지막으로 웨이트를 다시 걸어준다.</p>\n<p>Dot-Product Attention에 비해서는 진보된 방법이지만, Content-Based 방식의 문제점은 시퀀스에서의 자신의 위치에 상관없이 스코어링을 한다는 점이다.<br>\n이를 “similar speech fragments” 문제라고 한다고 한다.</p>\n<h2>Location-Based Attention</h2>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134008621-a79409c0-e98e-4d2e-8af9-7728cfdb867a.png\" alt=\"location-based\"></p>\n<p>그럼 이번에는 Location-Based 방식을 살펴보자.<br>\n이 방식은 alignment 계산시, 해당 스텝 디코어의 출력과, 이전 alignment를 고려해줌으로써, 현재 시퀀스에서 어느 위치인지를 알 수 있게끔 해주는 방식이다.</p>\n<p>하지만 이 방식은 인코더의 아웃풋을 전혀 고려하지 않고, 디코더의 아웃풋만을 가지고 예측하기 때문에 분명한 한계점이 존재한다.</p>\n<h2>Hybrid Attention</h2>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134008627-84e04a75-7ebe-43df-ba03-ba3133215c53.png\" alt=\"hybrid\"></p>\n<p>본 논문은 이러한 2 방식의 어텐션을 적절히 결합한 음성 인식용 어텐션을 제안한다.</p>\n<p>( 해당 어텐션을 Hybrid, Location-Aware, Location-Sensitive 등 여러 이름으로 불린다 )</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134008637-9fbf2912-0ad8-4814-85af-ce79d71b9dbe.png\" alt=\"hybrid-attention\"></p>\n<p>기존 Content-Based 방식에서 약간의 수식만이 추가됐을 뿐이다.</p>\n<p>기존 Content-Based 방식에서 이전 스텝의 alignment를 고려해준다.</p>\n<p>이때 이전 alignment에 웨이트를 주기 이전에, Convolution으로 1xC의 형상에서 KxC의 형상으로 늘려준다. (C: Classfication Number)</p>\n<p>그리고 해당 행렬에 웨이트를 주어서 Content + location 방식을 완성한다.</p>\n<h2>3 Potential Issue</h2>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134008657-11650c58-d6a0-49e1-b91d-20fae065606c.png\" alt=\"Eq6\"></p>\n<p>앞에서 살펴봤던 위의 수식에는 3가지의 이슈가 있다.</p>\n<ol>\n<li>인풋 시퀀스가 길다면, glimpse에는 노이즈가 섞여있을 가능성이 크다.</li>\n</ol>\n<p>만약 인풋 시퀀스가 길다면, 어떤 시점 t에서 멀리 떨어져 있는 t + k라는 시점에서의 음성과는 서로 관련이 없을 것이다. 하지만 Softmax 함수 특성상, 모든 인풋들에 값을 부여한다. 이러한 Softmax의 특성에 의해 많은 관련없는(irrelevant) 인코더의 출력들이 고려될 것이다. 이는 Noise로 작용된다.</p>\n<ol start=\"2\">\n<li>시간 복잡도가 크다.</li>\n</ol>\n<p>인풋 시퀀스의 길이가 L이라고 할 때, 디코더는 매 타임 스텝마다 이 L개의 frame을 고려해주어야 한다. 그리고, 디코딩 길이를 T라 할 때, 위의 과정을 T만큼 반복하게 된다.  이는 O(LT) 라는 높은 시간 복잡도를 만들게 된다.</p>\n<ol start=\"3\">\n<li>Softmax 함수는 Single Vector에만 집중 (focus) 하는 경향이 있다.</li>\n</ol>\n<p>이러한 경향은 top-score를 받은 여러 프레임을 고려할 수 없게 한다.</p>\n<h3><strong>Sharpening</strong> &#x26; <strong>Windowing</strong></h3>\n<p>본 논문은 위의 문제를 간단하게 해결하기 위해 “Sharpening”이라는 개념의 제안했다. Softmax 수식을 약간 수정하는 것이다.<br>\n<img src=\"https://user-images.githubusercontent.com/42150335/134008662-bffb507a-d196-4742-ae3f-474e707e042c.png\" alt=\"sharpening\"><br>\nwhen, β > 1</p>\n<p>본 논문에서는 inverse temperature를 걸어준다고 표현했다.<br>\n위의 수식이 왜 1번 문제를 해결해 주는지에 대해서는 아직 이해를 하지 못하였다.</p>\n<p>그리고 본 논문은 위의 방식이거나, top-k개의 프레임만을 뽑아서 re-normalization을 해주는 방식으로도 해결 가능하다고 말한다.<br>\n하지만, 위의 2 방식 모두 2번째 시간복잡도의 문제는 해결하지 못했으며, 2번째 방법의 경우는 오히려 시간 복잡도를 더 늘리게 된다.</p>\n<p>그리고 Windowing이라는 방법이 나오게 되는데, 이전 alignment의 중간값(median)을 기준으로 윈도우 크기 만큼만 고려해주는 방식이다. 해당 방법은 O(L+T)로 시간 복잡도를 낮춰준다.</p>\n<h1></h1>\n<p>Sharpening은 long-utterance (긴 발화)에서의 퍼포먼스는 개선했지만, 전체적인 퍼포먼스면에서는 좋지 못한 결과로 이어졌다.<br>\n(짧은 발화에서는 퍼포먼스가 별로였다)<br>\n하지만 해당 실험은 최상위 점수를 받은 프레임들을 선택하여 집계하는 방식이 좋을 것이라는 가정을 하도록 만들었다고 한다.</p>\n<h3><strong>Smoothing</strong></h3>\n<p>그래서 나오게 된 방법이 Smoothing 방법이다.<br>\n<img src=\"https://user-images.githubusercontent.com/42150335/134008674-f43e28a6-18ee-4ea4-aa4a-59e5ca666ef9.png\" alt=\"smoothing\"></p>\n<p>위의 식처럼 기존 Softmax 식에 Sigmoid를 추가해준 방식이다.<br>\nSigmoid로 Top-k frame과 아닌 frame들을 구분해주는 방식이라고 나는 이해했다.<br>\n이러한 방식은 다양성을 가져온다고 본 논문은 말한다.</p>\n<h2>Result</h2>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134008689-030fae61-b92c-4a5b-8bb9-61b7a29e097b.png\" alt=\"result\"></p>\n<p>본 논문에서 진행한 실험의 결과이다.<br>\n기본 모델보다는 Convolution을 적용한 모델이 더 좋은 결과를 내었고,<br>\nSmoothing까지 적용한 모델이 최상의 성적을 내었다.</p>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Attention-Based Models for Speech Recognition Paper Review"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134008551-c7b29862-1cf1-4ffc-98d0-36186a61dd39.png","alt":"title"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf"},"children":[{"type":"text","value":"http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Introduction"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 최근 도입된 (당시는 최근이였음) 어텐션 매커니즘이 여러 분야에서 좋은 성능을 보였지만, 음성 인식 분야의 특성을 충분히 반영한 매커니즘은 없었다고 주장한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"음성 인식은 NMT 등의 task에 비해 상당히 긴 input sequence를 가진다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n단어 단위로 수개에서 수십개의 인풋을 가지는 NMT에 비해 음성 인식에서는 20 ~ 40ms로 자른 프레임들이 수백~수천개의 인풋으로 들어가게 된다"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문은 이러한 음성 인식 분야의 특성에 맞게 새로운 어텐션 매커니즘을 제안한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134008561-29ab5819-3617-4fa6-8502-c0c61b2874d1.png","alt":"2paper"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"참고로 본 논문은 2015년 당시 음성 인식 분야에서 “Listen, Attend and Spell” 논문과 함께 Innovation이라고 불릴만큼 큰 파장을 준 논문이였다.\n기존 CTC 방식이 압도적이였던 당시에, End-to-End 방식의 포문을 열어준 논문이였기 때문이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"General Framework"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134008574-b70ac298-d51f-4588-9777-d2362600815f.png","alt":"base_attention"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"기본적인 어텐션에 대한 큰 그림이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n(본 논문에서는 α는 alignment, g는 glimpse라고 칭함 )"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"어떠한 매커니즘을 거쳐서 alignment (α) 를 구하고 나면, alignment와 인코더의 아웃풋들을 곱해서 glimpse를 구한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Attention의 개념"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 나와 있지 않지만 간단하게 개념을 정리하고 가자면, “alignment는 어떤 인코더를 고려해야 할까?”를 수치화해준 벡터이고, glimpse는 수치화 된 alignment와 인코더의 아웃풋들을 각각 곱해서 현재 디코딩에 필요한 인코더의 정보를 압축한 벡터이다. 그리고 glimpse와 디코더의 아웃풋을 고려해서 현재 스텝의 값을 예측한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134008583-eff08f57-d0d3-4241-b5cd-39aa3b407a3c.png","alt":"alignment"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그럼 alignment는 어떻게 구하지?"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n란 물음에 답해주는 부분이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"특정 방식으로 Score를 구한 뒤, 해당 점수를 Softmax 함수에 넣어서 전체 값을 0~1의 값으로, 전체 합을 1로 만들어 준다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n=> 각 인코더 아웃풋을 얼마씩 참고할지를 수치화하는 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그럼 Score를 구하는 특정 방식은 무엇이냐??"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134008595-d555ad41-1e2d-4aa3-bf30-54a1d6ce45ed.png","alt":"score_func"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"어텐션 스코어를 구하는 방법은 위와 같이 다양하다. 사실 위는 정말 몇 개만 뽑아온 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"어텐션 매커니즘의 종류는 이 스코어 함수가 무엇이냐에 따라 달라진다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고, 본 논문은 새로운 “스코어 함수”를 제안한 논문인 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 2가지 어텐션 방식에 주목했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Content-Based Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Location-Based Attention"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Content-Based Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134008607-25acb65f-6277-4cb1-868a-151c70f3ab18.png","alt":"content-based"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"아마 어텐션을 처음 공부할 때에 대부분 Dot-Product Attention으로 배웠을 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"해당 스텝의 디코더의 출력과 인코더의 모든 출력들을 내적하여 어텐션 스코어를 구하는 방식이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Content-Based Attention은 Dot-Product보다 조금 더 복잡한 수식으로 점수를 낸다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"단순한 내적이 아닌, 해당 스텝의 디코더의 출력과 인코더의 모든 출력들에 웨이트를 준다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 편향 및 Hyperbolic tangent를 걸어주고, 마지막으로 웨이트를 다시 걸어준다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Dot-Product Attention에 비해서는 진보된 방법이지만, Content-Based 방식의 문제점은 시퀀스에서의 자신의 위치에 상관없이 스코어링을 한다는 점이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이를 “similar speech fragments” 문제라고 한다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Location-Based Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134008621-a79409c0-e98e-4d2e-8af9-7728cfdb867a.png","alt":"location-based"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그럼 이번에는 Location-Based 방식을 살펴보자."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이 방식은 alignment 계산시, 해당 스텝 디코어의 출력과, 이전 alignment를 고려해줌으로써, 현재 시퀀스에서 어느 위치인지를 알 수 있게끔 해주는 방식이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"하지만 이 방식은 인코더의 아웃풋을 전혀 고려하지 않고, 디코더의 아웃풋만을 가지고 예측하기 때문에 분명한 한계점이 존재한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Hybrid Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134008627-84e04a75-7ebe-43df-ba03-ba3133215c53.png","alt":"hybrid"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문은 이러한 2 방식의 어텐션을 적절히 결합한 음성 인식용 어텐션을 제안한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"( 해당 어텐션을 Hybrid, Location-Aware, Location-Sensitive 등 여러 이름으로 불린다 )"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134008637-9fbf2912-0ad8-4814-85af-ce79d71b9dbe.png","alt":"hybrid-attention"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"기존 Content-Based 방식에서 약간의 수식만이 추가됐을 뿐이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"기존 Content-Based 방식에서 이전 스텝의 alignment를 고려해준다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이때 이전 alignment에 웨이트를 주기 이전에, Convolution으로 1xC의 형상에서 KxC의 형상으로 늘려준다. (C: Classfication Number)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 해당 행렬에 웨이트를 주어서 Content + location 방식을 완성한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"3 Potential Issue"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134008657-11650c58-d6a0-49e1-b91d-20fae065606c.png","alt":"Eq6"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"앞에서 살펴봤던 위의 수식에는 3가지의 이슈가 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"인풋 시퀀스가 길다면, glimpse에는 노이즈가 섞여있을 가능성이 크다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"만약 인풋 시퀀스가 길다면, 어떤 시점 t에서 멀리 떨어져 있는 t + k라는 시점에서의 음성과는 서로 관련이 없을 것이다. 하지만 Softmax 함수 특성상, 모든 인풋들에 값을 부여한다. 이러한 Softmax의 특성에 의해 많은 관련없는(irrelevant) 인코더의 출력들이 고려될 것이다. 이는 Noise로 작용된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{"start":2},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"시간 복잡도가 크다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"인풋 시퀀스의 길이가 L이라고 할 때, 디코더는 매 타임 스텝마다 이 L개의 frame을 고려해주어야 한다. 그리고, 디코딩 길이를 T라 할 때, 위의 과정을 T만큼 반복하게 된다.  이는 O(LT) 라는 높은 시간 복잡도를 만들게 된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{"start":3},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Softmax 함수는 Single Vector에만 집중 (focus) 하는 경향이 있다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이러한 경향은 top-score를 받은 여러 프레임을 고려할 수 없게 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Sharpening"}]},{"type":"text","value":" & "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Windowing"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문은 위의 문제를 간단하게 해결하기 위해 “Sharpening”이라는 개념의 제안했다. Softmax 수식을 약간 수정하는 것이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134008662-bffb507a-d196-4742-ae3f-474e707e042c.png","alt":"sharpening"},"children":[]},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nwhen, β > 1"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 inverse temperature를 걸어준다고 표현했다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n위의 수식이 왜 1번 문제를 해결해 주는지에 대해서는 아직 이해를 하지 못하였다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 본 논문은 위의 방식이거나, top-k개의 프레임만을 뽑아서 re-normalization을 해주는 방식으로도 해결 가능하다고 말한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n하지만, 위의 2 방식 모두 2번째 시간복잡도의 문제는 해결하지 못했으며, 2번째 방법의 경우는 오히려 시간 복잡도를 더 늘리게 된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 Windowing이라는 방법이 나오게 되는데, 이전 alignment의 중간값(median)을 기준으로 윈도우 크기 만큼만 고려해주는 방식이다. 해당 방법은 O(L+T)로 시간 복잡도를 낮춰준다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Sharpening은 long-utterance (긴 발화)에서의 퍼포먼스는 개선했지만, 전체적인 퍼포먼스면에서는 좋지 못한 결과로 이어졌다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n(짧은 발화에서는 퍼포먼스가 별로였다)"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n하지만 해당 실험은 최상위 점수를 받은 프레임들을 선택하여 집계하는 방식이 좋을 것이라는 가정을 하도록 만들었다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Smoothing"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그래서 나오게 된 방법이 Smoothing 방법이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134008674-f43e28a6-18ee-4ea4-aa4a-59e5ca666ef9.png","alt":"smoothing"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 식처럼 기존 Softmax 식에 Sigmoid를 추가해준 방식이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nSigmoid로 Top-k frame과 아닌 frame들을 구분해주는 방식이라고 나는 이해했다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이러한 방식은 다양성을 가져온다고 본 논문은 말한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Result"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134008689-030fae61-b92c-4a5b-8bb9-61b7a29e097b.png","alt":"result"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서 진행한 실험의 결과이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n기본 모델보다는 Convolution을 적용한 모델이 더 좋은 결과를 내었고,"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nSmoothing까지 적용한 모델이 최상의 성적을 내었다."}]}],"data":{"quirksMode":false}},"excerpt":"Attention-Based Models for Speech Recognition Paper Review title http://papers.nips.cc/paper/5847-attention-based-models-for-speech…","fields":{"readingTime":{"text":"11 min read"}},"frontmatter":{"title":"Attention-Based Models for Speech Recognition Paper Review","userDate":"20 January 2020","date":"2020-01-20T10:00:00.000Z","tags":["speech","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/1402657e6eeeaf54425d3d5cf34998c8/0f4e3/loc-attention.png","srcSet":"/static/1402657e6eeeaf54425d3d5cf34998c8/0f4e3/loc-attention.png 681w","sizes":"100vw"},"sources":[{"srcSet":"/static/1402657e6eeeaf54425d3d5cf34998c8/c0309/loc-attention.webp 681w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.4552129221732746}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}]}}]}},"relatedPosts":{"totalCount":20,"edges":[{"node":{"id":"64e39e81-9c08-53ad-967e-f53e0ffd1d51","excerpt":"한국어 Tacotron2 이번 포스팅에서는 Tacotron2 아키텍처로 한국어 TTS 시스템을 만드는 방법에 대해 다루겠습니다. Tacotron2 Tacotron2는 17년 12월 구글이 NATURAL TTS SYNTHESIS BY…","frontmatter":{"title":"한국어 Tacotron2","date":"2021-10-10T10:00:00.000Z"},"fields":{"readingTime":{"text":"11 min read"},"slug":"/korean_tacotron2/"}}},{"node":{"id":"8609f7b7-4942-59fe-bfaa-5f82c648649e","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP_ Generating expressive speech from raw audio/"}}},{"node":{"id":"75998e15-7d74-5d05-af5b-1112437e067d","excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","date":"2021-03-17T10:00:00.000Z"},"fields":{"readingTime":{"text":"3 min read"},"slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/"}}},{"node":{"id":"feb53e83-51f2-5350-ae0e-58157d8dfd22","excerpt":"PORORO Text-To-Speech (TTS) 얼마전에 저희 팀에서 공개한 PORORO: Platform Of neuRal mOdels for natuRal language prOcessing 라이브러리에 제가 공들여만든 TTS…","frontmatter":{"title":"PORORO Text-To-Speech (TTS)","date":"2021-02-16T10:00:00.000Z"},"fields":{"readingTime":{"text":"1 min read"},"slug":"/pororo-tts/"}}},{"node":{"id":"77bed2d4-fc96-5bff-9808-9b9cb45369f3","excerpt":"EMNLP Paper Review: Speech Adaptive Feature Selection for End-to-End Speech Translation (Biao Zhang et al) Incremental Text-to-Speech…","frontmatter":{"title":"EMNLP Paper Review: Speech","date":"2020-12-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/2020 EMNLP Speech Paper Review/"}}}]}},"pageContext":{"slug":"/loc-attention/","prev":{"excerpt":"SpecAugment: 「A Simple Data Augmentation Method for Automatic Speech Recognition」  Review title https://arxiv.org/abs/1904.08779 Abstract…","frontmatter":{"title":"SpecAugment Paper Review","tags":["speech","paper"],"date":"2020-01-12T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAADDElEQVQoz1WT32+TVRzGj3CD3piYjGgwJt6A7oLxB/gPeKGJXngDm47WDtd0INCVIkwHiHPLrN2ENMQwGexXV2hniSS6aeYoWybQjrGub8u2vu/b9+3PUVbFRFn3MX07L3aSk/Oc73Pyyfc5OUeUyxtUxsOwTN+dMFcezjN4N8Jw6AH98xF+8v/BzyOz9EfmGAiHuRINMzg7h3dqjuHZ+1yLRBiZjrD25G+DI/59tm6IE0euIt62Ipo+Q+y3I95xIEzH2b29nj3b6hFHjyAOHEY4DiPMDkT9cYTpEOKkDWH9lPuLWhW4vl42xM3RGRoujWLrD9I0FMDW56f5cpAvbdc45xzkE58X6+UbmIeCHAz4sUwM0OT1Yb41woc/XkfOlrYCA2P32O/x0TAW5OBwAMsPY5ivBjAP3eC8dZAWt4+PRsdoDPgwDQSwfhfk44HrNHr9WPr8aJknWyM72oYR7x7jueYvEKbTbG88xbaGNna8d4K6Fw7x/AfHEE0ORPNJhK0N0eJE2FoRTjvC6eDeUroKLJerHc6EYnQGp+iZmMH9a4je8RA949N4/Hfw9v6G59YUFyYmcU1M45oMcfHuOK7bU7hnJnHd/p1s8WkV+GyzQ9fFX3jd2s2bHR72tvewz37BmHs+/5636t3sc3ZTd8pNraeDNzrd7D16ibr2b6jt7aL2nBspmd8Ebt5h+7c32WE6w8ut3bzS0slrli5etXRRY3Wz+/0OdrWcZZe1g532r3nJ2UVNayc77V9R03aWF0+fZ07Obo1cLD1lJVtEW/0L9fGfpPJrqI9LqKsl1EIJrVIrlFBzayirayjFIkrFL1b9fzaTinw+x8LCArlclkI+h5JcRlNlUimFjK4ZOq1rZPQUy0sJQxdyGfRUatNXyKR1FEVBUVVEJpNBisdZXlkhGoshJeIsxhZ5MD+PlEiwEI0Sk2LEE3HDk+ISj5YeEV2MIsUTRl1WFJJykpSWQhQKeYNcKWi6jqalDHhln06nUVUVXdeR5aRxrrImZRlJiqFpGv+/442N6hf+D1oZ4qIlqS9AAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/a5b61e2900f2360216061fe9e8f8f640/4df46/specaugment.png","srcSet":"/static/a5b61e2900f2360216061fe9e8f8f640/4df46/specaugment.png 620w","sizes":"100vw"},"sources":[{"srcSet":"/static/a5b61e2900f2360216061fe9e8f8f640/cd871/specaugment.webp 620w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5919354838709677}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"20 min read"},"layout":"","slug":"/specaugment/"}},"next":{"excerpt":"LSTM & GRU 본 포스팅을 이해가기 위해서는 아래 글에 대한 이해가 선행되는 것이 좋습니다. RNN (Recurrent Neural Network) LSTM 등장 배경 RNN…","frontmatter":{"title":"LSTM & GRU","tags":["nlp"],"date":"2020-01-24T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABTklEQVQY00WQW0/iUBSF+/9/jQNRZxAyE+IDZGIMRAERoVhBaCm90nN6bt+kOOp62NnJXll75fOcczTSGow5r1gNShqchc/76XQiiiJw4AyYxq8dtVBYA9bas8/7CFMs9gGT8IGNHLOWY1blkLf8CVFJmhRVK47ZgaCY8VY9sUwXTLYrXrMJ23ROVuRIWeM1H61VdGdTuuM+y80VftlhfbjE3/+hVgZ3rgqJihnFF/hpm2ne4cftgNHuhln8i1JkGGX+N1SS7nRC7+43G/+aYNMmSH/ymvSpKoltAq2g0GtmWYtd0cNPOgz+DpmurnneXVHLDCdqvIaRNYbV/p3lfsHhOCdM58TikUy+o5X5CqzUllF4ycOuzX3QYvjSYxzdME/7aNewdHhxHFOWJa6Bah3oD7iYim+5r1mKAqEFaRESJWviJCQ6RiRJQp7n/AOTZMQLmbVhtgAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/385b4880d470b853d530d77243bb4c52/b3525/lstm_gru.png","srcSet":"/static/385b4880d470b853d530d77243bb4c52/2107c/lstm_gru.png 750w,\n/static/385b4880d470b853d530d77243bb4c52/b3525/lstm_gru.png 773w","sizes":"100vw"},"sources":[{"srcSet":"/static/385b4880d470b853d530d77243bb4c52/28c13/lstm_gru.webp 750w,\n/static/385b4880d470b853d530d77243bb4c52/b6f78/lstm_gru.webp 773w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.31565329883570503}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAZCAYAAAAxFw7TAAAACXBIWXMAAAsTAAALEwEAmpwYAAAGs0lEQVQ4yx2RaUzUiRmH/583TRO7csw9HAqIAoKgCMoNw8wwM8AwXDIzMAw4AwyXwzGAXEu5lQEWFZBDwAMC4hm16EZd1911u+12t+mxrkn7wSZtsmm3SdMPTZ4G3uT58n548vu9r2DIjEWXHkfGqRhSTsZy+vhxTicmkZKSQY7WQEVZGXarFbulkua6Zj7qHWbSe5WlpU1urG0z5Z2lo70Xx1knZYUlCDnpxyjQJFJiSKUoJw1dVjIZKckkJSWj1eRQVWbGWWGjqtKBp+U8k955bm0+58tf/4W33/+NnZ3PmfYu4m5qp7zEglBj0VCam0Jpbjrm/GysBVqsRbkUG3MxGvKwlZTiqqyk7mwdvecHmJ+/yc7zb/nx3/9jd/7w/T9YvnaXrvY+KsyVCN5+GxN9lQy2WfHUllBvM1FbXkSdrYwmRyW1lRW0OJ30ezxMjI2zvLDKJ88+5d279/zzX//l5Zt3zFy+SYu7C/OZcoRr003cmm1lY97D1rVutlb62FjsZ/VKP801Ngz6fAx6ExXWKlrc7VwY87J5a507tx9w995zFpa36O4dwelwYcwzIXxyb4qXT2b57OkCX75Y49s3t/nh94/oanUQFxdPdoYKm8lIh7OcPreL8x4PvT1DTIx7GR3x0t0zQI3DRYXFRn6uEWFurIb71wf44tkCf/zNNj/9/Q1PHy4TGhKONvU0w9VG5s6VsdBRyWyXg9HWOloaGmhpamV2fIRmVz0mYwl6Qz55u8Kx7lKGWo1M9pqZG3ewszVEW6OV8NBwes1qlhsLWPNYuNZh5aqnkpl2B+frqzFbKvjt0x3eP7nPp4/u0VPnIj05DWFjqZtWp4a22hzanDl81GxCk3EKdUIsg+VqLtj1XG8vY6PHzoCjlEZrMS3VZopMhdxdXoD3b/e+/dODx1woPIOweXOY/v6z1NUU4LTn0d1WjlaVQkHyCWrzMmkwqrnsKmKtvZyBqmI6y4tosxZRotdxsbmeLxam+e7eFr9buclSfR3CzuNLbGxdZGT0HFfnB/js9TpdnS5sqgQ6i1W0mrKZqdIxU5XDRLWRWqMGzxkd9cV5nNVlcaNNzYDNwHyXjdVeM8L97XFWVgfo76th9koPr17dYHS0HXdhKmPWTIbMKqZsWQyXpnOxXM18TS5jdj3u0lwWx+0s/rIAqyaeCx0mrvYYELY3R1hc7sPTbuPyTBePH80yOtZKd3k26/XZ3HYbWHJkc8WmZcqSxXpTPlN2He4iHV/vdPJ6vYkmcx7DnecY76xD2N4YYm6ukxa3mUsfd3B3e4rxCQ/eDgsrzgzutOUx71AzadPwsS2btYZcJu0ayjUpVJXm0VyWj8WgoTBHRWm+FmHjRj+Tk83U1RbgvehmdWWAgaFGvKMurlSlsOzSMGnJ4LJVzaApjWm7iolKNanHj3Ew5DAHA0I4dCCU6Igo4mNiEV488fLwzijXl3v51YNJnu9c4uGdCV6+WMLr0rPmUjFdkspFfSoThWlcc+moVp8iKCiE+JhoTh+NJiQwmIiwQ8RFRiK8+2aZP3+9yNtvVnn33U3++qdN3v9wl//8+Jyp0SaqMo+w7tZxuy2flQY91doEwkJCiQo/gkKqIEwZyAFlIKFBQcTHRCJ89XSc14/HeHZniO3VHlYvtzJ3oZH5iUYanSakcjkZJyIoSo8jIToCsTKYuLBQavMzKTSoiTkaTfiBA6QkHCch9iiCXnUMVVIkqSePkJUagzEnkSqzCkupCr02jfi4SKTKQHzEcnzFcpKiDtOqP0lH3jGqjWlkq7VkZaSjy0rlUHAgQsqpQ5TkJdBSl8tgTwXnPRZ0+iSORIZzIDSUhoIEBswpFKfF0JybwJZ7965ZnEk6jFjkR4AygKjISNSqTHTabITBzmJmRuz0tFvIykxEHqDkQ5EEuVKBRKEgMTqM640qXg2auNWqo9+cTPaJQ8hkMhQKJQq5nODAQGKio6murkJYnz1Hb3sZYok/fv4iAo4cRhoUjEypJCAwCF+xAm1iBPON2YQdDMTHX4pIIkUu30WGQq4gOCiIqMgIkpOTEe6tdhN/Mpyf7/sZ8gAZobHRyIIOIgsIQKFUIpXKCQ4O4HhUKH5iCQqFHKVSsSeUyiVIZTLkuymDg/bEwvRwLT7iffhJfoGfTISfTIxYKUOyW1kuQyaXIpaK8ReLkO5JZHvJdvf+EtFeK5HIH7FYtIdgM6v4YN8H+Mp88JH64SMX4asQ4y8XI5KJEEtFiCT+iKRiJLJduQSJVIxYJuZDX1/27fdhv88u+/Hx9eH/E3w7u01Rad8AAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"18 min read"},"layout":"","slug":"/lstm_gru/"}},"primaryTag":"speech"}},
    "staticQueryHashes": ["3170763342","3229353822"]}