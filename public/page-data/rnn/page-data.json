{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/rnn/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>RNN (Recurrent Neural Network)</h1>\n<p>본 포스팅을 이해하기 위해서는 피드포워드 네트워크에 대한 이해가 선행되는 것이 좋습니다.</p>\n<h2>RNN의 등장 배경</h2>\n<p>RNN에 대해 알아보기 전에 RNN이라는 놈이 왜 나왔는지 부터 생각해보자.</p>\n<h3>피드포워드 신경망의 문제점</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/134012351-39033340-44ce-435c-a8dc-d3fe1df705a1.png\" width=\"400\">\n<p>피드포워드란 흐름이 단방향인 신경망을 뜻한다. 피드포워드 구조는 구성이 단순하여 구조를 이해하기 쉽고 많은 문제에 응용할 수 있다는 장점이 있지만, 커다란 단점이 하나 있으니 바로 시계열 데이터를 잘 다루지 못한다는 것이다. 즉, 단순한 피드포워드 신경망에서는 시계열 데이터의 성질(패턴)을 충분히 학습할 수 없다. 그래서 순환신경망<sup>Recurrent Neural Network(RNN)</sup>이 등장하게 된다.</p>\n<h2>순환하는 신경망</h2>\n<p>RNN의 특징은 순환하는 경로 (닫힌 경로)가 있다는 것이다. 이 순환 경로를 따라 데이터는 끊임없이 순활할 수 있다. 그리고 데이터가 순환되기 때문에 과거의 정보를 기억하는 동시에 최신 데이터로 갱신될 수 있다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134012485-b71a5fac-4111-4873-bda6-60cf9ad2c1f0.png\" width=\"400\">\n<p>위의 그림처럼 RNN 계층은 순환하는 경로를 포함한다.<br>\n이 순환 경로를 따라 데이터를 계층 안에서 순환시킬 수 있다.</p>\n<p>여기서 Xt는 (X0, X1, …, Xt) 가 RNN 계층에 입력됨을 표현한 것이다.<br>\n그리고 그 입력에 대응하여 (h0, h1, …, ht) 가 출력된다.</p>\n<h2>순환 구조 펼치기</h2>\n<p>RNN의 순환 구조는 피드포워드 구조에서는 볼 수 없던 구조이지만, 이 순환 구조를 펼치면 친숙한 피드포워드와 유사한 신경망으로 변신시킬 수 있다. 위의 그림에서 보듯, RNN 계층의 순환 구조를 펼침으로써 오른쪽으로 진행하는 피드포워드 신경망과 비슷한 구조가 된 것을 볼 수 있다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134012663-ee63b994-77a5-4d37-aaf1-cac8f29435a8.png\" width=\"500\">\n<p>위의 그림에서 보듯, RNN 계층의 순환 구조를 펼침으로써 오른쪽으로 진행하는 피드포워드 신경망과 비슷한 구조가 된 것을 볼 수 있다.</p>\n<p>하지만 RNN에서는 다수의 RNN 계층 모두가 실제로는 ‘같은 계층’인 것이 피드포워드 신경망과는 다르다.</p>\n<p>위의 그림에서 알 수있듯, 각 시각의 RNN 계층은 그 계층으로의 입력과 그 전의 RNN 계층으로부터의 출력을 받는다.</p>\n<p>그리고 이 두 정보를 바탕으로 현 시각의 출력을 계산한다. 이때 수행하는 계산 수식은 다음과 같다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134012757-dcb7a411-532d-43bc-8fc8-4aa5aa0c106f.png\" width=\"300\">  \n<p>앞의 그림에서 보이듯이, RNN은 2개의 입력을 받는다. 그렇기에 각 입력에 대해 2개의 가중치가 있다. 하나는 입력 x를 출력 h로 변환하기 위한 가중치 W<sub>x</sub>이고, 다른 하나는 1개의 RNN 출력을 다음 시각의 출력으로 변환하기 위한 가중치 W<sub>h</sub>이다. 위의 식에서 행렬 곱을 계산하고 그 합을 tanh 함수를 이용해 변환한다.</p>\n<p>즉, 이 식에서 볼 수 있듯이 현재의 출력 h<sub>t</sub>는 한 시각 이전 출력 h<sub>t-1</sub>에 기초해 계산됨을 알 수 있다.</p>\n<p>이러한 구조를 갖고 있기에 RNN 계층을 메모리가 있는 계층이라고 한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">\nRNN의 h는 상태를 기억해 시각이 1 스텝 진행될 때마다 위의 식 형태로 갱신된다.\n이 h를 통상적으로 은닉 상태 (Hidden State) 혹은 은닉 상태 벡터 (Hidden State Vector)라고 한다.</code></pre></div>\n<p>위의 식이 어떻게 동작하는지를 자세히 살펴보기 전에 Hyperbolic tangent에 대해 살펴보고 가자.</p>\n<h2>Hyperbolic tangent</h2>\n<p>해당 부분은 <a href=\"http://taewan.kim/post/tanh_diff/\">http://taewan.kim/post/tanh_diff/</a> 을 참고했습니다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134013283-75af9e82-0dbf-49f1-9e67-1d140a635344.png\" width=\"400\">\n<p>Hyperbolic tangent 함수는 머신러닝에서 자주 사용되는 활성화 함수인 Sigmoid의 대체제로 사용될 수 있는 활성화 함수이다.</p>\n<p>tanh는 sigmoid와 매우 유사한데, tanh와 sigmoid의 차이점은 sigmoid의 출력 범위가 0 ~ 1인 반면 tanh의 출력 범위는 -1 ~ +1 사이라는 점이다.</p>\n<p>Sigmoid와 비교하여 tanh는 출력 범위가 더 넓고 경사면이 큰 범위가 더 크기 때문에 더 빠르게 수렴하여 학습하는 특성이 있다.</p>\n<h3>Hyperbolic tangent의 미분</h3>\n<p>RNN의 역전파를 계산하기 위해 필요한 tanh의 미분은 다음과 같다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134013301-aec980e9-d91c-4dd1-bd4f-447e3d0f160c.png\" width=\"400\">\n<p>이후 RNN의 역전파 (Backpropagation) 에서 해당 수식이 사용된다.</p>\n<h2>RNN 계층의 순전파</h2>\n<p>앞에서 RNN 계층에서의 순전파에 사용되는 수식을 살펴봤다. 1개의 인풋 데이터에 대한 입력과, 한 시각 이전의 Hidden State가 입력된다는 점과 이를 tanh 함수에 넣는다는 점을 기억하자.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134013593-a47a383d-6d50-4ded-9b32-902dafe5f938.png\" width=\"300\">\n<p>해당 수식을 시각화하여 표현하면 다음 그림과 같다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134013617-0a32f7fa-bc1d-4ee3-83d5-8ef3207cd2db.png\" width=\"500\">\n<p>X와 h<sub>prev</sub>를 입력으로 받아 내부적인 계산을 거친 후, h<sub>next</sub>라는 출력을 분기시켜 내놓는다.</p>\n<p>다음 과정을 파이썬 코드로 표현하면 다음과 같다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134013811-6e993d35-2ab7-41a8-b8c2-2f0f406aa06f.png\" width=\"400\">\n<p>순전파는 위의 그림만 머리에 넣었다면 쉽게 이해할 수 있을 것이다. 그러면 이제 RNN에서의 역전파 (Backpropagation) 를 살펴보자.</p>\n<h2>BPTT (Backpropagation Through Time)</h2>\n<p>앞에서 봤듯이 RNN 계층은 가로로 펼친 신경망으로 간주할 수 있다. 따라서 RNN의 학습도 보통의 신경망과 같은 순서로 진행할 수 있다. 이를 그림으로 보면 다음과 같다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134013934-a141a438-4184-4e84-9362-ba0759c238ec.png\" width=\"500\">\n<p>위의 그림에서 보듯, 순환 구조를 펼친 후의 RNN에는 피드포워드 오차역전파법을 적용할 수 있다. 즉, 먼저 순전파를 진행하고 이어서 역전파를 수행하여 원하는 기울기<sup>gradient</sup>를 구할 수 있다.</p>\n<p>여기서의 오차역전파법은 ‘시간 방향’으로 펼친 신경망의 오차역전파법이라는 뜻으로 BPTT<sup>Backpropagation Through Time</sup>이라고 한다.</p>\n<p>이 BPTT를 이용하면 RNN을 학습할 수 있다. 하지만 위의 BPTT를 그대로 적용하게 되면 문제가 하나 있다.</p>\n<p>바로 긴 시계열 데이터를 학습할 때 시계열 데이터의 크기가 커지는 것에 비례하여 BPTT가 소키하는 컴퓨팅 자원도 증가하기 때문이다. 또한 시간 크기가 커지면 역전파 시에 기울기 값이 조금씩 작아져서 0에 가까워지는 <strong>Vanishing Gradient Problem</strong>도 발생하게 된다.</p>\n<h2>Truncated BPTT</h2>\n<p>이러한 문제를 해결하기 위해 나온 방법이 <strong>Truncated BPTT</strong>이다.\n시간축 방향으로 길어진 신경망을 적당한 지점에서 잘라내 여러 개로 만든다는 아이디어다.\n그리고 이 잘라낸 작은 블록 단위로 오차역전파법을 수행한다.\n이것이 <strong>Truncated BPTT</strong>라는 기법이다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134014257-438b6043-6906-4302-8f04-7ac0df6d7b3e.png\" width=\"500\">\n<p>여기서 주의할 점은 역전파의 연결만 끊고, 순전파의 연결은 반드시 그대로 유지해야한다는 점이다. 즉, 순전파의 흐름은 끊어지지 않고 전파되어야 한다.</p>\n<h2>Backpropagation Review</h2>\n<p>이제 본격적인 RNN 계층의 역전파에 들어가기 전에, 기본적으로 필요한 역전파에 대한 개념을 간단히 살펴보자.</p>\n<h3>덧셈 노드의 역전파</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/134014470-79b927d1-e970-45c2-8392-f77bbf97c155.png\" width=\"400\">  \n<p>덧셈 노드의 역전파는 상류에서 전해진 미분을 그대로 흘려보낸다.<br>\nz = x + y 라는 식이 있을 때 역전파를 생각해보면 다음과 같다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">σz/σx = 1, \nσz/σy = 1 </code></pre></div>\n<p>즉, 상류에서 전해진 미분에 x1 을 하는 것이므로 그대로 흘려보내는 것과 같다.</p>\n<h3>곱셈 노드의 역전파</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/134014479-759fc3eb-dbe2-4d9e-8043-77a9dfb9fde7.png\" width=\"500\">\n<p>곱셈 노드의 역전파는 상류의 값에 순전파 때의 입력 신호들을 ‘서로 바꾼 값’을 곱해서 하류로 보낸다.\nz = xy 라는 식의 역전파를 생각해보면 다음과 같다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">σz/σx = y, \nσz/σy = x </code></pre></div>\n<p>전파 때의 입력신호들을 서로 바꾼 값이 되는 것을 확인할 수 있다. 그러므로 상류에서 들어온 신호에 순전파 때의 입력 신호들을 서로 바꾼 값을 곱해서 하류로 흘려보내면 된다.</p>\n<p>여기서 그냥 곱셈인 경우와 행렬 곱셈은 다르지 않나? 라고 생각할 수 있는데, 행렬도 어차피 각 요소간의 곱으로 이루어진 집합이기 때문에 결과적으로는 같은 결과를 보인다.</p>\n<h2>Backpropagation in RNN</h2>\n<p>그럼 이제 본격적으로 RNN에서의 역전파를 살펴보자.</p>\n<h3>RNN 계층의 역전파  -  (1) dh<sub>next</sub></h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/134014902-01635d08-73fb-47e2-8cb4-7e43a114cfc6.png\" width=\"500\">\n<p>먼저 다음 층의 역전파인 dhnext를 넘겨받는다. 피드포워드 신경망에서와 마찬가지로 순전파때의 반대 방향으로 역전파가 진행되는 점 주의</p>\n<h3>RNN 계층의 역전파  -  (2) d<sub>tanh</sub></h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/134015092-d5405345-bf7a-44dd-9ced-58a225b1abdb.png\" width=\"500\">\n<p>먼저 tanh의 미분결과가 1 - tanh2(x)이였던 점을 기억하자.<br>\n다음으로 위에서 흘러온 dh<sub>next</sub>와 1 - tanh2(x)값을 곱해서 d<sub>tanh</sub>를 계산한다.</p>\n<h3>RNN 계층의 역전파  -  (3) 덧셈 노드</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/134015263-787b2f3b-4299-4a48-afe3-97a72994db08.png\" width=\"500\">\n<p>앞에서 간단히 살펴봤듯이 덧셈 노드의 경우 역전파를 그대로 흘려보낸다.<br>\n그러므로 db를 제외한 역전파는 dtanh를 그대로 흘려보내주면 되므로 따로 계산하지 않는다.<br>\n여기서는 미니배치 단위 학습을 고려해서 코드를 작성하므로 db는 NxH 형상을 가진다.<br>\n그러므로 편향 (b) 의 역전파는 데이터를 단위로 한 축인 axis=0의 합으로 계산한다.</p>\n<h3>RNN 계층의 역전파  -  (4) 곱셈 노드</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/134015383-601a04ce-3541-41d1-8208-1155b9503f6b.png\" width=\"500\">\n<p>역시 앞에서 간단히 살펴봤듯이 곱셈 노드의 경우 상류에서 들어온 값에 순전파 때의 입력 신호들을 서로 바꾼 값을 곱해준다.<br>\n해당 법칙에 따라 나머지 모든 역전파를 계산한다.</p>\n<h2>Time RNN</h2>\n<img src=\"https://user-images.githubusercontent.com/42150335/134015580-32f33314-f54f-44fb-bda5-d04e9caa9532.png\" width=\"600\">\n<p>앞에서까지는 하나의 RNN 계층에서 일어나는 순전파 및 역전파에 대해 살펴봤다.<br>\n이제 RNN 계층 T개를 연결한 신경망을 완성해보자.<br>\n이렇게 T개의 RNN을 연결한 신경망을 TimeRNN이라고 부를 것이다.<br>\n그리고 이 구현에서는 Truncated BPTT로 구현한다.</p>\n<h3>Time RNN의 순전파</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/134015697-1fa6975c-e316-4113-b39f-dac65917eb6d.png\" width=\"600\">\n<p>순전파는 아래로부터 T개의 입력데이터인 xs를 입력으로 받는다.<br>\n미니배치 처리까지 고려했을 때의 xs의 형상은 NxTxD가 된다.<br>\n(N : 미니배치 수, T : 시계열 데이터 수, D : 입력벡터 차원 수)</p>\n<p>앞에서 각 RNN 층에서의 순전파를 구현해놨기 때문에 이를 적당히 이어주기만 하면 된다.<br>\nHidden State인 h는 처음 호출 시 영행렬로 초기화를 하고, 시계열 데이터의 수만큼 for문을 돌면서Hidden State를 업데이트 한다.<br>\n그리고 이 Hidden State T개의 집합인 hs를 반환한다.</p>\n<h3>Time RNN의 역전파</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/134015858-d773df4c-8c81-4f13-bd92-db1c2f843121.png\" width=\"500\">\n<p>RNN은 순전파시에 출력이 2개로 분기되었던 점을 떠올리자.<br>\n이렇게 순전파시 분기한 경우, 역전파에서는 각 기울기가 합산되어야 한다.<br>\n따라서 역전파 시 RNN 계층에서는 기울기 (dh<sub>t</sub>, dh<sub>next</sub>)가 한산되어야 한다.</p>\n<p>각 RNN 계층에서 역전파를 이미 구현해놨기 때문에 위 사항만 주의하여 적절하게 이어주면 된다.</p>\n<p>추가적으로 주의할 점은 Truncated BPTT 방식이기 때문에 처음 dh는 0으로 시작된다는 점이다.</p>\n<hr>\n<p>각 시각의 기울기인 dx를 모아서 dxs에 저장하고, 가중치 매개변수 역시 각 RNN 계층의 가중치 기울기를 합산하여 최종 결과를 멤버 변수 self.grads에 덮어쓴다.</p>\n<p>이상으로 RNN의 등장배경부터 개념 및 순전파 역전파 등에 대해서 알아봤다.<br>\n다음에는 해당 RNN을 개선한 LSTM에 대해 알아보자.</p>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"RNN (Recurrent Neural Network)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 포스팅을 이해하기 위해서는 피드포워드 네트워크에 대한 이해가 선행되는 것이 좋습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"RNN의 등장 배경"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"RNN에 대해 알아보기 전에 RNN이라는 놈이 왜 나왔는지 부터 생각해보자."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"피드포워드 신경망의 문제점"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134012351-39033340-44ce-435c-a8dc-d3fe1df705a1.png","width":400},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"피드포워드란 흐름이 단방향인 신경망을 뜻한다. 피드포워드 구조는 구성이 단순하여 구조를 이해하기 쉽고 많은 문제에 응용할 수 있다는 장점이 있지만, 커다란 단점이 하나 있으니 바로 시계열 데이터를 잘 다루지 못한다는 것이다. 즉, 단순한 피드포워드 신경망에서는 시계열 데이터의 성질(패턴)을 충분히 학습할 수 없다. 그래서 순환신경망"},{"type":"element","tagName":"sup","properties":{},"children":[{"type":"text","value":"Recurrent Neural Network(RNN)"}]},{"type":"text","value":"이 등장하게 된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"순환하는 신경망"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"RNN의 특징은 순환하는 경로 (닫힌 경로)가 있다는 것이다. 이 순환 경로를 따라 데이터는 끊임없이 순활할 수 있다. 그리고 데이터가 순환되기 때문에 과거의 정보를 기억하는 동시에 최신 데이터로 갱신될 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134012485-b71a5fac-4111-4873-bda6-60cf9ad2c1f0.png","width":400},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 그림처럼 RNN 계층은 순환하는 경로를 포함한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이 순환 경로를 따라 데이터를 계층 안에서 순환시킬 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"여기서 Xt는 (X0, X1, …, Xt) 가 RNN 계층에 입력됨을 표현한 것이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그리고 그 입력에 대응하여 (h0, h1, …, ht) 가 출력된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"순환 구조 펼치기"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"RNN의 순환 구조는 피드포워드 구조에서는 볼 수 없던 구조이지만, 이 순환 구조를 펼치면 친숙한 피드포워드와 유사한 신경망으로 변신시킬 수 있다. 위의 그림에서 보듯, RNN 계층의 순환 구조를 펼침으로써 오른쪽으로 진행하는 피드포워드 신경망과 비슷한 구조가 된 것을 볼 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134012663-ee63b994-77a5-4d37-aaf1-cac8f29435a8.png","width":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 그림에서 보듯, RNN 계층의 순환 구조를 펼침으로써 오른쪽으로 진행하는 피드포워드 신경망과 비슷한 구조가 된 것을 볼 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"하지만 RNN에서는 다수의 RNN 계층 모두가 실제로는 ‘같은 계층’인 것이 피드포워드 신경망과는 다르다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 그림에서 알 수있듯, 각 시각의 RNN 계층은 그 계층으로의 입력과 그 전의 RNN 계층으로부터의 출력을 받는다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 이 두 정보를 바탕으로 현 시각의 출력을 계산한다. 이때 수행하는 계산 수식은 다음과 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134012757-dcb7a411-532d-43bc-8fc8-4aa5aa0c106f.png","width":300},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"앞의 그림에서 보이듯이, RNN은 2개의 입력을 받는다. 그렇기에 각 입력에 대해 2개의 가중치가 있다. 하나는 입력 x를 출력 h로 변환하기 위한 가중치 W"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"x"}]},{"type":"text","value":"이고, 다른 하나는 1개의 RNN 출력을 다음 시각의 출력으로 변환하기 위한 가중치 W"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"h"}]},{"type":"text","value":"이다. 위의 식에서 행렬 곱을 계산하고 그 합을 tanh 함수를 이용해 변환한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"즉, 이 식에서 볼 수 있듯이 현재의 출력 h"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"text","value":"는 한 시각 이전 출력 h"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t-1"}]},{"type":"text","value":"에 기초해 계산됨을 알 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이러한 구조를 갖고 있기에 RNN 계층을 메모리가 있는 계층이라고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"className":["language-text"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"\nRNN의 h는 상태를 기억해 시각이 1 스텝 진행될 때마다 위의 식 형태로 갱신된다.\n이 h를 통상적으로 은닉 상태 (Hidden State) 혹은 은닉 상태 벡터 (Hidden State Vector)라고 한다."}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 식이 어떻게 동작하는지를 자세히 살펴보기 전에 Hyperbolic tangent에 대해 살펴보고 가자."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Hyperbolic tangent"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"해당 부분은 "},{"type":"element","tagName":"a","properties":{"href":"http://taewan.kim/post/tanh_diff/"},"children":[{"type":"text","value":"http://taewan.kim/post/tanh_diff/"}]},{"type":"text","value":" 을 참고했습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134013283-75af9e82-0dbf-49f1-9e67-1d140a635344.png","width":400},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Hyperbolic tangent 함수는 머신러닝에서 자주 사용되는 활성화 함수인 Sigmoid의 대체제로 사용될 수 있는 활성화 함수이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"tanh는 sigmoid와 매우 유사한데, tanh와 sigmoid의 차이점은 sigmoid의 출력 범위가 0 ~ 1인 반면 tanh의 출력 범위는 -1 ~ +1 사이라는 점이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Sigmoid와 비교하여 tanh는 출력 범위가 더 넓고 경사면이 큰 범위가 더 크기 때문에 더 빠르게 수렴하여 학습하는 특성이 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Hyperbolic tangent의 미분"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"RNN의 역전파를 계산하기 위해 필요한 tanh의 미분은 다음과 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134013301-aec980e9-d91c-4dd1-bd4f-447e3d0f160c.png","width":400},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이후 RNN의 역전파 (Backpropagation) 에서 해당 수식이 사용된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"RNN 계층의 순전파"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"앞에서 RNN 계층에서의 순전파에 사용되는 수식을 살펴봤다. 1개의 인풋 데이터에 대한 입력과, 한 시각 이전의 Hidden State가 입력된다는 점과 이를 tanh 함수에 넣는다는 점을 기억하자."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134013593-a47a383d-6d50-4ded-9b32-902dafe5f938.png","width":300},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"해당 수식을 시각화하여 표현하면 다음 그림과 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134013617-0a32f7fa-bc1d-4ee3-83d5-8ef3207cd2db.png","width":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"X와 h"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"prev"}]},{"type":"text","value":"를 입력으로 받아 내부적인 계산을 거친 후, h"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"next"}]},{"type":"text","value":"라는 출력을 분기시켜 내놓는다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"다음 과정을 파이썬 코드로 표현하면 다음과 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134013811-6e993d35-2ab7-41a8-b8c2-2f0f406aa06f.png","width":400},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"순전파는 위의 그림만 머리에 넣었다면 쉽게 이해할 수 있을 것이다. 그러면 이제 RNN에서의 역전파 (Backpropagation) 를 살펴보자."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"BPTT (Backpropagation Through Time)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"앞에서 봤듯이 RNN 계층은 가로로 펼친 신경망으로 간주할 수 있다. 따라서 RNN의 학습도 보통의 신경망과 같은 순서로 진행할 수 있다. 이를 그림으로 보면 다음과 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134013934-a141a438-4184-4e84-9362-ba0759c238ec.png","width":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 그림에서 보듯, 순환 구조를 펼친 후의 RNN에는 피드포워드 오차역전파법을 적용할 수 있다. 즉, 먼저 순전파를 진행하고 이어서 역전파를 수행하여 원하는 기울기"},{"type":"element","tagName":"sup","properties":{},"children":[{"type":"text","value":"gradient"}]},{"type":"text","value":"를 구할 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"여기서의 오차역전파법은 ‘시간 방향’으로 펼친 신경망의 오차역전파법이라는 뜻으로 BPTT"},{"type":"element","tagName":"sup","properties":{},"children":[{"type":"text","value":"Backpropagation Through Time"}]},{"type":"text","value":"이라고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이 BPTT를 이용하면 RNN을 학습할 수 있다. 하지만 위의 BPTT를 그대로 적용하게 되면 문제가 하나 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"바로 긴 시계열 데이터를 학습할 때 시계열 데이터의 크기가 커지는 것에 비례하여 BPTT가 소키하는 컴퓨팅 자원도 증가하기 때문이다. 또한 시간 크기가 커지면 역전파 시에 기울기 값이 조금씩 작아져서 0에 가까워지는 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Vanishing Gradient Problem"}]},{"type":"text","value":"도 발생하게 된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Truncated BPTT"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이러한 문제를 해결하기 위해 나온 방법이 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Truncated BPTT"}]},{"type":"text","value":"이다.\n시간축 방향으로 길어진 신경망을 적당한 지점에서 잘라내 여러 개로 만든다는 아이디어다.\n그리고 이 잘라낸 작은 블록 단위로 오차역전파법을 수행한다.\n이것이 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Truncated BPTT"}]},{"type":"text","value":"라는 기법이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134014257-438b6043-6906-4302-8f04-7ac0df6d7b3e.png","width":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"여기서 주의할 점은 역전파의 연결만 끊고, 순전파의 연결은 반드시 그대로 유지해야한다는 점이다. 즉, 순전파의 흐름은 끊어지지 않고 전파되어야 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Backpropagation Review"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이제 본격적인 RNN 계층의 역전파에 들어가기 전에, 기본적으로 필요한 역전파에 대한 개념을 간단히 살펴보자."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"덧셈 노드의 역전파"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134014470-79b927d1-e970-45c2-8392-f77bbf97c155.png","width":400},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"덧셈 노드의 역전파는 상류에서 전해진 미분을 그대로 흘려보낸다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nz = x + y 라는 식이 있을 때 역전파를 생각해보면 다음과 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"className":["language-text"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"σz/σx = 1, \nσz/σy = 1 "}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"즉, 상류에서 전해진 미분에 x1 을 하는 것이므로 그대로 흘려보내는 것과 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"곱셈 노드의 역전파"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134014479-759fc3eb-dbe2-4d9e-8043-77a9dfb9fde7.png","width":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"곱셈 노드의 역전파는 상류의 값에 순전파 때의 입력 신호들을 ‘서로 바꾼 값’을 곱해서 하류로 보낸다.\nz = xy 라는 식의 역전파를 생각해보면 다음과 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"className":["language-text"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"σz/σx = y, \nσz/σy = x "}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"전파 때의 입력신호들을 서로 바꾼 값이 되는 것을 확인할 수 있다. 그러므로 상류에서 들어온 신호에 순전파 때의 입력 신호들을 서로 바꾼 값을 곱해서 하류로 흘려보내면 된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"여기서 그냥 곱셈인 경우와 행렬 곱셈은 다르지 않나? 라고 생각할 수 있는데, 행렬도 어차피 각 요소간의 곱으로 이루어진 집합이기 때문에 결과적으로는 같은 결과를 보인다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Backpropagation in RNN"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그럼 이제 본격적으로 RNN에서의 역전파를 살펴보자."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"RNN 계층의 역전파  -  (1) dh"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"next"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134014902-01635d08-73fb-47e2-8cb4-7e43a114cfc6.png","width":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"먼저 다음 층의 역전파인 dhnext를 넘겨받는다. 피드포워드 신경망에서와 마찬가지로 순전파때의 반대 방향으로 역전파가 진행되는 점 주의"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"RNN 계층의 역전파  -  (2) d"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"tanh"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134015092-d5405345-bf7a-44dd-9ced-58a225b1abdb.png","width":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"먼저 tanh의 미분결과가 1 - tanh2(x)이였던 점을 기억하자."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n다음으로 위에서 흘러온 dh"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"next"}]},{"type":"text","value":"와 1 - tanh2(x)값을 곱해서 d"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"tanh"}]},{"type":"text","value":"를 계산한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"RNN 계층의 역전파  -  (3) 덧셈 노드"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134015263-787b2f3b-4299-4a48-afe3-97a72994db08.png","width":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"앞에서 간단히 살펴봤듯이 덧셈 노드의 경우 역전파를 그대로 흘려보낸다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그러므로 db를 제외한 역전파는 dtanh를 그대로 흘려보내주면 되므로 따로 계산하지 않는다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n여기서는 미니배치 단위 학습을 고려해서 코드를 작성하므로 db는 NxH 형상을 가진다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그러므로 편향 (b) 의 역전파는 데이터를 단위로 한 축인 axis=0의 합으로 계산한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"RNN 계층의 역전파  -  (4) 곱셈 노드"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134015383-601a04ce-3541-41d1-8208-1155b9503f6b.png","width":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"역시 앞에서 간단히 살펴봤듯이 곱셈 노드의 경우 상류에서 들어온 값에 순전파 때의 입력 신호들을 서로 바꾼 값을 곱해준다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n해당 법칙에 따라 나머지 모든 역전파를 계산한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Time RNN"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134015580-32f33314-f54f-44fb-bda5-d04e9caa9532.png","width":600},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"앞에서까지는 하나의 RNN 계층에서 일어나는 순전파 및 역전파에 대해 살펴봤다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이제 RNN 계층 T개를 연결한 신경망을 완성해보자."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이렇게 T개의 RNN을 연결한 신경망을 TimeRNN이라고 부를 것이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그리고 이 구현에서는 Truncated BPTT로 구현한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Time RNN의 순전파"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134015697-1fa6975c-e316-4113-b39f-dac65917eb6d.png","width":600},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"순전파는 아래로부터 T개의 입력데이터인 xs를 입력으로 받는다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n미니배치 처리까지 고려했을 때의 xs의 형상은 NxTxD가 된다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n(N : 미니배치 수, T : 시계열 데이터 수, D : 입력벡터 차원 수)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"앞에서 각 RNN 층에서의 순전파를 구현해놨기 때문에 이를 적당히 이어주기만 하면 된다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nHidden State인 h는 처음 호출 시 영행렬로 초기화를 하고, 시계열 데이터의 수만큼 for문을 돌면서Hidden State를 업데이트 한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그리고 이 Hidden State T개의 집합인 hs를 반환한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Time RNN의 역전파"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134015858-d773df4c-8c81-4f13-bd92-db1c2f843121.png","width":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"RNN은 순전파시에 출력이 2개로 분기되었던 점을 떠올리자."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이렇게 순전파시 분기한 경우, 역전파에서는 각 기울기가 합산되어야 한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n따라서 역전파 시 RNN 계층에서는 기울기 (dh"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"text","value":", dh"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"next"}]},{"type":"text","value":")가 한산되어야 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"각 RNN 계층에서 역전파를 이미 구현해놨기 때문에 위 사항만 주의하여 적절하게 이어주면 된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"추가적으로 주의할 점은 Truncated BPTT 방식이기 때문에 처음 dh는 0으로 시작된다는 점이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"각 시각의 기울기인 dx를 모아서 dxs에 저장하고, 가중치 매개변수 역시 각 RNN 계층의 가중치 기울기를 합산하여 최종 결과를 멤버 변수 self.grads에 덮어쓴다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이상으로 RNN의 등장배경부터 개념 및 순전파 역전파 등에 대해서 알아봤다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n다음에는 해당 RNN을 개선한 LSTM에 대해 알아보자."}]}],"data":{"quirksMode":false}},"excerpt":"RNN (Recurrent Neural Network) 본 포스팅을 이해하기 위해서는 피드포워드 네트워크에 대한 이해가 선행되는 것이 좋습니다. RNN의 등장 배경 RNN에 대해 알아보기 전에 RNN…","fields":{"readingTime":{"text":"16 min read"}},"frontmatter":{"title":"RNN (Recurrent Neural Network)","userDate":"26 December 2019","date":"2019-12-26T10:00:00.000Z","tags":["nlp"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/0ed5ff7ccf0292f6675e6399032d0148/a0c94/rnn.png","srcSet":"/static/0ed5ff7ccf0292f6675e6399032d0148/a0c94/rnn.png 579w","sizes":"100vw"},"sources":[{"srcSet":"/static/0ed5ff7ccf0292f6675e6399032d0148/671dd/rnn.webp 579w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5785837651122625}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}]}}]}},"relatedPosts":{"totalCount":30,"edges":[{"node":{"id":"06ac0e32-0688-50f0-810d-134ef8b168ab","excerpt":"Decoding Strategy (디코딩 전략) 이번 포스팅에서는 자연어처리 모델의 디코딩 전략에 관해서 다뤄보려고 합니다. 디코딩이란 말처럼 디코딩은 디코더에서\n수행하는 작업입니다. 즉, BERT와 같은 인코더 모델에서 사용하는게 아니라 GPT…","frontmatter":{"title":"Decoding Strategy (디코딩 전략)","date":"2022-01-15T10:00:00.000Z"},"fields":{"readingTime":{"text":"9 min read"},"slug":"/generate/"}}},{"node":{"id":"db36f120-4fb0-5bf7-af53-16447fe6cdd4","excerpt":"Generation with Retrieval 이번에 딥마인드에서 RETRO(Retrieval-Enhanced Transformer) 라는 모델을 내놓았습니다. 문서 retrieval + GPT 기반 모델인데,\n7B 모델임에도 불구하고 2…","frontmatter":{"title":"Generation with Retrieval","date":"2022-01-04T23:00:00.000Z"},"fields":{"readingTime":{"text":"6 min read"},"slug":"/fid_and_rag/"}}},{"node":{"id":"3b4040eb-d53d-5064-beec-cfbf7a7a0fe2","excerpt":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems Paper Review Paper: https://aclanthology.org/2021.naacl-main.12…","frontmatter":{"title":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems Paper Review","date":"2021-12-18T10:00:00.000Z"},"fields":{"readingTime":{"text":"2 min read"},"slug":"/bert_fp/"}}},{"node":{"id":"78976688-33d9-53c4-8489-5099082b9972","excerpt":"GPT (Generative Pre-trained Transformer) 1 gpt1 먼저 알아보고, gpt2에 대해 알아보겠습니다. GPT1 Improving Language Understanding by Generative Pre-Training…","frontmatter":{"title":"GPT (Generative Pre-trained Transformer)","date":"2021-11-23T11:00:00.000Z"},"fields":{"readingTime":{"text":"13 min read"},"slug":"/gpt/"}}},{"node":{"id":"ad5b0c9b-8199-5f10-bfc9-6bb05942e164","excerpt":"Large Scale LM (2) Distributed Programming (작성중) 이 자료는 [해당 link…","frontmatter":{"title":"Large Scale LM (2) Distributed Programming","date":"2021-11-22T11:00:00.000Z"},"fields":{"readingTime":{"text":"17 min read"},"slug":"/big-model2/"}}}]}},"pageContext":{"slug":"/rnn/","prev":{"excerpt":"네이버 2019 해커톤 - Speech 결선진출 네이버  2019 해커톤 - Speech 대회 예선전에서 100 팀 중 11위를 기록하며 예선전을 통과했다 !! 오늘 아침까지만 해도 10위여서 Top 1…","frontmatter":{"title":"네이버 2019 해커톤 - Speech 결선진출","tags":["record"],"date":"2019-11-21T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAbCAYAAAB836/YAAAACXBIWXMAAAsTAAALEwEAmpwYAAAHHklEQVRIxy2VeVDVhxHHf380nek0f3R6j/FIE080YFRQUFFB8UCqVsEYr6io1cRRUFAe7/rd7+bxgPd4PBAwIiJGBBS5LCIyWqMxKpeKJ15ptUkP06aZ6acD5o+d2d3Z+cx+d3ZnhdaTh2k7fZQbl87y9O4NXgz08c2LAb779iX/+89L/vXVPXo+b6P600LKgj6KCrzke11UlBURCvgIFOSSkLCQN378U370xk8QPj9XT8+VNrqvtPP8YR//fvUN8D3f//cVd3qvcfzIQdw2CU004nGoOHQFTZHIz/XgsKsoshVRtJCamkp8fAJC39V2bn/ZwaNbXwDf8e0/X3D9aifVlaX4fS48Lhtuh4bTpuJy6DhsGoos4rDrKJKIMTuLzIx0TEYDxmwDwuPb1/j6cR93rnfSWH+MUCAHj1PD47aT63XhctnQdRVVldE0BVVVkEQRWbFgthjINmZhtVgwGLLYv38/wrWLrVQdCqJL2ZiMB1BVEV2T0DUZm01FUaQhSVarBVEUkSQZSZYo8lQSch8jR/eze9cnaLrKylXJCLKYjeHA3iGY0XQAozFryDebjYiiFVkWESURWVbRdDs2uxO7w0ld/k3OFf2N5sAD3HuD1Psv4913HMFqzcZkzsJoPIDBkElWVuYQ9DXQ8hooSyiqhm6zY3e68HoC1Pq6aMi7T7N/gM6yr7lQ/FeuHn6FIEmDcsxYLSYsFhNmswnLYGz9IS9ZsAxKHexQs6HZ7HgcQRoC/TQH79MUuM+Z/HucDQ5wLvQUQdcUJNmKVTIjDq6AJCErCrKiDs3MIlowD85PkpFlDbNkxqkU0nLwEa3lD2gtfkCT/z6thY84V/wMYW96Goqm4tePUKgfR1LUIZM1DVH+wVc1TFYTRosJXfJQYmukveox56sHuFz/nC+b/sKlqmecK3qKEPHeRIpt9dTn9uHdW4dpvwO714PN7cHh9ZKZkcG6NcnYZR9lziZOeK5Tm9NLTU4vJ329NBX1c77iEe1lj18DtyWncSL3C2q8XexZ42R2+HwsokJ+cYj8YAleKcS6pE2U6Kc4nXeHGs8NanNucsLdQ42njxp3L7XePhoK7tIafIzQGnpAXV43Vc6rWLYWEzl6NvHRCykpOMGpYBfNhfc5E+gfqqnNvUltbhenfD3UenvwZ5+lyNLJQfnPlIgXqLRfQzhd0EuF7SKHlE78hkaS47czcWQEaR8qtJU84VR+L/V5fdT5eqnzDUK7qc/roTF4H+UjkT2LV7IzcTOJUcuInbQI4U+l96jJu0a52sGp0m6ktCBR42KJDU+gQu+gsfAeDYFbNIfuUOvtpil0m+bQbVqLntDgPs++hFmkz5+KtiqOHXFRCB1HHlHrv8GpshvUFV/HmV7FwsiVhI+KYPcHCu2fPqG1vJ/m4n5aSvq51vycK3XPOVv4kAvlLziwaguGxdMpSV1G1a6VCJ1Hn3Cl9SmHHR2EzC34Mk+zfEYSC8aPYUFEPEddV+ioekx75UMu1T7hfOUAFyqf0VY8wKWKv3Mx0EFL+jpO7EihKXMDQm3RZRqLbhE0NeHLrMNvaGbT/HUsDx/HzLdHcGCtyOW6l3R89pDT/tucKbxHa/ETLh5+yaUj/6Ct9Cu2xSaQHjMRLTEWQZOdVOVcpFRsQ99dSZ6hgb0bdZImT2JJ2O9YEj6FavdVGoN3qc/v47T/Fi2hh3j2nsC52UR74CI7V6SzIW4GW2KmIQzeqUMvoFhsJje7hkKtiR3JmUSPGkFK5ETixwxDTHVwruw5dXk9nMzt4pDWycKoFPbHRdHts7M9fhGLIyawNiYSQVUdWDUNXfJj33cM606dLdGRLJgxizVzZ5My5V3WzpxHg7+PltK71Ae7UNLKiRo3gwVjxvBJXDQZiZEsGjeC2GG/RLC7PKgON5rTjW4vwLj6D6wY+w5JcxaQMm8eqbPfZ3P0BFwf59J8eICc7M9Iik4i7Fe/ZvKw3xIz8jckThrF1jmTWT11PILT60V3edBdXlx5fsx70lg1P4HosaOZ+fZwUudNx7ZmPvb16zBsdLF0yjxmjXqLCT97k9/PmsmquDmkRIWxfvpEdsdPQ7C5cxA1B5LuRHXkonsDuApCeAsPsuujjSRPnUB6Yiy2dYlsnzOFxe+NxZC2n+WzYnj/528SPWo4Cya8y+aZ4WydFY6gOT1YFB2LasOqORB1J6LuwllQhDEjk2UR49k8L4p9SXPJWDqX5Glh5Lh8HKw4Sdq2ncSNHknMsF+wIWoSf5wdjiBpDsyyilW1vYbZnVgH/0ZeAFPGPpZGjGfNzCmkxkWyZ0ksH0aFYc7I5FB1A6WV9SgWlfh33mJTTAQ750xGEDU7FkUbAkq6A8nuQnK4cOUHUM1GVk8PJ3HyBFZMC2Pz3GkkTx3HrvVrqaptJlRezfrkD4gfPZyNMeF8HDeN/wO5P3OGzdcEhgAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/9cdc30b955fd570c857fed56edaa1f86/88663/2019_hackathon.png","srcSet":"/static/9cdc30b955fd570c857fed56edaa1f86/88663/2019_hackathon.png 720w","sizes":"100vw"},"sources":[{"srcSet":"/static/9cdc30b955fd570c857fed56edaa1f86/21362/2019_hackathon.webp 720w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.3347222222222221}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"5 min read"},"layout":"","slug":"/naver_2019_hackathon/"}},"next":{"excerpt":"SpecAugment: 「A Simple Data Augmentation Method for Automatic Speech Recognition」  Review title https://arxiv.org/abs/1904.08779 Abstract…","frontmatter":{"title":"SpecAugment Paper Review","tags":["speech","paper"],"date":"2020-01-12T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAADDElEQVQoz1WT32+TVRzGj3CD3piYjGgwJt6A7oLxB/gPeKGJXngDm47WDtd0INCVIkwHiHPLrN2ENMQwGexXV2hniSS6aeYoWybQjrGub8u2vu/b9+3PUVbFRFn3MX07L3aSk/Oc73Pyyfc5OUeUyxtUxsOwTN+dMFcezjN4N8Jw6AH98xF+8v/BzyOz9EfmGAiHuRINMzg7h3dqjuHZ+1yLRBiZjrD25G+DI/59tm6IE0euIt62Ipo+Q+y3I95xIEzH2b29nj3b6hFHjyAOHEY4DiPMDkT9cYTpEOKkDWH9lPuLWhW4vl42xM3RGRoujWLrD9I0FMDW56f5cpAvbdc45xzkE58X6+UbmIeCHAz4sUwM0OT1Yb41woc/XkfOlrYCA2P32O/x0TAW5OBwAMsPY5ivBjAP3eC8dZAWt4+PRsdoDPgwDQSwfhfk44HrNHr9WPr8aJknWyM72oYR7x7jueYvEKbTbG88xbaGNna8d4K6Fw7x/AfHEE0ORPNJhK0N0eJE2FoRTjvC6eDeUroKLJerHc6EYnQGp+iZmMH9a4je8RA949N4/Hfw9v6G59YUFyYmcU1M45oMcfHuOK7bU7hnJnHd/p1s8WkV+GyzQ9fFX3jd2s2bHR72tvewz37BmHs+/5636t3sc3ZTd8pNraeDNzrd7D16ibr2b6jt7aL2nBspmd8Ebt5h+7c32WE6w8ut3bzS0slrli5etXRRY3Wz+/0OdrWcZZe1g532r3nJ2UVNayc77V9R03aWF0+fZ07Obo1cLD1lJVtEW/0L9fGfpPJrqI9LqKsl1EIJrVIrlFBzayirayjFIkrFL1b9fzaTinw+x8LCArlclkI+h5JcRlNlUimFjK4ZOq1rZPQUy0sJQxdyGfRUatNXyKR1FEVBUVVEJpNBisdZXlkhGoshJeIsxhZ5MD+PlEiwEI0Sk2LEE3HDk+ISj5YeEV2MIsUTRl1WFJJykpSWQhQKeYNcKWi6jqalDHhln06nUVUVXdeR5aRxrrImZRlJiqFpGv+/442N6hf+D1oZ4qIlqS9AAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/a5b61e2900f2360216061fe9e8f8f640/4df46/specaugment.png","srcSet":"/static/a5b61e2900f2360216061fe9e8f8f640/4df46/specaugment.png 620w","sizes":"100vw"},"sources":[{"srcSet":"/static/a5b61e2900f2360216061fe9e8f8f640/cd871/specaugment.webp 620w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5919354838709677}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"20 min read"},"layout":"","slug":"/specaugment/"}},"primaryTag":"nlp"}},
    "staticQueryHashes": ["3170763342","3229353822"]}