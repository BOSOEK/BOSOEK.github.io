{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/las/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>「Listen, Attend and Spell」 Review</h1>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006522-b6c82f01-7912-407a-b6d6-dc4537f859d9.png\" alt=\"title\"></p>\n<p><a href=\"https://arxiv.org/abs/1508.01211\">https://arxiv.org/abs/1508.01211</a>  (William Chan et al. 2015)</p>\n<h1></h1>\n<h2><strong>Introduction</strong></h2>\n<p>어텐션 기반 Seq2seq 구조를 음성 인식에 적용한 논문이다.</p>\n<p>당시에는 CTC (Connectionist temporal classification) 이 음성 인식 분야를 점유하고 있던 시절이였던 터라, End-to-End 방식으로 본 논문 모델과 같은 성능을 낸 것은 굉장히 혁명적인 일이였다고 한다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006536-a9b4d4eb-c13b-49c5-aef3-955e7e781350.png\" alt=\"end-to-end\"></p>\n<p>본 논문 이후 Speech 분야는 CTC와 LAS로 나뉜다고 한다.</p>\n<h1></h1>\n<h2><strong>Model</strong></h2>\n<p>모델의 전체적인 구조는 Listener (encoder) 와 Speller (decoder) 로 이루어져 있다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006548-2e0f34fd-34e5-472f-8c2c-37601c058c00.png\" alt=\"las_model\"></p>\n<h3><strong>Listener</strong></h3>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006557-cfb9ef22-c262-420f-a7c8-bbcf4a08bab1.png\" alt=\"listener\"></p>\n<p>데이터의 피쳐를 입력받는 Encoder<br>\n입력 시퀀스 x를 High level feature인 h로 변형하는 역할을 담당한다<br>\n(더 의미있는 시퀀스로 변형한다)</p>\n<h3><strong>Speller</strong></h3>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006565-14cebeeb-2fd6-479d-ab67-caad4db6f2fb.png\" alt=\"speller\"></p>\n<p>리스너가 변형한 High Level feature인 h를 어텐션을 사용하여 문자로 출력한다. (Decoder)</p>\n<h1></h1>\n<h3><strong>pBLSTM</strong></h3>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006706-e8c86aa2-f746-420a-875d-5bb655022cb8.png\" alt=\"pBLSTM\"></p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006716-ce9a6994-c191-41d0-92e6-aa61a9fb55e4.png\" alt=\"pblstm_math\"></p>\n<p>모델의 첫 번째 특징으로는 Pyrimidal Bidirectional LSTM을 사용했다.<br>\n이전 레이어의 2i, 2i+1 번째 시퀀스를 Concatenate하여 다음 레이어의 i번째 RNN 셀의 입력으로 넣는 구조이다.</p>\n<p>상대적으로 매우 긴 시퀀스 길이를 가지는 Speech 모델의 단점을 완화 해주는 역할을 한다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006731-2115be84-5c7d-4627-9548-2711dbacce8d.png\" alt=\"blstm_pblstm\"></p>\n<p>위의 그림처럼 기존 BLSTM 구조에 반해, 시퀀스의 길이가 상당히 줄어드는 것을 확인할 수 있다.</p>\n<p>pBLSTM 레이어 1층 당 시퀀스 길이가 반씩 줄어들게 되는데, 본 논문에서는 이러한 레이어를 3개를 둠으로써, 총 시퀀스 길이를 8 분의 1로 줄였다고 한다.</p>\n<p>이러한 시퀀스 길이의 감소는 디코딩 &#x26; 어텐션 과정에서 연산량 감소를 가능하게한다.</p>\n<h1></h1>\n<h3><strong>Exposure Bias Problem</strong></h3>\n<p>Seq2seq에서 티쳐 포싱이라는 개념이 있다.<br>\n티쳐 포싱의 개념은 <a href=\"https://blog.naver.com/sooftware/221790750668\">이글</a>을 참고하기를 바랍니다.</p>\n<p>당시에는 티쳐 포싱은 Seq2seq 아키텍쳐에서 디폴트 100%로 사용됐던 것 같다.<br>\n티쳐 포싱은 학습을 빠르게 해준다는 장점이 있지만, Exposure Bias Problem이란게 존재한다.</p>\n<p>티쳐 포싱은 학습 시에 레이블을 제공받지만, 실제 추론 시에는 레이블을 제공 받을 수가 없다.<br>\n이러한 차이점이 실제 추론과 학습시의 성능에 차이가 있을 수 있다는 점이다.<br>\n이를 Exposure Bias Problem이라고 한다.</p>\n<p>본 논문에서는 이러한 문제점을 완화 및 실험해보기 위하여 티쳐 포싱 비율이 100%인 모델과 90%인 모델 2개를 학습시켰다고 한다.</p>\n<p>( 2019년 5월에 나온 「Exposure Bias for Neural Language Generation」논문에서는 이런 노출 편향 문제가 생각만큼 큰 영향을 미치지는 않는다는 연구 결과를 냈다고 한다 )</p>\n<h1></h1>\n<h3><strong>Decoding</strong></h3>\n<p>본 논문에서는 역시 빔서치를 사용했다고 한다. (빔사이즈 = 32)<br>\n빔서치에 대한 설명은 <a href=\"https://blog.naver.com/sooftware/221809101199\">이글</a>을 참고하길 바랍니다.</p>\n<p>본 논문에서 설명하기를, 기존 음성 인식 모델들은 모든 빔이 <eos>를 만나고 나면, 가장 높은 점수를 받은 빔을 선택한 후, Dictionary (사전) 을 통해 언어 교정을 하는 방식을 많이 사용했다고 한다.</p>\n<p>하지만, 본 논문에서 실험시에, 이 DIctionary 방식은 별로 필요가 없다고 주장한다.</p>\n<p>본 논문의 모델로 실험해본 결과, 어느 정도 학습 후에는 거의 항상 단어 단위에서는 완벽한 단어를 내놓기 때문에, 이러한 교정 과정이 필요가 없다는 것이다.</p>\n<h3><strong>Rescoring</strong></h3>\n<p>그래서 본 논문에서는 Dictionary 방식이 아닌 Rescoring 방식을 제안한다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006752-c4fb0449-9da4-4a84-812c-882cc46f7ba3.png\" alt=\"rescoring\"></p>\n<p>빔 사이즈 만큼의 후보를 뽑아놓은 후에, 여기에 Language Model을 이용하여 점수를 매긴 뒤, 기존 점수와 LM에서 나온 점수를 적절히 결합하여 새로 점수를 매기는 것이다.<br>\n해서, 최종적으로 가장 높은 점수를 받은 빔을 최종 선택지로 사용하는 것이다.</p>\n<h1></h1>\n<h2><strong>Experiments</strong></h2>\n<p>본 논문에서 진행한 실험 환경은 아래와 같다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006766-cd720767-2c86-4705-baac-e426b53a6ea3.png\" alt=\"experiment_environ\"></p>\n<p>위의 환경에서 진행한 실험 결과는 아래와 같다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006787-318572b3-268a-428a-a987-7b6fd86d6946.png\" alt=\"result\"></p>\n<p>Language Model을 적용하기 전에는 노이즈가 없는 환경에서는 14.1%의 WER (Word Error Rate), 노이즈가 있는 환경에서는 16.5%의 WER을 기록했다.</p>\n<p>결과를 보면, 100%의 티쳐 포싱 비율을 가진 모델보다, 90%의 티쳐 포싱 비율을 가진 모델이 더 좋은 결과를 기록한 것을 알 수 있다.</p>\n<p>그리고 본 논문에서 제안한 <strong>Beam Search + Language Model</strong>의 퍼포먼스는 매우 훌륭했다.</p>\n<p>모든 면에서 약 4%의 성능을 개선한 것을 확인할 수 있다.<br>\n인식률이 85%가 넘어가는 상황에서의 4%는 엄청난 발전이라고 볼 수 있다.</p>\n<p>본 논문은 위의 결과를 통해 새로 제안한 Rescoring 방식이 의미 있는 결과를 냈다는 것을 검증했다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006971-80b75612-9641-4031-827d-f8a831dcc381.png\" alt=\"sota\"></p>\n<p>또한, 본 논문의 모델은 당시 SOTA (State-Of-The-Art) 모델인 CLDNN-HMM 모델과 비교하여 2.3% WER 정도만의 차이를 기록했다.<br>\nCTC를 사용하지 않고도 이 정도의 성능을 낼 수 있다는 것을 보여준 셈이다.</p>\n<p>본 논문에서는 SOTA 모델과 자신들의 모델의 차이를 Convolution filter의 유무에 의해 생겼다고 추정하고 있다.</p>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"「Listen, Attend and Spell」 Review"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006522-b6c82f01-7912-407a-b6d6-dc4537f859d9.png","alt":"title"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1508.01211"},"children":[{"type":"text","value":"https://arxiv.org/abs/1508.01211"}]},{"type":"text","value":"  (William Chan et al. 2015)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Introduction"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"어텐션 기반 Seq2seq 구조를 음성 인식에 적용한 논문이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"당시에는 CTC (Connectionist temporal classification) 이 음성 인식 분야를 점유하고 있던 시절이였던 터라, End-to-End 방식으로 본 논문 모델과 같은 성능을 낸 것은 굉장히 혁명적인 일이였다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006536-a9b4d4eb-c13b-49c5-aef3-955e7e781350.png","alt":"end-to-end"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문 이후 Speech 분야는 CTC와 LAS로 나뉜다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Model"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"모델의 전체적인 구조는 Listener (encoder) 와 Speller (decoder) 로 이루어져 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006548-2e0f34fd-34e5-472f-8c2c-37601c058c00.png","alt":"las_model"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Listener"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006557-cfb9ef22-c262-420f-a7c8-bbcf4a08bab1.png","alt":"listener"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"데이터의 피쳐를 입력받는 Encoder"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n입력 시퀀스 x를 High level feature인 h로 변형하는 역할을 담당한다"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n(더 의미있는 시퀀스로 변형한다)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Speller"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006565-14cebeeb-2fd6-479d-ab67-caad4db6f2fb.png","alt":"speller"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"리스너가 변형한 High Level feature인 h를 어텐션을 사용하여 문자로 출력한다. (Decoder)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"pBLSTM"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006706-e8c86aa2-f746-420a-875d-5bb655022cb8.png","alt":"pBLSTM"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006716-ce9a6994-c191-41d0-92e6-aa61a9fb55e4.png","alt":"pblstm_math"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"모델의 첫 번째 특징으로는 Pyrimidal Bidirectional LSTM을 사용했다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이전 레이어의 2i, 2i+1 번째 시퀀스를 Concatenate하여 다음 레이어의 i번째 RNN 셀의 입력으로 넣는 구조이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"상대적으로 매우 긴 시퀀스 길이를 가지는 Speech 모델의 단점을 완화 해주는 역할을 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006731-2115be84-5c7d-4627-9548-2711dbacce8d.png","alt":"blstm_pblstm"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 그림처럼 기존 BLSTM 구조에 반해, 시퀀스의 길이가 상당히 줄어드는 것을 확인할 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"pBLSTM 레이어 1층 당 시퀀스 길이가 반씩 줄어들게 되는데, 본 논문에서는 이러한 레이어를 3개를 둠으로써, 총 시퀀스 길이를 8 분의 1로 줄였다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이러한 시퀀스 길이의 감소는 디코딩 & 어텐션 과정에서 연산량 감소를 가능하게한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Exposure Bias Problem"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Seq2seq에서 티쳐 포싱이라는 개념이 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n티쳐 포싱의 개념은 "},{"type":"element","tagName":"a","properties":{"href":"https://blog.naver.com/sooftware/221790750668"},"children":[{"type":"text","value":"이글"}]},{"type":"text","value":"을 참고하기를 바랍니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"당시에는 티쳐 포싱은 Seq2seq 아키텍쳐에서 디폴트 100%로 사용됐던 것 같다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n티쳐 포싱은 학습을 빠르게 해준다는 장점이 있지만, Exposure Bias Problem이란게 존재한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"티쳐 포싱은 학습 시에 레이블을 제공받지만, 실제 추론 시에는 레이블을 제공 받을 수가 없다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이러한 차이점이 실제 추론과 학습시의 성능에 차이가 있을 수 있다는 점이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이를 Exposure Bias Problem이라고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 이러한 문제점을 완화 및 실험해보기 위하여 티쳐 포싱 비율이 100%인 모델과 90%인 모델 2개를 학습시켰다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"( 2019년 5월에 나온 「Exposure Bias for Neural Language Generation」논문에서는 이런 노출 편향 문제가 생각만큼 큰 영향을 미치지는 않는다는 연구 결과를 냈다고 한다 )"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Decoding"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 역시 빔서치를 사용했다고 한다. (빔사이즈 = 32)"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n빔서치에 대한 설명은 "},{"type":"element","tagName":"a","properties":{"href":"https://blog.naver.com/sooftware/221809101199"},"children":[{"type":"text","value":"이글"}]},{"type":"text","value":"을 참고하길 바랍니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서 설명하기를, 기존 음성 인식 모델들은 모든 빔이 "},{"type":"element","tagName":"eos","properties":{},"children":[{"type":"text","value":"를 만나고 나면, 가장 높은 점수를 받은 빔을 선택한 후, Dictionary (사전) 을 통해 언어 교정을 하는 방식을 많이 사용했다고 한다."}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"하지만, 본 논문에서 실험시에, 이 DIctionary 방식은 별로 필요가 없다고 주장한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문의 모델로 실험해본 결과, 어느 정도 학습 후에는 거의 항상 단어 단위에서는 완벽한 단어를 내놓기 때문에, 이러한 교정 과정이 필요가 없다는 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Rescoring"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그래서 본 논문에서는 Dictionary 방식이 아닌 Rescoring 방식을 제안한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006752-c4fb0449-9da4-4a84-812c-882cc46f7ba3.png","alt":"rescoring"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"빔 사이즈 만큼의 후보를 뽑아놓은 후에, 여기에 Language Model을 이용하여 점수를 매긴 뒤, 기존 점수와 LM에서 나온 점수를 적절히 결합하여 새로 점수를 매기는 것이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n해서, 최종적으로 가장 높은 점수를 받은 빔을 최종 선택지로 사용하는 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Experiments"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서 진행한 실험 환경은 아래와 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006766-cd720767-2c86-4705-baac-e426b53a6ea3.png","alt":"experiment_environ"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 환경에서 진행한 실험 결과는 아래와 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006787-318572b3-268a-428a-a987-7b6fd86d6946.png","alt":"result"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Language Model을 적용하기 전에는 노이즈가 없는 환경에서는 14.1%의 WER (Word Error Rate), 노이즈가 있는 환경에서는 16.5%의 WER을 기록했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"결과를 보면, 100%의 티쳐 포싱 비율을 가진 모델보다, 90%의 티쳐 포싱 비율을 가진 모델이 더 좋은 결과를 기록한 것을 알 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 본 논문에서 제안한 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Beam Search + Language Model"}]},{"type":"text","value":"의 퍼포먼스는 매우 훌륭했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"모든 면에서 약 4%의 성능을 개선한 것을 확인할 수 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n인식률이 85%가 넘어가는 상황에서의 4%는 엄청난 발전이라고 볼 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문은 위의 결과를 통해 새로 제안한 Rescoring 방식이 의미 있는 결과를 냈다는 것을 검증했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006971-80b75612-9641-4031-827d-f8a831dcc381.png","alt":"sota"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"또한, 본 논문의 모델은 당시 SOTA (State-Of-The-Art) 모델인 CLDNN-HMM 모델과 비교하여 2.3% WER 정도만의 차이를 기록했다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nCTC를 사용하지 않고도 이 정도의 성능을 낼 수 있다는 것을 보여준 셈이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 SOTA 모델과 자신들의 모델의 차이를 Convolution filter의 유무에 의해 생겼다고 추정하고 있다."}]}],"data":{"quirksMode":false}},"excerpt":"「Listen, Attend and Spell」 Review title https://arxiv.org/abs/1508.01211  (William Chan et al. 2015)  Introduction 어텐션 기반 Seq2seq…","fields":{"readingTime":{"text":"8 min read"}},"frontmatter":{"title":"Listen, Attend and Spell Paper Review","userDate":"20 September 2019","date":"2019-09-20T10:00:00.000Z","tags":["speech","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#e8e8e8","images":{"fallback":{"src":"/static/c6dd3a7d5b5935928a2ffb65755ccaf6/61a36/las.png","srcSet":"/static/c6dd3a7d5b5935928a2ffb65755ccaf6/61a36/las.png 625w","sizes":"100vw"},"sources":[{"srcSet":"/static/c6dd3a7d5b5935928a2ffb65755ccaf6/09f70/las.webp 625w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.192}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}]}}]}},"relatedPosts":{"totalCount":20,"edges":[{"node":{"id":"64e39e81-9c08-53ad-967e-f53e0ffd1d51","excerpt":"한국어 Tacotron2 이번 포스팅에서는 Tacotron2 아키텍처로 한국어 TTS 시스템을 만드는 방법에 대해 다루겠습니다. Tacotron2 Tacotron2는 17년 12월 구글이 NATURAL TTS SYNTHESIS BY…","frontmatter":{"title":"한국어 Tacotron2","date":"2021-10-10T10:00:00.000Z"},"fields":{"readingTime":{"text":"11 min read"},"slug":"/korean_tacotron2/"}}},{"node":{"id":"8609f7b7-4942-59fe-bfaa-5f82c648649e","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP_ Generating expressive speech from raw audio/"}}},{"node":{"id":"75998e15-7d74-5d05-af5b-1112437e067d","excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","date":"2021-03-17T10:00:00.000Z"},"fields":{"readingTime":{"text":"3 min read"},"slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/"}}},{"node":{"id":"feb53e83-51f2-5350-ae0e-58157d8dfd22","excerpt":"PORORO Text-To-Speech (TTS) 얼마전에 저희 팀에서 공개한 PORORO: Platform Of neuRal mOdels for natuRal language prOcessing 라이브러리에 제가 공들여만든 TTS…","frontmatter":{"title":"PORORO Text-To-Speech (TTS)","date":"2021-02-16T10:00:00.000Z"},"fields":{"readingTime":{"text":"1 min read"},"slug":"/pororo-tts/"}}},{"node":{"id":"77bed2d4-fc96-5bff-9808-9b9cb45369f3","excerpt":"EMNLP Paper Review: Speech Adaptive Feature Selection for End-to-End Speech Translation (Biao Zhang et al) Incremental Text-to-Speech…","frontmatter":{"title":"EMNLP Paper Review: Speech","date":"2020-12-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/2020 EMNLP Speech Paper Review/"}}}]}},"pageContext":{"slug":"/las/","prev":{"excerpt":"MFCC (Mel-Frequency Cepstral Coefficient) ‘Voice Recognition Using MFCC Algorithm’ 논문 참고 MFCC란? 음성인식에서 MFCC, Mel-Spectrogram…","frontmatter":{"title":"MFCC (Mel-Frequency Cepstral Coefficient)","tags":["dsp","speech"],"date":"2019-06-18T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAADlUlEQVQ4y3VSXWgcVRS+TsAHQbB9EN9FEaNC0gd9EHz0RR8qCG2stPgoaERbfBCNtkIVsVQQ+yihSR4a+1IQ6k92MWDiSlMjBJqkmDS72dmfmd3Jzs7szNyf88m5m10nJR74OHPOPfPdb74zIlZ4BERHABzxg+TRdlceU7F8UkvzBICnCfQsgGcAjBLRKAGjILL1oDd4ZojdAA8aMg8BePiXO97RUjl8anMnHO105XgzUmM8ZAw9R0TjAMZANE7AGBk6RkQvAHieiIYQUQABQCTK2DxR3Bbi6+uiHGcO18aQ7TOISBA/U66XO2eINO434n3CGzsdMb+1I/xECg6pjKglUtR7qWh0M5FpI+pdKRb3zJBE50gFQOLbWxUuHrj+d8M5d/OeM1WoOCv1yPlupTmiDTnTK00n3jMOErBqJ4kz58bdplMNYieTZgTACBHZLNrmP7lzt+vi/R+3xUeFslipR+Lyn3V7+5VSXQSBFqbXV9LtZuKH9aYot2KRSTO0o6/w9d+EmLjAxeNXlnbPnbx29+N3fi5/ersevT21WD0fZvrExd/dL+/Uos833Oh0qvTptVo8eWnJnVrcCs5GmT4D4A0iOgXglEiIjnZTyb/NY1dvVSY++eneh18t7X6w0eq9+/1fzUmt6dVv/qhPunvZK0mqX1SaXvI76cs3N1rvNcLsgtbmCwAXichCpMB5EH0GYKob985WuslbAM6ADN/4JhGxggkAJ1nBPrg+AeA1AMcHIKLjIiW6BqJ5APNBJ5zf9ffmpFIzAK5qQ3MAZgzRLCNN09lerzerlJrT2sxIKaeVUtPGGAsA02zmAoAFzq7rLmxv/fPrxubmQhiGBQAFIhrmWq1msb6+XiiVSsW1tbXi8vJycXV1tch1kiRFXgaIiBNUGELFMWSWQUqJ/FmSJGg0GqjVami1WsiyDGmaIo5j9Ho9RFEEY4xdt2YA0NLzdLtS0X61auv8mVJKe56nfd/XYRhqKeWBsyRJbI8JQcb0Ffo+wkoFQbk8VDdQyNHpdBCGIdrtNlq+bxVxsELP8+xXCOQi4EHPQycIQH3GA6RMwHbYvG9J/mKG0Frb7+fwfR/VatXeNiDJE3LmeZuVOuDxIAQb67quNZjuU0SHKFRK2VrlCA8o5ANWxptjD1gt58Mir1D9n8J8wWazWibnQVbE4KXZWmsoKYdKDyXMyx36wy8qdQDsGS+CrbFz+xu+n/BfulJgXbEe9kQAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/e9157dcdd01342e368f885b8937a5d91/3a1d5/mfcc.png","srcSet":"/static/e9157dcdd01342e368f885b8937a5d91/52f05/mfcc.png 750w,\n/static/e9157dcdd01342e368f885b8937a5d91/3a1d5/mfcc.png 760w","sizes":"100vw"},"sources":[{"srcSet":"/static/e9157dcdd01342e368f885b8937a5d91/b6018/mfcc.webp 750w,\n/static/e9157dcdd01342e368f885b8937a5d91/2f184/mfcc.webp 760w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.0078947368421054}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"12 min read"},"layout":"","slug":"/mfcc/"}},"next":{"excerpt":"Deep Speech: Scaling up end-to-end speech recognition title https://arxiv.org/pdf/1412.5567.pdf (Awni Hannun et al. 2014) Abstract…","frontmatter":{"title":"DeepSpeech Paper Review","tags":["speech","paper"],"date":"2019-11-11T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAAAsTAAALEwEAmpwYAAADV0lEQVQ4y2WUW2/jVBSF+/P4EbwNb/wAQIIZeEEgpEE8DCAYMRrBIIFmkEZihpS2SdO0uSfF1EnapGnjxPEtqR0nrtMkdj50nEtJu6VjH+vsvbT2Xut4C2A+nzOZTAiCgKurq+i9Ctu2mU6nrPJWS8T19TXD4XB9JmJLPASIYRhYlsXe3h6DwSA67Pd60bfjOBuAIsIg5LhcplQqbZxtrVjMZjMcxyYeT1Cr1aKkwWjE9u4u9Xp9g0W4ZC/JMsnDw4jp6nxrPB6jKAqEIVeWReVfiaOjA240lV69TjWTIZVMMvI8UUE4n8HsBiyDRqlMLpHgRJZvAQWzXq+HqqpkMmkqtRY7yWP+yZ2x/2cMtValmC9EY5hOZ5hOgGHekI6lOMmkaZzI1E5PNxmK+U2mU8qlEtLJGY1LA8eHdL7I+eUF3W43KpgFASMfXB/qikmhVETpdDZbFhuhqhh8v99n5+9tXNeOEhzbJhaLrZUkmuFqEREo5POLuYbhrcqj0SgCE6D7+/tomr4QZTAgHo+j6/od2yz2siyTy+Xuq+z7fmQZ13XJ5/OR6iI8zyObzd6zzUrt09NTKpXKfUDBULQlAJPJ5JqRYJhIJKIZ3/WhCAGWX7a8ASiYCHML4GKxuGYomIuCldHvMmw0GlSr1eUM/wcoCoTPHGdAYj+BbiwY2pHR4ximsRQljNZ8CS5J0prhWuV1C/OQYDLGsR2uPZ/JeMzE97FtB8/zmd7cMA0hEOlByDwMGftj3IHLfDZlHswWgOES8K2k82i7wctUjtflXX7IFXl6XOaNlOUg/xe/Vo+RS09Q/Sd84ansFFPsyEfElAIvWjLfSH1G3oyt+eKm83VS4d0XFzw+OuQz+YiPsmUe1ip830jy3VmCr2oZts1HvD9M8eC8wk9nb3lcz/JJs8aD4xYPd20mfricoTPkw5dx3nua4vM3r/i4XOTTdIFnlR1+rB/wZU3it8of/O4/4518yKv0a35p7/H8Ms23FxIfyOf8nDFvjT2ZTGl2LRRzgNozuTBNWqaFbluo9hVdu09La9EZWrT712hOH33o0h04i+UOcf3ZLaD4QSiXTXStizcaoXc7tBUFravRMy0MTYu+TU2n3WrSaXfQVJVOu02r2cQybm/Sf2rLNHT3g5j+AAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/360f6a5bb171f9136086a6bf108b401d/2241d/deepspeech.png","srcSet":"/static/360f6a5bb171f9136086a6bf108b401d/2241d/deepspeech.png 541w","sizes":"100vw"},"sources":[{"srcSet":"/static/360f6a5bb171f9136086a6bf108b401d/1edf8/deepspeech.webp 541w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.9149722735674677}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"10 min read"},"layout":"","slug":"/deepspeech/"}},"primaryTag":"speech"}},
    "staticQueryHashes": ["3170763342","3229353822"]}