{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/las/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>「Listen, Attend and Spell」 Review</h1>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006522-b6c82f01-7912-407a-b6d6-dc4537f859d9.png\" alt=\"title\"></p>\n<p><a href=\"https://arxiv.org/abs/1508.01211\">https://arxiv.org/abs/1508.01211</a>  (William Chan et al. 2015)</p>\n<h1></h1>\n<h2><strong>Introduction</strong></h2>\n<p>어텐션 기반 Seq2seq 구조를 음성 인식에 적용한 논문이다.</p>\n<p>당시에는 CTC (Connectionist temporal classification) 이 음성 인식 분야를 점유하고 있던 시절이였던 터라, End-to-End 방식으로 본 논문 모델과 같은 성능을 낸 것은 굉장히 혁명적인 일이였다고 한다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006536-a9b4d4eb-c13b-49c5-aef3-955e7e781350.png\" alt=\"end-to-end\"></p>\n<p>본 논문 이후 Speech 분야는 CTC와 LAS로 나뉜다고 한다.</p>\n<h1></h1>\n<h2><strong>Model</strong></h2>\n<p>모델의 전체적인 구조는 Listener (encoder) 와 Speller (decoder) 로 이루어져 있다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006548-2e0f34fd-34e5-472f-8c2c-37601c058c00.png\" alt=\"las_model\"></p>\n<h3><strong>Listener</strong></h3>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006557-cfb9ef22-c262-420f-a7c8-bbcf4a08bab1.png\" alt=\"listener\"></p>\n<p>데이터의 피쳐를 입력받는 Encoder<br>\n입력 시퀀스 x를 High level feature인 h로 변형하는 역할을 담당한다<br>\n(더 의미있는 시퀀스로 변형한다)</p>\n<h3><strong>Speller</strong></h3>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006565-14cebeeb-2fd6-479d-ab67-caad4db6f2fb.png\" alt=\"speller\"></p>\n<p>리스너가 변형한 High Level feature인 h를 어텐션을 사용하여 문자로 출력한다. (Decoder)</p>\n<h1></h1>\n<h3><strong>pBLSTM</strong></h3>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006706-e8c86aa2-f746-420a-875d-5bb655022cb8.png\" alt=\"pBLSTM\"></p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006716-ce9a6994-c191-41d0-92e6-aa61a9fb55e4.png\" alt=\"pblstm_math\"></p>\n<p>모델의 첫 번째 특징으로는 Pyrimidal Bidirectional LSTM을 사용했다.<br>\n이전 레이어의 2i, 2i+1 번째 시퀀스를 Concatenate하여 다음 레이어의 i번째 RNN 셀의 입력으로 넣는 구조이다.</p>\n<p>상대적으로 매우 긴 시퀀스 길이를 가지는 Speech 모델의 단점을 완화 해주는 역할을 한다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006731-2115be84-5c7d-4627-9548-2711dbacce8d.png\" alt=\"blstm_pblstm\"></p>\n<p>위의 그림처럼 기존 BLSTM 구조에 반해, 시퀀스의 길이가 상당히 줄어드는 것을 확인할 수 있다.</p>\n<p>pBLSTM 레이어 1층 당 시퀀스 길이가 반씩 줄어들게 되는데, 본 논문에서는 이러한 레이어를 3개를 둠으로써, 총 시퀀스 길이를 8 분의 1로 줄였다고 한다.</p>\n<p>이러한 시퀀스 길이의 감소는 디코딩 &#x26; 어텐션 과정에서 연산량 감소를 가능하게한다.</p>\n<h1></h1>\n<h3><strong>Exposure Bias Problem</strong></h3>\n<p>Seq2seq에서 티쳐 포싱이라는 개념이 있다.<br>\n티쳐 포싱의 개념은 <a href=\"https://blog.naver.com/sooftware/221790750668\">이글</a>을 참고하기를 바랍니다.</p>\n<p>당시에는 티쳐 포싱은 Seq2seq 아키텍쳐에서 디폴트 100%로 사용됐던 것 같다.<br>\n티쳐 포싱은 학습을 빠르게 해준다는 장점이 있지만, Exposure Bias Problem이란게 존재한다.</p>\n<p>티쳐 포싱은 학습 시에 레이블을 제공받지만, 실제 추론 시에는 레이블을 제공 받을 수가 없다.<br>\n이러한 차이점이 실제 추론과 학습시의 성능에 차이가 있을 수 있다는 점이다.<br>\n이를 Exposure Bias Problem이라고 한다.</p>\n<p>본 논문에서는 이러한 문제점을 완화 및 실험해보기 위하여 티쳐 포싱 비율이 100%인 모델과 90%인 모델 2개를 학습시켰다고 한다.</p>\n<p>( 2019년 5월에 나온 「Exposure Bias for Neural Language Generation」논문에서는 이런 노출 편향 문제가 생각만큼 큰 영향을 미치지는 않는다는 연구 결과를 냈다고 한다 )</p>\n<h1></h1>\n<h3><strong>Decoding</strong></h3>\n<p>본 논문에서는 역시 빔서치를 사용했다고 한다. (빔사이즈 = 32)<br>\n빔서치에 대한 설명은 <a href=\"https://blog.naver.com/sooftware/221809101199\">이글</a>을 참고하길 바랍니다.</p>\n<p>본 논문에서 설명하기를, 기존 음성 인식 모델들은 모든 빔이 <eos>를 만나고 나면, 가장 높은 점수를 받은 빔을 선택한 후, Dictionary (사전) 을 통해 언어 교정을 하는 방식을 많이 사용했다고 한다.</p>\n<p>하지만, 본 논문에서 실험시에, 이 DIctionary 방식은 별로 필요가 없다고 주장한다.</p>\n<p>본 논문의 모델로 실험해본 결과, 어느 정도 학습 후에는 거의 항상 단어 단위에서는 완벽한 단어를 내놓기 때문에, 이러한 교정 과정이 필요가 없다는 것이다.</p>\n<h3><strong>Rescoring</strong></h3>\n<p>그래서 본 논문에서는 Dictionary 방식이 아닌 Rescoring 방식을 제안한다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006752-c4fb0449-9da4-4a84-812c-882cc46f7ba3.png\" alt=\"rescoring\"></p>\n<p>빔 사이즈 만큼의 후보를 뽑아놓은 후에, 여기에 Language Model을 이용하여 점수를 매긴 뒤, 기존 점수와 LM에서 나온 점수를 적절히 결합하여 새로 점수를 매기는 것이다.<br>\n해서, 최종적으로 가장 높은 점수를 받은 빔을 최종 선택지로 사용하는 것이다.</p>\n<h1></h1>\n<h2><strong>Experiments</strong></h2>\n<p>본 논문에서 진행한 실험 환경은 아래와 같다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006766-cd720767-2c86-4705-baac-e426b53a6ea3.png\" alt=\"experiment_environ\"></p>\n<p>위의 환경에서 진행한 실험 결과는 아래와 같다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006787-318572b3-268a-428a-a987-7b6fd86d6946.png\" alt=\"result\"></p>\n<p>Language Model을 적용하기 전에는 노이즈가 없는 환경에서는 14.1%의 WER (Word Error Rate), 노이즈가 있는 환경에서는 16.5%의 WER을 기록했다.</p>\n<p>결과를 보면, 100%의 티쳐 포싱 비율을 가진 모델보다, 90%의 티쳐 포싱 비율을 가진 모델이 더 좋은 결과를 기록한 것을 알 수 있다.</p>\n<p>그리고 본 논문에서 제안한 <strong>Beam Search + Language Model</strong>의 퍼포먼스는 매우 훌륭했다.</p>\n<p>모든 면에서 약 4%의 성능을 개선한 것을 확인할 수 있다.<br>\n인식률이 85%가 넘어가는 상황에서의 4%는 엄청난 발전이라고 볼 수 있다.</p>\n<p>본 논문은 위의 결과를 통해 새로 제안한 Rescoring 방식이 의미 있는 결과를 냈다는 것을 검증했다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/134006971-80b75612-9641-4031-827d-f8a831dcc381.png\" alt=\"sota\"></p>\n<p>또한, 본 논문의 모델은 당시 SOTA (State-Of-The-Art) 모델인 CLDNN-HMM 모델과 비교하여 2.3% WER 정도만의 차이를 기록했다.<br>\nCTC를 사용하지 않고도 이 정도의 성능을 낼 수 있다는 것을 보여준 셈이다.</p>\n<p>본 논문에서는 SOTA 모델과 자신들의 모델의 차이를 Convolution filter의 유무에 의해 생겼다고 추정하고 있다.</p>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"「Listen, Attend and Spell」 Review"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006522-b6c82f01-7912-407a-b6d6-dc4537f859d9.png","alt":"title"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1508.01211"},"children":[{"type":"text","value":"https://arxiv.org/abs/1508.01211"}]},{"type":"text","value":"  (William Chan et al. 2015)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Introduction"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"어텐션 기반 Seq2seq 구조를 음성 인식에 적용한 논문이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"당시에는 CTC (Connectionist temporal classification) 이 음성 인식 분야를 점유하고 있던 시절이였던 터라, End-to-End 방식으로 본 논문 모델과 같은 성능을 낸 것은 굉장히 혁명적인 일이였다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006536-a9b4d4eb-c13b-49c5-aef3-955e7e781350.png","alt":"end-to-end"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문 이후 Speech 분야는 CTC와 LAS로 나뉜다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Model"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"모델의 전체적인 구조는 Listener (encoder) 와 Speller (decoder) 로 이루어져 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006548-2e0f34fd-34e5-472f-8c2c-37601c058c00.png","alt":"las_model"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Listener"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006557-cfb9ef22-c262-420f-a7c8-bbcf4a08bab1.png","alt":"listener"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"데이터의 피쳐를 입력받는 Encoder"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n입력 시퀀스 x를 High level feature인 h로 변형하는 역할을 담당한다"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n(더 의미있는 시퀀스로 변형한다)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Speller"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006565-14cebeeb-2fd6-479d-ab67-caad4db6f2fb.png","alt":"speller"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"리스너가 변형한 High Level feature인 h를 어텐션을 사용하여 문자로 출력한다. (Decoder)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"pBLSTM"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006706-e8c86aa2-f746-420a-875d-5bb655022cb8.png","alt":"pBLSTM"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006716-ce9a6994-c191-41d0-92e6-aa61a9fb55e4.png","alt":"pblstm_math"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"모델의 첫 번째 특징으로는 Pyrimidal Bidirectional LSTM을 사용했다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이전 레이어의 2i, 2i+1 번째 시퀀스를 Concatenate하여 다음 레이어의 i번째 RNN 셀의 입력으로 넣는 구조이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"상대적으로 매우 긴 시퀀스 길이를 가지는 Speech 모델의 단점을 완화 해주는 역할을 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006731-2115be84-5c7d-4627-9548-2711dbacce8d.png","alt":"blstm_pblstm"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 그림처럼 기존 BLSTM 구조에 반해, 시퀀스의 길이가 상당히 줄어드는 것을 확인할 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"pBLSTM 레이어 1층 당 시퀀스 길이가 반씩 줄어들게 되는데, 본 논문에서는 이러한 레이어를 3개를 둠으로써, 총 시퀀스 길이를 8 분의 1로 줄였다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이러한 시퀀스 길이의 감소는 디코딩 & 어텐션 과정에서 연산량 감소를 가능하게한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Exposure Bias Problem"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Seq2seq에서 티쳐 포싱이라는 개념이 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n티쳐 포싱의 개념은 "},{"type":"element","tagName":"a","properties":{"href":"https://blog.naver.com/sooftware/221790750668"},"children":[{"type":"text","value":"이글"}]},{"type":"text","value":"을 참고하기를 바랍니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"당시에는 티쳐 포싱은 Seq2seq 아키텍쳐에서 디폴트 100%로 사용됐던 것 같다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n티쳐 포싱은 학습을 빠르게 해준다는 장점이 있지만, Exposure Bias Problem이란게 존재한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"티쳐 포싱은 학습 시에 레이블을 제공받지만, 실제 추론 시에는 레이블을 제공 받을 수가 없다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이러한 차이점이 실제 추론과 학습시의 성능에 차이가 있을 수 있다는 점이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이를 Exposure Bias Problem이라고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 이러한 문제점을 완화 및 실험해보기 위하여 티쳐 포싱 비율이 100%인 모델과 90%인 모델 2개를 학습시켰다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"( 2019년 5월에 나온 「Exposure Bias for Neural Language Generation」논문에서는 이런 노출 편향 문제가 생각만큼 큰 영향을 미치지는 않는다는 연구 결과를 냈다고 한다 )"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Decoding"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 역시 빔서치를 사용했다고 한다. (빔사이즈 = 32)"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n빔서치에 대한 설명은 "},{"type":"element","tagName":"a","properties":{"href":"https://blog.naver.com/sooftware/221809101199"},"children":[{"type":"text","value":"이글"}]},{"type":"text","value":"을 참고하길 바랍니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서 설명하기를, 기존 음성 인식 모델들은 모든 빔이 "},{"type":"element","tagName":"eos","properties":{},"children":[{"type":"text","value":"를 만나고 나면, 가장 높은 점수를 받은 빔을 선택한 후, Dictionary (사전) 을 통해 언어 교정을 하는 방식을 많이 사용했다고 한다."}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"하지만, 본 논문에서 실험시에, 이 DIctionary 방식은 별로 필요가 없다고 주장한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문의 모델로 실험해본 결과, 어느 정도 학습 후에는 거의 항상 단어 단위에서는 완벽한 단어를 내놓기 때문에, 이러한 교정 과정이 필요가 없다는 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Rescoring"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그래서 본 논문에서는 Dictionary 방식이 아닌 Rescoring 방식을 제안한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006752-c4fb0449-9da4-4a84-812c-882cc46f7ba3.png","alt":"rescoring"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"빔 사이즈 만큼의 후보를 뽑아놓은 후에, 여기에 Language Model을 이용하여 점수를 매긴 뒤, 기존 점수와 LM에서 나온 점수를 적절히 결합하여 새로 점수를 매기는 것이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n해서, 최종적으로 가장 높은 점수를 받은 빔을 최종 선택지로 사용하는 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Experiments"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서 진행한 실험 환경은 아래와 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006766-cd720767-2c86-4705-baac-e426b53a6ea3.png","alt":"experiment_environ"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 환경에서 진행한 실험 결과는 아래와 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006787-318572b3-268a-428a-a987-7b6fd86d6946.png","alt":"result"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Language Model을 적용하기 전에는 노이즈가 없는 환경에서는 14.1%의 WER (Word Error Rate), 노이즈가 있는 환경에서는 16.5%의 WER을 기록했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"결과를 보면, 100%의 티쳐 포싱 비율을 가진 모델보다, 90%의 티쳐 포싱 비율을 가진 모델이 더 좋은 결과를 기록한 것을 알 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 본 논문에서 제안한 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Beam Search + Language Model"}]},{"type":"text","value":"의 퍼포먼스는 매우 훌륭했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"모든 면에서 약 4%의 성능을 개선한 것을 확인할 수 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n인식률이 85%가 넘어가는 상황에서의 4%는 엄청난 발전이라고 볼 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문은 위의 결과를 통해 새로 제안한 Rescoring 방식이 의미 있는 결과를 냈다는 것을 검증했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134006971-80b75612-9641-4031-827d-f8a831dcc381.png","alt":"sota"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"또한, 본 논문의 모델은 당시 SOTA (State-Of-The-Art) 모델인 CLDNN-HMM 모델과 비교하여 2.3% WER 정도만의 차이를 기록했다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nCTC를 사용하지 않고도 이 정도의 성능을 낼 수 있다는 것을 보여준 셈이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 SOTA 모델과 자신들의 모델의 차이를 Convolution filter의 유무에 의해 생겼다고 추정하고 있다."}]}],"data":{"quirksMode":false}},"excerpt":"「Listen, Attend and Spell」 Review title https://arxiv.org/abs/1508.01211  (William Chan et al. 2015)  Introduction 어텐션 기반 Seq2seq…","fields":{"readingTime":{"text":"8 min read"}},"frontmatter":{"title":"Listen, Attend and Spell Paper Review","userDate":"20 September 2019","date":"2019-09-20T10:00:00.000Z","tags":["speech","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#e8e8e8","images":{"fallback":{"src":"/static/c6dd3a7d5b5935928a2ffb65755ccaf6/61a36/las.png","srcSet":"/static/c6dd3a7d5b5935928a2ffb65755ccaf6/61a36/las.png 625w","sizes":"100vw"},"sources":[{"srcSet":"/static/c6dd3a7d5b5935928a2ffb65755ccaf6/09f70/las.webp 625w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.192}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}]}}]}},"relatedPosts":{"totalCount":20,"edges":[{"node":{"id":"64e39e81-9c08-53ad-967e-f53e0ffd1d51","excerpt":"한국어 Tacotron2 이번 포스팅에서는 Tacotron2 아키텍처로 한국어 TTS 시스템을 만드는 방법에 대해 다루겠습니다. Tacotron2 Tacotron2는 17년 12월 구글이 NATURAL TTS SYNTHESIS BY…","frontmatter":{"title":"한국어 Tacotron2","date":"2021-10-10T10:00:00.000Z"},"fields":{"readingTime":{"text":"11 min read"},"slug":"/korean_tacotron2/"}}},{"node":{"id":"8609f7b7-4942-59fe-bfaa-5f82c648649e","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP_ Generating expressive speech from raw audio/"}}},{"node":{"id":"75998e15-7d74-5d05-af5b-1112437e067d","excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","date":"2021-03-17T10:00:00.000Z"},"fields":{"readingTime":{"text":"3 min read"},"slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/"}}},{"node":{"id":"feb53e83-51f2-5350-ae0e-58157d8dfd22","excerpt":"PORORO Text-To-Speech (TTS) 얼마전에 저희 팀에서 공개한 PORORO: Platform Of neuRal mOdels for natuRal language prOcessing 라이브러리에 제가 공들여만든 TTS…","frontmatter":{"title":"PORORO Text-To-Speech (TTS)","date":"2021-02-16T10:00:00.000Z"},"fields":{"readingTime":{"text":"1 min read"},"slug":"/pororo-tts/"}}},{"node":{"id":"77bed2d4-fc96-5bff-9808-9b9cb45369f3","excerpt":"EMNLP Paper Review: Speech Adaptive Feature Selection for End-to-End Speech Translation (Biao Zhang et al) Incremental Text-to-Speech…","frontmatter":{"title":"EMNLP Paper Review: Speech","date":"2020-12-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/2020 EMNLP Speech Paper Review/"}}}]}},"pageContext":{"slug":"/las/","prev":{"excerpt":"MFCC (Mel-Frequency Cepstral Coefficient) ‘Voice Recognition Using MFCC Algorithm’ 논문 참고 MFCC란? 음성인식에서 MFCC, Mel-Spectrogram…","frontmatter":{"title":"MFCC (Mel-Frequency Cepstral Coefficient)","tags":["dsp","speech"],"date":"2019-06-18T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC0UlEQVQ4y4VVS2/TQBDOb+AOohcQ4sJ/KFIvFRJHfgRXuKAqBy4lPC5UIA5UaiUkxAWKhBCqFAFtaWnCo60NIqRJ7NohcWLHTuLX2v7QbJywSVMx0mo8u7PfzHwz2WSSJAEtktDtols/Avo9JHHM94bnot9JNkkmSTdIGpaNH2oDhuWg54Xo+OzYhVj4hnB3BMid4oGxWWni9fca1iQdFdPFi18m32/2A/gsgh9Gg0qiGEeOx/VkBRkx+tJWFbfWDrCw/huHpoeFTQ0sjvFwt471qoXnchMeY9hr2Mh+qOFdtTOl5PTDCSLMPirgfDaPi7kdfDw0Mbt8gBtvy7j8ZB9XVmVcWvqGa89+4tzdAk7d3sGZB1/wWbHHqBgDnHtcxLlsHhfubCNfamN+WcL1N2XMPT3A/IqMmVwBV1clzOR2cTpXxNl7RWxOAxyCrhQV3Hy1j2z+EOW2i8VPOhIkWNzWsaF28L7Wgea42Gs6uL9bx4ZqUcFTmpIakm7i5dcqtsoNNLs+5FYfUZygUO+OLqa3oXQ8zu+goUJTRPQ/lgNJacJ3XUSMjUrBsIspLG1FsRBAGKGxkt2ujbZaQ9TtQsx8OKthGCIIAkRRxDMjPfiOJ7qcRm+1WtBVFUqthn6//28kUt1ut/lSFAWyLKNSqUCSJJRKJW5TsLGSmW2D9V2eCS3xzPM8NBoN6LrOQX3f53u9Xo8HJ03ZjpUcGgZMVUVL044NLGMMhmHwKhzH4QHFMwKnvQFg+hAwclZVWIpyDJDEtm0OZpom2q0W545z77o8GIFmxC5Z5GgYsC1rMCQTrwoBhEEw0Cklk69Phuqm+pE2RdM0Hm3a00Sa/LlmbIzjoWSIXCKa9P/ePcqM+CKbMTb1vczQAWVGnRt2jfQ0ETNkJ2UoGkT2cCzIMeY/rZg3jdtRBBaGo0ynAorpjvihi4yNLeKMGkHUcD/hL0IE/Atv5vH1TnvRNwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/e9157dcdd01342e368f885b8937a5d91/3a1d5/mfcc.png","srcSet":"/static/e9157dcdd01342e368f885b8937a5d91/52f05/mfcc.png 750w,\n/static/e9157dcdd01342e368f885b8937a5d91/3a1d5/mfcc.png 760w","sizes":"100vw"},"sources":[{"srcSet":"/static/e9157dcdd01342e368f885b8937a5d91/b6018/mfcc.webp 750w,\n/static/e9157dcdd01342e368f885b8937a5d91/2f184/mfcc.webp 760w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.0078947368421054}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"12 min read"},"layout":"","slug":"/mfcc/"}},"next":{"excerpt":"Deep Speech: Scaling up end-to-end speech recognition title https://arxiv.org/pdf/1412.5567.pdf (Awni Hannun et al. 2014) Abstract…","frontmatter":{"title":"DeepSpeech Paper Review","tags":["speech","paper"],"date":"2019-11-11T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAAAsTAAALEwEAmpwYAAADV0lEQVQ4y2WUW2/jVBSF+/P4EbwNb/wAQIIZeEEgpEE8DCAYMRrBIIFmkEZihpS2SdO0uSfF1EnapGnjxPEtqR0nrtMkdj50nEtJu6VjH+vsvbT2Xut4C2A+nzOZTAiCgKurq+i9Ctu2mU6nrPJWS8T19TXD4XB9JmJLPASIYRhYlsXe3h6DwSA67Pd60bfjOBuAIsIg5LhcplQqbZxtrVjMZjMcxyYeT1Cr1aKkwWjE9u4u9Xp9g0W4ZC/JMsnDw4jp6nxrPB6jKAqEIVeWReVfiaOjA240lV69TjWTIZVMMvI8UUE4n8HsBiyDRqlMLpHgRJZvAQWzXq+HqqpkMmkqtRY7yWP+yZ2x/2cMtValmC9EY5hOZ5hOgGHekI6lOMmkaZzI1E5PNxmK+U2mU8qlEtLJGY1LA8eHdL7I+eUF3W43KpgFASMfXB/qikmhVETpdDZbFhuhqhh8v99n5+9tXNeOEhzbJhaLrZUkmuFqEREo5POLuYbhrcqj0SgCE6D7+/tomr4QZTAgHo+j6/od2yz2siyTy+Xuq+z7fmQZ13XJ5/OR6iI8zyObzd6zzUrt09NTKpXKfUDBULQlAJPJ5JqRYJhIJKIZ3/WhCAGWX7a8ASiYCHML4GKxuGYomIuCldHvMmw0GlSr1eUM/wcoCoTPHGdAYj+BbiwY2pHR4ximsRQljNZ8CS5J0prhWuV1C/OQYDLGsR2uPZ/JeMzE97FtB8/zmd7cMA0hEOlByDwMGftj3IHLfDZlHswWgOES8K2k82i7wctUjtflXX7IFXl6XOaNlOUg/xe/Vo+RS09Q/Sd84ansFFPsyEfElAIvWjLfSH1G3oyt+eKm83VS4d0XFzw+OuQz+YiPsmUe1ip830jy3VmCr2oZts1HvD9M8eC8wk9nb3lcz/JJs8aD4xYPd20mfricoTPkw5dx3nua4vM3r/i4XOTTdIFnlR1+rB/wZU3it8of/O4/4518yKv0a35p7/H8Ms23FxIfyOf8nDFvjT2ZTGl2LRRzgNozuTBNWqaFbluo9hVdu09La9EZWrT712hOH33o0h04i+UOcf3ZLaD4QSiXTXStizcaoXc7tBUFravRMy0MTYu+TU2n3WrSaXfQVJVOu02r2cQybm/Sf2rLNHT3g5j+AAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/360f6a5bb171f9136086a6bf108b401d/2241d/deepspeech.png","srcSet":"/static/360f6a5bb171f9136086a6bf108b401d/2241d/deepspeech.png 541w","sizes":"100vw"},"sources":[{"srcSet":"/static/360f6a5bb171f9136086a6bf108b401d/1edf8/deepspeech.webp 541w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.9149722735674677}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"10 min read"},"layout":"","slug":"/deepspeech/"}},"primaryTag":"speech"}},
    "staticQueryHashes": ["3170763342","3229353822"]}