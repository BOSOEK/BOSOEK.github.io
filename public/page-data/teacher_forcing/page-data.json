{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/teacher_forcing/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>Teacher Forcing</h1>\n<p>본 포스팅을 이해하기 위해서는 다음 글에 대한 이해가 선행되는 것이 좋습니다.</p>\n<ul>\n<li><a href=\"https://sooftware.io/rnn/\">RNN (Recurrent Neural Network)</a></li>\n<li><a href=\"https://sooftware.io/lstm_gru/\">LSTM &#x26; GRU (Long Short Term Memory &#x26; Gated Recurrent Unit)</a></li>\n<li><a href=\"https://sooftware.io/seq2seq/\">Seq2seq (Sequence to sequence)</a></li>\n</ul>\n<hr>\n<h2>Teacher Forcing의 개요</h2>\n<p><em><strong>Teacher Forcing is the technique where the target word is passed as the next input to the decoder</strong></em></p>\n<p>티쳐 포싱은 Seq2seq (Encoder-Decoder) 을 기반으로 한 모델들에서 많이 사용되는 기법이다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/149659739-9dc7e4be-3702-438f-85b9-f1a7604e9d43.png\" width=\"400\">  \n<p>티쳐 포싱은 target word(Ground Truth)를 디코더의 다음 입력으로 넣어주는 기법이다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/149659792-cbe4ba4e-7862-476a-8e47-b85a6bfff9b7.png\" width=\"400\">\n<p>티쳐 포싱이 적용되지 않은 Seq2seq 모델의 디코더를 생각해보자.  t-1 번째의 디코더 셀이 예측한 값 (y_hat) 을 t번째 디코더의 입력으로 넣어준다.</p>\n<p>t-1번째에서 정확한 예측이 이루어졌다면 상관없지만, 잘못된 예측이 이루어졌다면 t번째 디코더의 추론 역시 잘못된 예측으로 이어질 것이다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/149659836-550c5dea-1d1e-4587-9acb-6cd2d14ec8a2.png\" width=\"400\">  \n<p>이전 예측을 고려해주는 디코더의 장점이 잘못된 예측 앞에서는 엄청난 단점이 되어버린다.</p>\n<p>특히 이러한 단점은 학습 초기에 학습 속도 저하의 요인이 된다. 이러한 단점을 해결하기 위해 나온 기법이 티쳐포싱(Teacher Forcing) 기법이다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/149659856-b7ef82fb-b260-453a-9242-7f7a3a00fcfd.png\" width=\"400\">\n<p>위와 같이 입력을 Ground Truth로 넣어주게 되면, 학습시 더 정확한 예측이 가능하게 되기 때문에 초기 학습 속도를 빠르게 올릴 수 있다.</p>\n<h2>Teacher Forcing의 쉬운 비유</h2>\n<img src=\"https://user-images.githubusercontent.com/42150335/149659876-d032ef83-6165-4bf3-8cf4-008f1036e324.png\" width=\"400\">\n<p>위와 같이 문제 A의 답이 문제 B의 계산에 필요하고, 문제 B의 답이 문제 C의 풀이에 이용되는 문제들을 생각해보자.</p>\n<h3>Teacher Forcing 미사용</h3>\n<p>학생은 문제 A, B, C를 순서대로 풀이하고 답 a, b, c를 한꺼번에 작성하여 제출<br>\n교사는 이 답안지를 보고 a, b, c를 한꺼번에 채점하여 점수를 알려줌.</p>\n<h3>Teacher Forcing 사용</h3>\n<p>학생은 문제 A를 풀이하고 답 a를 제출\n교사는 답안지를 가져가고, 정답 a를 알려줌<br>\n학생은 문제 A의 답 a를 가지로 문제 B를 풀이하고 답 b를 제출</p>\n<h2>Teacher Forcing 기법의 장단점</h2>\n<h3>학습이 빠르다</h3>\n<p>학습 초기 단계에서는 모델의 예측 성능이 나쁘다. 때문에 Teacher Forcing을 이용하지 않으면 잘못된 예측 값을 토대로 Hidden State 값이 업데이트되고, 이 때문에 모델의 학습 속도는 더뎌지게 된다.</p>\n<h3>노출 편향 문제 (Exposure Bias Problem)</h3>\n<p>추론 (Inference) 과정에서는 Ground Truth를 제공할 수 없다. 때문에 모델은 전 단계의 자기 자신의 출력값을 기반으로 예측을 이어가야한다. 이러한 학습과 추론 단계에서의 차이 (discrepancy) 가 존재하여 모델의 성능과 안정성을 떨어뜨릴 수 있다.</p>\n<p>다만 노출 편향 문제가 생각만큼 큰 영향을 미치지 않는다는 2019년 연구 결과가 나와 있다고 한다.<br>\n(T. He, J. Zhang, Z. Zhou, and J. Glass. Quantifying Exposure Bias for Neural Language Generation (2019), arXiv.)</p>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Teacher Forcing"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 포스팅을 이해하기 위해서는 다음 글에 대한 이해가 선행되는 것이 좋습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://sooftware.io/rnn/"},"children":[{"type":"text","value":"RNN (Recurrent Neural Network)"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://sooftware.io/lstm_gru/"},"children":[{"type":"text","value":"LSTM & GRU (Long Short Term Memory & Gated Recurrent Unit)"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://sooftware.io/seq2seq/"},"children":[{"type":"text","value":"Seq2seq (Sequence to sequence)"}]}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Teacher Forcing의 개요"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"em","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Teacher Forcing is the technique where the target word is passed as the next input to the decoder"}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"티쳐 포싱은 Seq2seq (Encoder-Decoder) 을 기반으로 한 모델들에서 많이 사용되는 기법이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/149659739-9dc7e4be-3702-438f-85b9-f1a7604e9d43.png","width":400},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"티쳐 포싱은 target word(Ground Truth)를 디코더의 다음 입력으로 넣어주는 기법이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/149659792-cbe4ba4e-7862-476a-8e47-b85a6bfff9b7.png","width":400},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"티쳐 포싱이 적용되지 않은 Seq2seq 모델의 디코더를 생각해보자.  t-1 번째의 디코더 셀이 예측한 값 (y_hat) 을 t번째 디코더의 입력으로 넣어준다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"t-1번째에서 정확한 예측이 이루어졌다면 상관없지만, 잘못된 예측이 이루어졌다면 t번째 디코더의 추론 역시 잘못된 예측으로 이어질 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/149659836-550c5dea-1d1e-4587-9acb-6cd2d14ec8a2.png","width":400},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이전 예측을 고려해주는 디코더의 장점이 잘못된 예측 앞에서는 엄청난 단점이 되어버린다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"특히 이러한 단점은 학습 초기에 학습 속도 저하의 요인이 된다. 이러한 단점을 해결하기 위해 나온 기법이 티쳐포싱(Teacher Forcing) 기법이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/149659856-b7ef82fb-b260-453a-9242-7f7a3a00fcfd.png","width":400},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위와 같이 입력을 Ground Truth로 넣어주게 되면, 학습시 더 정확한 예측이 가능하게 되기 때문에 초기 학습 속도를 빠르게 올릴 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Teacher Forcing의 쉬운 비유"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/149659876-d032ef83-6165-4bf3-8cf4-008f1036e324.png","width":400},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위와 같이 문제 A의 답이 문제 B의 계산에 필요하고, 문제 B의 답이 문제 C의 풀이에 이용되는 문제들을 생각해보자."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Teacher Forcing 미사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"학생은 문제 A, B, C를 순서대로 풀이하고 답 a, b, c를 한꺼번에 작성하여 제출"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n교사는 이 답안지를 보고 a, b, c를 한꺼번에 채점하여 점수를 알려줌."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Teacher Forcing 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"학생은 문제 A를 풀이하고 답 a를 제출\n교사는 답안지를 가져가고, 정답 a를 알려줌"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n학생은 문제 A의 답 a를 가지로 문제 B를 풀이하고 답 b를 제출"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Teacher Forcing 기법의 장단점"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"학습이 빠르다"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"학습 초기 단계에서는 모델의 예측 성능이 나쁘다. 때문에 Teacher Forcing을 이용하지 않으면 잘못된 예측 값을 토대로 Hidden State 값이 업데이트되고, 이 때문에 모델의 학습 속도는 더뎌지게 된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"노출 편향 문제 (Exposure Bias Problem)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"추론 (Inference) 과정에서는 Ground Truth를 제공할 수 없다. 때문에 모델은 전 단계의 자기 자신의 출력값을 기반으로 예측을 이어가야한다. 이러한 학습과 추론 단계에서의 차이 (discrepancy) 가 존재하여 모델의 성능과 안정성을 떨어뜨릴 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"다만 노출 편향 문제가 생각만큼 큰 영향을 미치지 않는다는 2019년 연구 결과가 나와 있다고 한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n(T. He, J. Zhang, Z. Zhou, and J. Glass. Quantifying Exposure Bias for Neural Language Generation (2019), arXiv.)"}]}],"data":{"quirksMode":false}},"excerpt":"Teacher Forcing 본 포스팅을 이해하기 위해서는 다음 글에 대한 이해가 선행되는 것이 좋습니다. RNN (Recurrent Neural Network) LSTM & GRU (Long Short Term Memory & Gated…","fields":{"readingTime":{"text":"5 min read"}},"frontmatter":{"title":"Teacher Forcing","userDate":"31 January 2020","date":"2020-01-31T10:00:00.000Z","tags":["nlp"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/43c1417fedd352e3d13e64f0a4ceec5b/e46c1/teacher_forcing.png","srcSet":"/static/43c1417fedd352e3d13e64f0a4ceec5b/436bd/teacher_forcing.png 750w,\n/static/43c1417fedd352e3d13e64f0a4ceec5b/e46c1/teacher_forcing.png 773w","sizes":"100vw"},"sources":[{"srcSet":"/static/43c1417fedd352e3d13e64f0a4ceec5b/a51c3/teacher_forcing.webp 750w,\n/static/43c1417fedd352e3d13e64f0a4ceec5b/10f41/teacher_forcing.webp 773w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.37516170763260026}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}]}}]}},"relatedPosts":{"totalCount":30,"edges":[{"node":{"id":"06ac0e32-0688-50f0-810d-134ef8b168ab","excerpt":"Decoding Strategy (디코딩 전략) 이번 포스팅에서는 자연어처리 모델의 디코딩 전략에 관해서 다뤄보려고 합니다. 디코딩이란 말처럼 디코딩은 디코더에서\n수행하는 작업입니다. 즉, BERT와 같은 인코더 모델에서 사용하는게 아니라 GPT…","frontmatter":{"title":"Decoding Strategy (디코딩 전략)","date":"2022-01-15T10:00:00.000Z"},"fields":{"readingTime":{"text":"9 min read"},"slug":"/generate/"}}},{"node":{"id":"db36f120-4fb0-5bf7-af53-16447fe6cdd4","excerpt":"Generation with Retrieval 이번에 딥마인드에서 RETRO(Retrieval-Enhanced Transformer) 라는 모델을 내놓았습니다. 문서 retrieval + GPT 기반 모델인데,\n7B 모델임에도 불구하고 2…","frontmatter":{"title":"Generation with Retrieval","date":"2022-01-04T23:00:00.000Z"},"fields":{"readingTime":{"text":"6 min read"},"slug":"/fid_and_rag/"}}},{"node":{"id":"3b4040eb-d53d-5064-beec-cfbf7a7a0fe2","excerpt":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems Paper Review Paper: https://aclanthology.org/2021.naacl-main.12…","frontmatter":{"title":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems Paper Review","date":"2021-12-18T10:00:00.000Z"},"fields":{"readingTime":{"text":"2 min read"},"slug":"/bert_fp/"}}},{"node":{"id":"78976688-33d9-53c4-8489-5099082b9972","excerpt":"GPT (Generative Pre-trained Transformer) 1 gpt1 먼저 알아보고, gpt2에 대해 알아보겠습니다. GPT1 Improving Language Understanding by Generative Pre-Training…","frontmatter":{"title":"GPT (Generative Pre-trained Transformer)","date":"2021-11-23T11:00:00.000Z"},"fields":{"readingTime":{"text":"13 min read"},"slug":"/gpt/"}}},{"node":{"id":"ad5b0c9b-8199-5f10-bfc9-6bb05942e164","excerpt":"Large Scale LM (2) Distributed Programming (작성중) 이 자료는 [해당 link…","frontmatter":{"title":"Large Scale LM (2) Distributed Programming","date":"2021-11-22T11:00:00.000Z"},"fields":{"readingTime":{"text":"17 min read"},"slug":"/big-model2/"}}}]}},"pageContext":{"slug":"/teacher_forcing/","prev":{"excerpt":"Attention 본 포스팅을 이해하기 위해서는 다음 글에 대한 이해가 선행되는 것이 좋습니다. RNN (Recurrent Neural Network) LSTM & GRU (Long Short Term Memory & Gated Recurrent…","frontmatter":{"title":"Attention Mechanism (어텐션 메커니즘)","tags":["nlp"],"date":"2020-01-26T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABXElEQVQoz4WRW0sCYRCG9w93EQj9gy6CzuVVRVl2WCQsw3NWalpBbRpBSVAXlnhoy+Ouu37fE+5KeFUDwzPvwAzf+43CHyGHKaVTdwY2X6ZBHzDAYU8KDCno2Db2qK8MB/5KhHAW4w/C/BpSPYHFDTgIwcI67Lp99o5gegUFMQApGKcc09K2nBfq4RS17UP0eJrGbhA9maW6f8xnPO1wqCtbARQxsvcf5040JjfTeGNFPL4s3tgDnq0Mq9EiU9uXjp7YSKNYeT/tpxzmtUr7KY+Z8zs0sj5az1eIvA9R/6BQz5MqqWjlC85fAtyVzxx9W05yWtqnULlAvVlB6YRnaWtRmpFFusUEzcgS3YdTWtFluo/nNEMz2O+vaN8pMpUd7qpRrhoq9/U4ueoBWj1GrqpS1JMk3tZQvmzXUss9Jrr7ZdR67pVr5sjzwPVvdC3Mjo20YNAHYfFbyz78ABZI/M2kpFnAAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/a9c3aa2f1074a9ebdde0b36f5881c273/e7aa3/attention.png","srcSet":"/static/a9c3aa2f1074a9ebdde0b36f5881c273/ea5bc/attention.png 750w,\n/static/a9c3aa2f1074a9ebdde0b36f5881c273/e7aa3/attention.png 773w","sizes":"100vw"},"sources":[{"srcSet":"/static/a9c3aa2f1074a9ebdde0b36f5881c273/2a863/attention.webp 750w,\n/static/a9c3aa2f1074a9ebdde0b36f5881c273/99eab/attention.webp 773w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.351875808538163}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"8 min read"},"layout":"","slug":"/attention/"}},"next":{"excerpt":"「STATE-OF-THE-ART SPEECH RECOGNITION WITH SEQUENCE-TO-SEQUENCE MODEL」 Review title https://arxiv.org/abs/1712.0176…","frontmatter":{"title":"STATE-OF-THE-ART SPEECH RECOGNITION WITH SEQUENCE-TO-SEQUENCE MODEL Paper Review","tags":["speech","paper"],"date":"2020-02-03T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAABaUlEQVQ4y51T22rCQBDN//9HW/pSFfok9PKiFCy0pRQTNbcmXqjdGt1bsiY5ZdcqUbxQhx12OHP2MLM7a6FiZVlu9nWsLc9zCC725nbN2gWqB5RSSNMU5IcgiqKt/CFR65AYZRSKK3T5AJekgWtyixZ7Bkq9zhAcjUdIpgleyQeuvhq4GNbwxF/OFzT3JgUEFVBUYRKOkcv85D1a+0BD/qtEhhLBjQ+/5oK+UYOZ9d8KN5VmS+Qsw+DdgWf3wTk/WuXBV15xSwSBRLOZoF6P0OnMTrdcHYNtXwnGUYrH+xke7r5hdyWAwuR2u1nH1u5sFUVR8RUpyxiSZGpijRtehb8leGzq16bUEozxkzytZQnO0Gq3EcUxJuMRHMeB67no9XsIP0MEYQjf9zEaxvCDAK47gB/45ufo2PM82LaNNMvWFRYghEAIgeVSYT6fY75YgFJqXlRKaXb9BXXMGAMXwmDaKWMG061rwV98N/SYP6gHTQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/ccbbeff79541c85288ad06b096472221/8e70e/sota_speech.png","srcSet":"/static/ccbbeff79541c85288ad06b096472221/8e70e/sota_speech.png 546w","sizes":"100vw"},"sources":[{"srcSet":"/static/ccbbeff79541c85288ad06b096472221/dc201/sota_speech.webp 546w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.6465201465201466}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"12 min read"},"layout":"","slug":"/sota_sr_speech/"}},"primaryTag":"nlp"}},
    "staticQueryHashes": ["3170763342","3229353822"]}