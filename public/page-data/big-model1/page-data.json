{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/big-model1/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>Large Scale LM (1) Background</h1>\n<p>이 자료는 <a href=\"https://github.com/tunib-ai/large-scale-lm-tutorials\">[해당 link]</a> 를 참고하며 제 언어로 재작성한 글입니다.<br>\n저의 추가적인 메모나 의견이 삽입되거나 삭제된 내용이 있습니다.<br>\n더 퀄리티가 좋은 자료는 위의 링크를 참고하시길 바랍니다.</p>\n<h2>Motivation</h2>\n<p>2020년에 등장한 GPT-3는 역대 최고의 언어모델로 여겨지고 있습니다. 아키텍처보다도 더 많은 데이터, 더 큰 사이즈의 모델이\n가장 중요하다고 여겨지게 만들어준 장본인이죠. 개인적으로는 지금까지 나온 모델 중에는 AGI(Artificial General Intelligence)에 가장 가까운\n모델이라고 생각합니다. 그래서 저는 더 많은 데이터를 모으고, 이런 데이터를 많이 먹을 수 있는 Large-Scale 모델 학습 관련 기술이 굉장히\n중요하다고 생각합니다.</p>\n<h2>모델 아키텍처가 그다지 중요하지 않다?</h2>\n<p>지금까지도 많은 논문이 새로운 모델 아키텍처를 도입하기 위한 연구를 많이 해왔습니다. 하지만 아쉽게도 트랜스포머의 등장 이후로는\n큰 아키텍처의 변화보다는 트랜스포머 모델의 최적화가 더 많은 변화를 가져온 것 같습니다. 그리고 최근 연구 결과에 따르면\n모델 아키텍처 자체는 그렇게 중요하지 않다라는 뉘앙스의 논문도 많이 나오고 있습니다. 그리고 이러한 결과를 뒷받침하듯, GPT-3가 아키텍처는\nGPT-2를 유지하면서 많은 데이터와, 파라미터 수를 늘렸더니 엄청난 성능 향상을 보여줘서 전 세계를 놀라게했죠.</p>\n<img src=\"https://github.com/tunib-ai/large-scale-lm-tutorials/raw/ca29ff9f945a59abcc3e3f1000c4d83de97973d4/images/arch_is_not_important.png\" width=\"500\">  \n<p>그래서 결국 지금까지의 연구 결과들을 놓고보자면, 관건은 데이터와 모델의 크기가 가장 중요한 것 같습니다.\n단순히 벤치마크 성능만 올라가는게 아니라 Fine-tuning 없이도 번역, 요약, 분류 등의 태스크를 하는 등 새로운 지표를 열어서\nPrompt-Engineering이라는 용어까지 나오며 새로운 트렌드로 자리잡았습니다.</p>\n<img src=\"https://github.com/tunib-ai/large-scale-lm-tutorials/raw/ca29ff9f945a59abcc3e3f1000c4d83de97973d4/images/scale_is_all_you_need.png\" width=\"500\">  \n<p>위의 그래프에서 볼 수 있듯이, 모델 성능에 가장 중요한건 모델의 사이즈, 다음이 데이터의 크기라는 걸 볼 수 있습니다.\nY축이 log-scale이라는 것을 생각하면 모델의 크기가 성능에 미치는 영향은 실로 엄청납니다.</p>\n<h2>이대로 간다면..?</h2>\n<img src=\"https://github.com/tunib-ai/large-scale-lm-tutorials/raw/ca29ff9f945a59abcc3e3f1000c4d83de97973d4/images/GPT-X.png\" width=\"500\">  \n<p>이대로 간다면 몇년 뒤에는 GPT-3와도 비교도 안 될 만큼 큰 모델들이 등장하며, 더 마법같은 일을 보여주지 않을까 싶습니다.\n한국에서도 네이버가 빠르게 앞장서서 하이퍼클로바라는 모델을 만들어서 한국어 GPT-3를 만들었고, 카카오브레인도 최근에 GPT-6B 모델을\n오픈소스로 공개하며 앞으로 더 큰 모델을 공개하겠다는 포부를 밝혔죠. Large-Scale LM은 이제는 엄연한 트렌드로 받아들여야 할 것 같습니다.</p>\n<h2>장벽</h2>\n<img src=\"https://github.com/tunib-ai/large-scale-lm-tutorials/raw/ca29ff9f945a59abcc3e3f1000c4d83de97973d4/images/hard_core_engineering.png\" width=\"500\">  \n<p>하지만 현실적으로 Large-Scale 모델 학습은 굉장히 어렵습니다. 개념적으로도 Megatron-LM, Zero 등을 이해해야하며\n뒤이어 따라오는 엔지니어링 능력은 실로 많은 능력을 필요로합니다. 빅데이터 처리 기술 역시 마찬가지고요. 그래서 Large-Scale LM 학습을 위해 필요한 많은 지식들을\n공부하며 블로그에 기록해보려고 합니다. 모델 학습부터 시작해서 데이터 처리, 이후 배포 등 많은 엔지니어링 기술을 필요로 할텐데 하나하나 공부하며 기록해보겠습니다.</p>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Large Scale LM (1) Background"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이 자료는 "},{"type":"element","tagName":"a","properties":{"href":"https://github.com/tunib-ai/large-scale-lm-tutorials"},"children":[{"type":"text","value":"[해당 link]"}]},{"type":"text","value":" 를 참고하며 제 언어로 재작성한 글입니다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n저의 추가적인 메모나 의견이 삽입되거나 삭제된 내용이 있습니다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n더 퀄리티가 좋은 자료는 위의 링크를 참고하시길 바랍니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Motivation"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"2020년에 등장한 GPT-3는 역대 최고의 언어모델로 여겨지고 있습니다. 아키텍처보다도 더 많은 데이터, 더 큰 사이즈의 모델이\n가장 중요하다고 여겨지게 만들어준 장본인이죠. 개인적으로는 지금까지 나온 모델 중에는 AGI(Artificial General Intelligence)에 가장 가까운\n모델이라고 생각합니다. 그래서 저는 더 많은 데이터를 모으고, 이런 데이터를 많이 먹을 수 있는 Large-Scale 모델 학습 관련 기술이 굉장히\n중요하다고 생각합니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"모델 아키텍처가 그다지 중요하지 않다?"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"지금까지도 많은 논문이 새로운 모델 아키텍처를 도입하기 위한 연구를 많이 해왔습니다. 하지만 아쉽게도 트랜스포머의 등장 이후로는\n큰 아키텍처의 변화보다는 트랜스포머 모델의 최적화가 더 많은 변화를 가져온 것 같습니다. 그리고 최근 연구 결과에 따르면\n모델 아키텍처 자체는 그렇게 중요하지 않다라는 뉘앙스의 논문도 많이 나오고 있습니다. 그리고 이러한 결과를 뒷받침하듯, GPT-3가 아키텍처는\nGPT-2를 유지하면서 많은 데이터와, 파라미터 수를 늘렸더니 엄청난 성능 향상을 보여줘서 전 세계를 놀라게했죠."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://github.com/tunib-ai/large-scale-lm-tutorials/raw/ca29ff9f945a59abcc3e3f1000c4d83de97973d4/images/arch_is_not_important.png","width":500},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그래서 결국 지금까지의 연구 결과들을 놓고보자면, 관건은 데이터와 모델의 크기가 가장 중요한 것 같습니다.\n단순히 벤치마크 성능만 올라가는게 아니라 Fine-tuning 없이도 번역, 요약, 분류 등의 태스크를 하는 등 새로운 지표를 열어서\nPrompt-Engineering이라는 용어까지 나오며 새로운 트렌드로 자리잡았습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://github.com/tunib-ai/large-scale-lm-tutorials/raw/ca29ff9f945a59abcc3e3f1000c4d83de97973d4/images/scale_is_all_you_need.png","width":500},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 그래프에서 볼 수 있듯이, 모델 성능에 가장 중요한건 모델의 사이즈, 다음이 데이터의 크기라는 걸 볼 수 있습니다.\nY축이 log-scale이라는 것을 생각하면 모델의 크기가 성능에 미치는 영향은 실로 엄청납니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"이대로 간다면..?"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://github.com/tunib-ai/large-scale-lm-tutorials/raw/ca29ff9f945a59abcc3e3f1000c4d83de97973d4/images/GPT-X.png","width":500},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이대로 간다면 몇년 뒤에는 GPT-3와도 비교도 안 될 만큼 큰 모델들이 등장하며, 더 마법같은 일을 보여주지 않을까 싶습니다.\n한국에서도 네이버가 빠르게 앞장서서 하이퍼클로바라는 모델을 만들어서 한국어 GPT-3를 만들었고, 카카오브레인도 최근에 GPT-6B 모델을\n오픈소스로 공개하며 앞으로 더 큰 모델을 공개하겠다는 포부를 밝혔죠. Large-Scale LM은 이제는 엄연한 트렌드로 받아들여야 할 것 같습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"장벽"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://github.com/tunib-ai/large-scale-lm-tutorials/raw/ca29ff9f945a59abcc3e3f1000c4d83de97973d4/images/hard_core_engineering.png","width":500},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"하지만 현실적으로 Large-Scale 모델 학습은 굉장히 어렵습니다. 개념적으로도 Megatron-LM, Zero 등을 이해해야하며\n뒤이어 따라오는 엔지니어링 능력은 실로 많은 능력을 필요로합니다. 빅데이터 처리 기술 역시 마찬가지고요. 그래서 Large-Scale LM 학습을 위해 필요한 많은 지식들을\n공부하며 블로그에 기록해보려고 합니다. 모델 학습부터 시작해서 데이터 처리, 이후 배포 등 많은 엔지니어링 기술을 필요로 할텐데 하나하나 공부하며 기록해보겠습니다."}]}],"data":{"quirksMode":false}},"excerpt":"Large Scale LM (1) Background 이 자료는 [해당 link…","fields":{"readingTime":{"text":"5 min read"}},"frontmatter":{"title":"Large Scale LM (1) Background","userDate":"22 November 2021","date":"2021-11-22T10:00:00.000Z","tags":["nlp","parallelism","large-scale","lm"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/d6ed5ee3d7d66b561a10d5ef112636a6/31d4c/big-model1.png","srcSet":"/static/d6ed5ee3d7d66b561a10d5ef112636a6/4ec63/big-model1.png 750w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/2df19/big-model1.png 1080w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/e3ec4/big-model1.png 1366w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/31d4c/big-model1.png 1720w","sizes":"100vw"},"sources":[{"srcSet":"/static/d6ed5ee3d7d66b561a10d5ef112636a6/6c332/big-model1.webp 750w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/03806/big-model1.webp 1080w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/698e5/big-model1.webp 1366w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/e01f3/big-model1.webp 1720w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.4872093023255814}}},"author":[]}},"relatedPosts":{"totalCount":30,"edges":[{"node":{"id":"06ac0e32-0688-50f0-810d-134ef8b168ab","excerpt":"Decoding Strategy (디코딩 전략) 이번 포스팅에서는 자연어처리 모델의 디코딩 전략에 관해서 다뤄보려고 합니다. 디코딩이란 말처럼 디코딩은 디코더에서\n수행하는 작업입니다. 즉, BERT와 같은 인코더 모델에서 사용하는게 아니라 GPT…","frontmatter":{"title":"Decoding Strategy (디코딩 전략)","date":"2022-01-15T10:00:00.000Z"},"fields":{"readingTime":{"text":"9 min read"},"slug":"/generate/"}}},{"node":{"id":"db36f120-4fb0-5bf7-af53-16447fe6cdd4","excerpt":"Generation with Retrieval 이번에 딥마인드에서 RETRO(Retrieval-Enhanced Transformer) 라는 모델을 내놓았습니다. 문서 retrieval + GPT 기반 모델인데,\n7B 모델임에도 불구하고 2…","frontmatter":{"title":"Generation with Retrieval","date":"2022-01-04T23:00:00.000Z"},"fields":{"readingTime":{"text":"6 min read"},"slug":"/fid_and_rag/"}}},{"node":{"id":"3b4040eb-d53d-5064-beec-cfbf7a7a0fe2","excerpt":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems Paper Review Paper: https://aclanthology.org/2021.naacl-main.12…","frontmatter":{"title":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems Paper Review","date":"2021-12-18T10:00:00.000Z"},"fields":{"readingTime":{"text":"2 min read"},"slug":"/bert_fp/"}}},{"node":{"id":"78976688-33d9-53c4-8489-5099082b9972","excerpt":"GPT (Generative Pre-trained Transformer) 1 gpt1 먼저 알아보고, gpt2에 대해 알아보겠습니다. GPT1 Improving Language Understanding by Generative Pre-Training…","frontmatter":{"title":"GPT (Generative Pre-trained Transformer)","date":"2021-11-23T11:00:00.000Z"},"fields":{"readingTime":{"text":"13 min read"},"slug":"/gpt/"}}},{"node":{"id":"ad5b0c9b-8199-5f10-bfc9-6bb05942e164","excerpt":"Large Scale LM (2) Distributed Programming (작성중) 이 자료는 [해당 link…","frontmatter":{"title":"Large Scale LM (2) Distributed Programming","date":"2021-11-22T11:00:00.000Z"},"fields":{"readingTime":{"text":"17 min read"},"slug":"/big-model2/"}}}]}},"pageContext":{"slug":"/big-model1/","prev":{"excerpt":"튜닙 블로그 개편 2021.11.16일자로 튜닙 블로그가 개편되었습니다! 기존에는 노션을 이용해 다소 밋밋한 텍스트 위주의 UI…","frontmatter":{"title":"튜닙 블로그 개편","tags":["record"],"date":"2021-11-20T11:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAAD6ElEQVQ4y0VUa2/aZhQ+mKsxYGNjJ2A7GBMokGwpgVzaTVs0rZq0i9Rqnbbu3m1aP3TqpiqYtdhcEkjSpv06Teu0v/pM7/u6zYdH5zm3Rz7vOUDTyQgn0QjL6Qin0xFW8wBniwAXiwDL+QjL2QirGcsdcyxizKMRZhGzx1hELB/gJApA56chnq1CvDwPcXkW4nnMX15EeMHsWYgXZxNcriZ4vpzg2WqCC44QF7F/vhS9l6sINNjp4KDfwY3dLg52u9jf7eBwwHgHh8Mebgy7OBx2uL/P0cUeqx10MOgLvj/oYrDTxeB6B+RWdbjVMhxLRdUqwVlXYa9rcGs61gwF6xUFTlXlcc/Rec4y8nCqGtbMEqxKEdU1FWZFw5qlglij5xjYrFto1i206hb8ugnX1tH2a2j7VTRcU+RdnfOWX4PnVNDYsNDw1lF3TAHbBGVTBDlFyGcI+TShkCXkMgQ5Q1AyIq5kCSVZQI5zubRAhiEl+jJJAhVyCXDIAkU5gZKSgKYkoDLkBbSCiBcV4pbVKq+RYzUS8lkJpMqiscjFiDeXC0KAi8WiXCRPUBV6k2OipbwQZP3sw4iNmWMjs7HYl7DCDKGUI2hKEsUcQZVFXskRCgyyqFVkKZ5IQkFO8hyZmswfWmWF8Zu5NQMVLYs0EfRiir+dqSvQtSwMLceF2NsnEwQ5TUhLBCKCoWZBx48f4b9//sIkeIyjd/fw9b27+PfV3/jjtwe4/cktzJ6O8PMP9/DT91/h0cNf8fvDB/jl/re4e/tjfHh0E1/c+RTffPk57nz2Ee7/+B3IdSy81fZw6+gA9aqOQX8bw50O+h0f7++9jfeG2xhut9Dv+eh3mxhstbj/wTsHOLzeQ8tdQ7fh4ubuNhqey76UkJUIhRTxMVJE8GwD7aaDrW4D3Wt12DUDW70m2psbaPkOBv0evI11pBKElCRGT5KwlMskUFYkGMUk8vx0JL4EtoxMfGtsIXLMlfTVvZbybCkSVEWCXkhyTknp6qizcVMhPmR+4Flx2OyYmXA+tiyWjY+a96YEp7KqoKIrqGgKLF3h2zTKivgdGwXuWwbbsKhjvlku8FhZY1xBpaxALQmfmp6Ntm/jmm+j27TR8m1sMr9po9dy0PadN7Fmo8bjnaaDdpP5jAu72bDhezao7lhwbROeY8J3LXiOxe+S2eaGxeP119wVYHzDseDYJreubcHfsODULNAiCjCdjDGPAoRPGQ+wnAZYhGPMwoCD8dVsjGgSYB6OsYhrozBAOBH8ZDrGkycBaDkb4/LkT5xOhejpLMD5fIxnizFOpgH/az+bj3nDNBI5Jr6as/rxVS4W/x9dJl8G4Ok7gAAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/b5a6d3b176ab7ee59450faa209f9df41/c7240/tunib-tistory.png","srcSet":"/static/b5a6d3b176ab7ee59450faa209f9df41/c7240/tunib-tistory.png 600w","sizes":"100vw"},"sources":[{"srcSet":"/static/b5a6d3b176ab7ee59450faa209f9df41/6d09e/tunib-tistory.webp 600w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}},"author":[]},"fields":{"readingTime":{"text":"2 min read"},"layout":"","slug":"/tunib-blog/"}},"next":{"excerpt":"Large Scale LM (2) Distributed Programming (작성중) 이 자료는 [해당 link…","frontmatter":{"title":"Large Scale LM (2) Distributed Programming","tags":["nlp","parallelism","large-scale","lm"],"date":"2021-11-22T11:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABL0lEQVQY002OW0sCURSF/f8PPfQUIQT1Uj2UJBgZvVR4ATMVEy+gQpqlzjg3ZzxnzpkvnHGmNmz24mOvtXcuiiJMy2NlOCzXTqxdL2BlOvzYNnPLxfYljutjuh5rd4vh7ghEiOkGWW+8HfusXATcPb5zVaxxWaxRKL/R6n1ydvvM9WuTo2KF6uCbQqnOeblK/qnOyUObznjJ8U2di3Kb02KDfKmJVJrcPnVfw/GMbn9KWsL3aQxbbIMgY/PFhNF8AokFFUoG0y8Wqw0pzGmdiNF4TudjHGsdRWgZ8tJtEIQKDkcHkyH1QT8JUyqe95Ue7dEs8eno70PPdbAsM/tGaYVlWKgwzJjvBziWHevUZ6yXbEwjY1mglBIhRGaOtEZIES+lO1or5OFAyoSQhP/YLx8cxPDPpKPhAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/346eaa433ec4cadd1f537f0536913cd3/ca97d/big-model2.png","srcSet":"/static/346eaa433ec4cadd1f537f0536913cd3/260a8/big-model2.png 750w,\n/static/346eaa433ec4cadd1f537f0536913cd3/99085/big-model2.png 1080w,\n/static/346eaa433ec4cadd1f537f0536913cd3/cff5f/big-model2.png 1366w,\n/static/346eaa433ec4cadd1f537f0536913cd3/ca97d/big-model2.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/346eaa433ec4cadd1f537f0536913cd3/72019/big-model2.webp 750w,\n/static/346eaa433ec4cadd1f537f0536913cd3/acb5c/big-model2.webp 1080w,\n/static/346eaa433ec4cadd1f537f0536913cd3/41355/big-model2.webp 1366w,\n/static/346eaa433ec4cadd1f537f0536913cd3/a0759/big-model2.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.30885416666666665}}},"author":[]},"fields":{"readingTime":{"text":"17 min read"},"layout":"","slug":"/big-model2/"}},"primaryTag":"nlp"}},
    "staticQueryHashes": ["3170763342","3229353822"]}