{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/longformer/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>Longformer: The Long-Document Transformer</h1>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2004.05150\">Paper</a> <a href=\"https://github.com/allenai/longformer\">Code</a></li>\n<li>Iz Beltagy et al.</li>\n</ul>\n<h2>Introduction</h2>\n<ul>\n<li>트랜스포머는 긴 시퀀스는 처리하지 못한다는 한계를 가지고 있음</li>\n<li>이유는 시퀀스 길이에 O(n^2)하게 늘어나는 높은 복잡도 때문</li>\n<li>본 논문은 Attention 이루어지는 복잡도를 낮추는 방법을 제안 (O(n))</li>\n<li>BERT는 512 토큰을 limit으로 가지는데, 본 논문 모델은 4096 토큰까지 가능</li>\n<li><code class=\"language-text\">text8</code>, <code class=\"language-text\">enwik8</code>, <code class=\"language-text\">Wiki-Hop</code>, <code class=\"language-text\">TriviaQA</code>에서 State-Of-The-Art 달성</li>\n</ul>\n<h2>Attention Method</h2>\n<img src=\"https://haebinshin.github.io/public/img/longformer/figure2.png\">  \n<p>본 논문에서는 위 그림과 같은 3가지 어텐션 방식을 제안</p>\n<h3>Sliding window Attention</h3>\n<ul>\n<li>크기가 w인 sliding window 내에서만 attention을 수행하는 방법</li>\n<li>이 방법은 텍스트 길이 n에 대해 O(n x w)의 복잡도를 가짐</li>\n<li>이러한 방식은 레이어가 깊어짐에 따라 receptive field가 넓어지는 CNN과 유사함</li>\n</ul>\n<img src=\"https://haebinshin.github.io/public/img/longformer/receptive_field.png\">  \n<ul>\n<li>예) window size가 2일 때, 레이어가 쌓일수록 w만큼 receptive field가 넓어짐.</li>\n</ul>\n<img src=\"https://haebinshin.github.io/public/img/longformer/text_sliding_window_receptive_field.jpg\">\n<ul>\n<li>l x w의 receptive field size를 가지게 됨.</li>\n<li>각 레이어마다 w의 크기를 다르게 주는 방법이 도움이 될 수도 있음</li>\n</ul>\n<h3>Dilated Sliding Window</h3>\n<ul>\n<li>Sliding window attention보다도 receptive field를 더 넓히기 위해 고안된 방법</li>\n<li>Dilated Convolution에서 착안</li>\n</ul>\n<img src=\"https://haebinshin.github.io/public/img/longformer/dilation_convolution.gif\">  \n<ul>\n<li>dilated 값을 줘서 토큰을 d만큼 건너뛰면서 어텐션하도록 하는 방법</li>\n<li>예) window size가 2이고 dilation size가 2일 때, 아래 그림과 같이 w x d만큼 receptive field가 넓어짐</li>\n</ul>\n<img src=\"https://haebinshin.github.io/public/img/longformer/text_dilated_sliding_window_receptive_field.jpg\">\n<ul>\n<li>l x d x w의 receptive field size를 가지게 됨.</li>\n</ul>\n<h3>Global Attention</h3>\n<ul>\n<li>BERT의 [CLS] 토큰 같은 경우는 전체 컨텍스트를 바라봐야하는데, 위의 2가지 방법만으로는 Finetuning하는 태스크에서는 부족한 부분이 있을 수 있음</li>\n<li>따라서 스페셜 토큰 몇 개에 대해서는 global attention을 수행하도록 함.</li>\n<li>전체 토큰 수에 비해서는 스페셜 토큰은 매우 적기 때문에 복잡도는 여전히 O(n)</li>\n</ul>\n<h3>Linear Projections for Global Attention</h3>\n<ul>\n<li>보통의 트랜스포머의 어텐션은 Q, K, V로 이루어 지는데, sliding window 기반 어텐션과 global 어텐션을 위해 sliding Q, K, V와 global Q, K, V 두 세트로 나눠서 어텐션을 계산하도록 구현</li>\n</ul>\n<h2>Experiments</h2>\n<p>2가지 방식으로 평가를 진행.</p>\n<h3>Autoregressive Language Modeling</h3>\n<ul>\n<li>모델 자체의 임베딩 평가를 위함</li>\n<li>character/token 단위의 language modeling을 수행.</li>\n<li><code class=\"language-text\">text8</code>, <code class=\"language-text\">enwik8</code> 데이터셋에서 SOTA를 달성</li>\n<li>본 태스크는 dilated sliding window attention 사용</li>\n</ul>\n<img src=\"https://haebinshin.github.io/public/img/longformer/table_2_3.png\">\n<h3>Pre-training and Fine-tuning</h3>\n<ul>\n<li>RoBERTa 체크포인트로부터 시작해서 학습</li>\n<li>sliding window attention를 사용</li>\n<li>각 태스크에 따라 스페셜 토큰을 지정하여 global attention을 사용</li>\n<li><code class=\"language-text\">WikiHop</code>과 <code class=\"language-text\">TriviaQA</code>에서 SOTA 달성</li>\n</ul>\n<img src=\"https://haebinshin.github.io/public/img/longformer/table8.png\">","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Longformer: The Long-Document Transformer"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/2004.05150"},"children":[{"type":"text","value":"Paper"}]},{"type":"text","value":" "},{"type":"element","tagName":"a","properties":{"href":"https://github.com/allenai/longformer"},"children":[{"type":"text","value":"Code"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Iz Beltagy et al."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Introduction"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"트랜스포머는 긴 시퀀스는 처리하지 못한다는 한계를 가지고 있음"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이유는 시퀀스 길이에 O(n^2)하게 늘어나는 높은 복잡도 때문"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"본 논문은 Attention 이루어지는 복잡도를 낮추는 방법을 제안 (O(n))"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"BERT는 512 토큰을 limit으로 가지는데, 본 논문 모델은 4096 토큰까지 가능"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"text8"}]},{"type":"text","value":", "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"enwik8"}]},{"type":"text","value":", "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"Wiki-Hop"}]},{"type":"text","value":", "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"TriviaQA"}]},{"type":"text","value":"에서 State-Of-The-Art 달성"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Attention Method"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://haebinshin.github.io/public/img/longformer/figure2.png"},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 위 그림과 같은 3가지 어텐션 방식을 제안"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Sliding window Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"크기가 w인 sliding window 내에서만 attention을 수행하는 방법"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이 방법은 텍스트 길이 n에 대해 O(n x w)의 복잡도를 가짐"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이러한 방식은 레이어가 깊어짐에 따라 receptive field가 넓어지는 CNN과 유사함"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://haebinshin.github.io/public/img/longformer/receptive_field.png"},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"예) window size가 2일 때, 레이어가 쌓일수록 w만큼 receptive field가 넓어짐."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://haebinshin.github.io/public/img/longformer/text_sliding_window_receptive_field.jpg"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"l x w의 receptive field size를 가지게 됨."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"각 레이어마다 w의 크기를 다르게 주는 방법이 도움이 될 수도 있음"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Dilated Sliding Window"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Sliding window attention보다도 receptive field를 더 넓히기 위해 고안된 방법"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Dilated Convolution에서 착안"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://haebinshin.github.io/public/img/longformer/dilation_convolution.gif"},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"dilated 값을 줘서 토큰을 d만큼 건너뛰면서 어텐션하도록 하는 방법"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"예) window size가 2이고 dilation size가 2일 때, 아래 그림과 같이 w x d만큼 receptive field가 넓어짐"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://haebinshin.github.io/public/img/longformer/text_dilated_sliding_window_receptive_field.jpg"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"l x d x w의 receptive field size를 가지게 됨."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Global Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"BERT의 [CLS] 토큰 같은 경우는 전체 컨텍스트를 바라봐야하는데, 위의 2가지 방법만으로는 Finetuning하는 태스크에서는 부족한 부분이 있을 수 있음"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"따라서 스페셜 토큰 몇 개에 대해서는 global attention을 수행하도록 함."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"전체 토큰 수에 비해서는 스페셜 토큰은 매우 적기 때문에 복잡도는 여전히 O(n)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Linear Projections for Global Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"보통의 트랜스포머의 어텐션은 Q, K, V로 이루어 지는데, sliding window 기반 어텐션과 global 어텐션을 위해 sliding Q, K, V와 global Q, K, V 두 세트로 나눠서 어텐션을 계산하도록 구현"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Experiments"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"2가지 방식으로 평가를 진행."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Autoregressive Language Modeling"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"모델 자체의 임베딩 평가를 위함"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"character/token 단위의 language modeling을 수행."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"text8"}]},{"type":"text","value":", "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"enwik8"}]},{"type":"text","value":" 데이터셋에서 SOTA를 달성"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"본 태스크는 dilated sliding window attention 사용"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://haebinshin.github.io/public/img/longformer/table_2_3.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Pre-training and Fine-tuning"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"RoBERTa 체크포인트로부터 시작해서 학습"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"sliding window attention를 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"각 태스크에 따라 스페셜 토큰을 지정하여 global attention을 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"WikiHop"}]},{"type":"text","value":"과 "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"TriviaQA"}]},{"type":"text","value":"에서 SOTA 달성"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://haebinshin.github.io/public/img/longformer/table8.png"},"children":[]}],"data":{"quirksMode":false}},"excerpt":"Longformer: The Long-Document Transformer Paper Code Iz Beltagy et al. Introduction 트랜스포머는 긴 시퀀스는 처리하지 못한다는 한계를 가지고 있음 이유는 시퀀스 길이에 O(n^…","fields":{"readingTime":{"text":"4 min read"}},"frontmatter":{"title":"Longformer Paper Review","userDate":"6 February 2021","date":"2021-02-06T23:46:37.121Z","tags":["nlp","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/ef32af94bd92c05cbfebe0cb00c3966e/597e6/longformer.png","srcSet":"/static/ef32af94bd92c05cbfebe0cb00c3966e/597e6/longformer.png 512w","sizes":"100vw"},"sources":[{"srcSet":"/static/ef32af94bd92c05cbfebe0cb00c3966e/3d2a6/longformer.webp 512w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.8046875000000001}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}]}}]}},"relatedPosts":{"totalCount":30,"edges":[{"node":{"id":"06ac0e32-0688-50f0-810d-134ef8b168ab","excerpt":"Decoding Strategy (디코딩 전략) 이번 포스팅에서는 자연어처리 모델의 디코딩 전략에 관해서 다뤄보려고 합니다. 디코딩이란 말처럼 디코딩은 디코더에서\n수행하는 작업입니다. 즉, BERT와 같은 인코더 모델에서 사용하는게 아니라 GPT…","frontmatter":{"title":"Decoding Strategy (디코딩 전략)","date":"2022-01-15T10:00:00.000Z"},"fields":{"readingTime":{"text":"9 min read"},"slug":"/generate/"}}},{"node":{"id":"db36f120-4fb0-5bf7-af53-16447fe6cdd4","excerpt":"Generation with Retrieval 이번에 딥마인드에서 RETRO(Retrieval-Enhanced Transformer) 라는 모델을 내놓았습니다. 문서 retrieval + GPT 기반 모델인데,\n7B 모델임에도 불구하고 2…","frontmatter":{"title":"Generation with Retrieval","date":"2022-01-04T23:00:00.000Z"},"fields":{"readingTime":{"text":"6 min read"},"slug":"/fid_and_rag/"}}},{"node":{"id":"3b4040eb-d53d-5064-beec-cfbf7a7a0fe2","excerpt":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems Paper Review Paper: https://aclanthology.org/2021.naacl-main.12…","frontmatter":{"title":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems Paper Review","date":"2021-12-18T10:00:00.000Z"},"fields":{"readingTime":{"text":"2 min read"},"slug":"/bert_fp/"}}},{"node":{"id":"78976688-33d9-53c4-8489-5099082b9972","excerpt":"GPT (Generative Pre-trained Transformer) 1 gpt1 먼저 알아보고, gpt2에 대해 알아보겠습니다. GPT1 Improving Language Understanding by Generative Pre-Training…","frontmatter":{"title":"GPT (Generative Pre-trained Transformer)","date":"2021-11-23T11:00:00.000Z"},"fields":{"readingTime":{"text":"13 min read"},"slug":"/gpt/"}}},{"node":{"id":"ad5b0c9b-8199-5f10-bfc9-6bb05942e164","excerpt":"Large Scale LM (2) Distributed Programming (작성중) 이 자료는 [해당 link…","frontmatter":{"title":"Large Scale LM (2) Distributed Programming","date":"2021-11-22T11:00:00.000Z"},"fields":{"readingTime":{"text":"17 min read"},"slug":"/big-model2/"}}}]}},"pageContext":{"slug":"/longformer/","prev":{"excerpt":"Computer Architecture Review 오랜만에 컴퓨터 구조에서 배운 내용을 조금 복습해보며 감을 잡기 위함 컴퓨터가 코드를 처리하는 과정 Read Code Assembly 변환 CPU에서 실행 CPU에서 하나의 명령(Ex) Add…","frontmatter":{"title":"Computer Architecture Review","tags":["cs"],"date":"2021-02-05T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAACGklEQVQ4y21UCXLbMAz0/7/YVLYsx4oO6+BNguSmgI8kbTmDASGSi2uhQ60VvHwImOdZhIhgjEHTNPjdNLJ3zol9uVzgvcc4ThinCVobuT9NE/q+xwH/WbUU1JJBKYFi5A9IMT4kIBP9uE8PO6WEg6aMyViMymDSRvSgNLaQYHIV2SNh9QG2QOzVR6iUsaeMXCEOSimIMeIw+ghrLahWaGuxa41NKVgfwMXItYJKQUiElLPYiSPi+7nAl4qSCaXWO+AcIlLwEvLtdoPWGh99j33f4Z2DsxbWGKh9x75tcm5CBJWKLeV/ASfPgEEAt8eDEMKrLrxyzjifz3Lef3xgsQ4uFxgqKLinXL8iTHBaSRoMxgdcj2dNWNjBMAzSbbZRyqN7VRpI3wGXXDDMM361Z3Rdh2VZhCKntsXxeIRSCuM4Cp2ssaKZMsZaOO8FjO+/ukwVMCFIrTg1bhBH07y94dJ1ArauK9q2Fa6xQ96zZuFv7ITLwc5/8JDDZuLyg+O1R9t14oibdTqd8P7+LukzsVmzIwZje54mvI0zDgzyt3DozXSTVOKjQdfrvfNcpycQTwrX+rYs8JyZ8V8RPsGkFkQ4DSOMVhJZf71CK/U1GSnBO/uiFTviBs0u3AGfQE9NOWOwHj4l3LYdu7HCvVQqYilwPJIVIpn/A5SF/P33CL8DcnNMotfoaSpYQ5TJuO8TFGWR5xmP4vonwk/QczzzyGBGjgAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/83debf50782f47536b48f20a7c0c3d46/ac5ac/computer-architecture.png","srcSet":"/static/83debf50782f47536b48f20a7c0c3d46/1d01f/computer-architecture.png 750w,\n/static/83debf50782f47536b48f20a7c0c3d46/ac5ac/computer-architecture.png 798w","sizes":"100vw"},"sources":[{"srcSet":"/static/83debf50782f47536b48f20a7c0c3d46/96ac1/computer-architecture.webp 750w,\n/static/83debf50782f47536b48f20a7c0c3d46/2afea/computer-architecture.webp 798w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.7017543859649122}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"2 min read"},"layout":"","slug":"/computer-ar-review/"}},"next":{"excerpt":"PORORO Text-To-Speech (TTS) 얼마전에 저희 팀에서 공개한 PORORO: Platform Of neuRal mOdels for natuRal language prOcessing 라이브러리에 제가 공들여만든 TTS…","frontmatter":{"title":"PORORO Text-To-Speech (TTS)","tags":["speech","toolkit","record"],"date":"2021-02-16T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAADMElEQVQ4y3WTbUyVZRjHr/t+znke4LwhKih4PMiRlx2JDkfXmvDBGMs5yLS5MHEHEOdYwQrFMXrTUEcLXW1y7Ph6WNBxOntxHJMiZa5aWzXXiNl6oWJ8CJZjfelDffm185zp7EP39t917b633/2/711/sQyNZShMLZhKMJQg8v8yDMOuSilbmf6eFJJtKLIMlYEqwTQUHrcLn9eLy+UiOzsbj8eD1+vFsiwbkJOTc/+CNEQ/aMLtEFxpOTVahNI1AWZmZpidnWV6eprx8XHm5uZYWFggHo/T29vL/Pw8yWQSQ6v7ziyHwpelkAJTWG4KeaaQhnssJ9HmXaRXV1cngUCAyclJvpuaIhKJ2PsTE5+ytXGLbSDH1BT5DEIFBuEiA/GbQqEpFDiFfEtjilC4bAmLi4uEw1X2M2KxGKlUyu479rUzN/srifPnWOZ2UrlCU1tisLFYs36lQpqKhYZCoXapUOlV5CuhujxoO6mv22RDkhcvcevWTdaHQ7wzMsIHV1P2eUudny1lQm2xQUWewu9WyLtPCvHNQn+NsLdSUbtciPjzuH4tRahinQ08/FIfidjr1FUX8c3Nyyz8/DXDR/eyY51QX+7gkYCDyGon4dUmcvBRoXOD0FYlbC0RGkM+Ysf6GBp4mbdfbWb0eCep2G7Gzh9guCfIjUQP7w9GOdu/h4bqXMJ+TeUqk7KVTtauMJHBbcKJHcLJ3cKFDuG9XuHHa0389ctp/vnpBbg7zN/fP8NvH+/iUrdwY7SPy6f6uJIYpKGmlLVLhWC+iT/PQVGuA4lHhZEOYeyg8Pkx4c4Z4e5XTfxxO8qfPxzm9y9buXP9Oa4OtdG1rYq3+rs41B3lwLOtVKzKZYkl+LIMPJbGbWmkq0bofUw40iC8uVM4t09zO3WUmS+G+HbiLFOffcj4xZOkriQZvXCGxs319L/yIp98NMaG8EP2H1sOjaElM+Cb/MLjJcIT5cJTIeHphzWJN/Yzeuo42zeW83zrdk4MvMbAkUO0NO+kZ383ba0ttLfvoWRNIBNHnUlLesgl6BZKvUKZL1ODXsGfI3hEcGnBSuf7wSzr/2bbTsq9XhT/AnHU0lj4MouRAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/f50103e49747efd11b2901a9ce98f113/b444b/tts.png","srcSet":"/static/f50103e49747efd11b2901a9ce98f113/b444b/tts.png 600w","sizes":"100vw"},"sources":[{"srcSet":"/static/f50103e49747efd11b2901a9ce98f113/9ff6b/tts.webp 600w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.6666666666666666}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAZCAYAAAAxFw7TAAAACXBIWXMAAAsTAAALEwEAmpwYAAAGs0lEQVQ4yx2RaUzUiRmH/583TRO7csw9HAqIAoKgCMoNw8wwM8AwXDIzMAw4AwyXwzGAXEu5lQEWFZBDwAMC4hm16EZd1911u+12t+mxrkn7wSZtsmm3SdMPTZ4G3uT58n548vu9r2DIjEWXHkfGqRhSTsZy+vhxTicmkZKSQY7WQEVZGXarFbulkua6Zj7qHWbSe5WlpU1urG0z5Z2lo70Xx1knZYUlCDnpxyjQJFJiSKUoJw1dVjIZKckkJSWj1eRQVWbGWWGjqtKBp+U8k955bm0+58tf/4W33/+NnZ3PmfYu4m5qp7zEglBj0VCam0Jpbjrm/GysBVqsRbkUG3MxGvKwlZTiqqyk7mwdvecHmJ+/yc7zb/nx3/9jd/7w/T9YvnaXrvY+KsyVCN5+GxN9lQy2WfHUllBvM1FbXkSdrYwmRyW1lRW0OJ30ezxMjI2zvLDKJ88+5d279/zzX//l5Zt3zFy+SYu7C/OZcoRr003cmm1lY97D1rVutlb62FjsZ/VKP801Ngz6fAx6ExXWKlrc7VwY87J5a507tx9w995zFpa36O4dwelwYcwzIXxyb4qXT2b57OkCX75Y49s3t/nh94/oanUQFxdPdoYKm8lIh7OcPreL8x4PvT1DTIx7GR3x0t0zQI3DRYXFRn6uEWFurIb71wf44tkCf/zNNj/9/Q1PHy4TGhKONvU0w9VG5s6VsdBRyWyXg9HWOloaGmhpamV2fIRmVz0mYwl6Qz55u8Kx7lKGWo1M9pqZG3ewszVEW6OV8NBwes1qlhsLWPNYuNZh5aqnkpl2B+frqzFbKvjt0x3eP7nPp4/u0VPnIj05DWFjqZtWp4a22hzanDl81GxCk3EKdUIsg+VqLtj1XG8vY6PHzoCjlEZrMS3VZopMhdxdXoD3b/e+/dODx1woPIOweXOY/v6z1NUU4LTn0d1WjlaVQkHyCWrzMmkwqrnsKmKtvZyBqmI6y4tosxZRotdxsbmeLxam+e7eFr9buclSfR3CzuNLbGxdZGT0HFfnB/js9TpdnS5sqgQ6i1W0mrKZqdIxU5XDRLWRWqMGzxkd9cV5nNVlcaNNzYDNwHyXjdVeM8L97XFWVgfo76th9koPr17dYHS0HXdhKmPWTIbMKqZsWQyXpnOxXM18TS5jdj3u0lwWx+0s/rIAqyaeCx0mrvYYELY3R1hc7sPTbuPyTBePH80yOtZKd3k26/XZ3HYbWHJkc8WmZcqSxXpTPlN2He4iHV/vdPJ6vYkmcx7DnecY76xD2N4YYm6ukxa3mUsfd3B3e4rxCQ/eDgsrzgzutOUx71AzadPwsS2btYZcJu0ayjUpVJXm0VyWj8WgoTBHRWm+FmHjRj+Tk83U1RbgvehmdWWAgaFGvKMurlSlsOzSMGnJ4LJVzaApjWm7iolKNanHj3Ew5DAHA0I4dCCU6Igo4mNiEV488fLwzijXl3v51YNJnu9c4uGdCV6+WMLr0rPmUjFdkspFfSoThWlcc+moVp8iKCiE+JhoTh+NJiQwmIiwQ8RFRiK8+2aZP3+9yNtvVnn33U3++qdN3v9wl//8+Jyp0SaqMo+w7tZxuy2flQY91doEwkJCiQo/gkKqIEwZyAFlIKFBQcTHRCJ89XSc14/HeHZniO3VHlYvtzJ3oZH5iUYanSakcjkZJyIoSo8jIToCsTKYuLBQavMzKTSoiTkaTfiBA6QkHCch9iiCXnUMVVIkqSePkJUagzEnkSqzCkupCr02jfi4SKTKQHzEcnzFcpKiDtOqP0lH3jGqjWlkq7VkZaSjy0rlUHAgQsqpQ5TkJdBSl8tgTwXnPRZ0+iSORIZzIDSUhoIEBswpFKfF0JybwJZ7965ZnEk6jFjkR4AygKjISNSqTHTabITBzmJmRuz0tFvIykxEHqDkQ5EEuVKBRKEgMTqM640qXg2auNWqo9+cTPaJQ8hkMhQKJQq5nODAQGKio6murkJYnz1Hb3sZYok/fv4iAo4cRhoUjEypJCAwCF+xAm1iBPON2YQdDMTHX4pIIkUu30WGQq4gOCiIqMgIkpOTEe6tdhN/Mpyf7/sZ8gAZobHRyIIOIgsIQKFUIpXKCQ4O4HhUKH5iCQqFHKVSsSeUyiVIZTLkuymDg/bEwvRwLT7iffhJfoGfTISfTIxYKUOyW1kuQyaXIpaK8ReLkO5JZHvJdvf+EtFeK5HIH7FYtIdgM6v4YN8H+Mp88JH64SMX4asQ4y8XI5KJEEtFiCT+iKRiJLJduQSJVIxYJuZDX1/27fdhv88u+/Hx9eH/E3w7u01Rad8AAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"1 min read"},"layout":"","slug":"/pororo-tts/"}},"primaryTag":"nlp"}},
    "staticQueryHashes": ["3170763342","3229353822"]}