{
    "componentChunkName": "component---src-templates-tags-tsx",
    "path": "/tags/parallelism/",
    "result": {"data":{"allTagYaml":{"edges":[{"node":{"id":"speeches","description":"Some of the greatest words ever spoken.","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#282818","images":{"fallback":{"src":"/static/4dd7b5f567d2699202bc626bbcda5936/7f071/speeches-cover.jpg","srcSet":"/static/4dd7b5f567d2699202bc626bbcda5936/7a1a9/speeches-cover.jpg 750w,\n/static/4dd7b5f567d2699202bc626bbcda5936/25ec5/speeches-cover.jpg 1080w,\n/static/4dd7b5f567d2699202bc626bbcda5936/28e43/speeches-cover.jpg 1366w,\n/static/4dd7b5f567d2699202bc626bbcda5936/7f071/speeches-cover.jpg 1400w","sizes":"100vw"},"sources":[{"srcSet":"/static/4dd7b5f567d2699202bc626bbcda5936/662b3/speeches-cover.webp 750w,\n/static/4dd7b5f567d2699202bc626bbcda5936/6e6c6/speeches-cover.webp 1080w,\n/static/4dd7b5f567d2699202bc626bbcda5936/e4be3/speeches-cover.webp 1366w,\n/static/4dd7b5f567d2699202bc626bbcda5936/04d29/speeches-cover.webp 1400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.4407142857142857}}}}}]},"allMarkdownRemark":{"totalCount":5,"edges":[{"node":{"excerpt":"GPT (Generative Pre-trained Transformer) 1 gpt1 먼저 알아보고, gpt2에 대해 알아보겠습니다. GPT1 Improving Language Understanding by Generative Pre-Training…","frontmatter":{"title":"GPT (Generative Pre-trained Transformer)","excerpt":null,"tags":["nlp","parallelism","large-scale","lm"],"date":"2021-11-23T11:00:00.000Z","image":null,"author":[]},"fields":{"readingTime":{"text":"13 min read"},"layout":"","slug":"/gpt/"}}},{"node":{"excerpt":"Large Scale LM (2) Distributed Programming (작성중) 이 자료는 [해당 link…","frontmatter":{"title":"Large Scale LM (2) Distributed Programming","excerpt":null,"tags":["nlp","parallelism","large-scale","lm"],"date":"2021-11-22T11:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/346eaa433ec4cadd1f537f0536913cd3/ca97d/big-model2.png","srcSet":"/static/346eaa433ec4cadd1f537f0536913cd3/260a8/big-model2.png 750w,\n/static/346eaa433ec4cadd1f537f0536913cd3/99085/big-model2.png 1080w,\n/static/346eaa433ec4cadd1f537f0536913cd3/cff5f/big-model2.png 1366w,\n/static/346eaa433ec4cadd1f537f0536913cd3/ca97d/big-model2.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/346eaa433ec4cadd1f537f0536913cd3/72019/big-model2.webp 750w,\n/static/346eaa433ec4cadd1f537f0536913cd3/acb5c/big-model2.webp 1080w,\n/static/346eaa433ec4cadd1f537f0536913cd3/41355/big-model2.webp 1366w,\n/static/346eaa433ec4cadd1f537f0536913cd3/a0759/big-model2.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.30885416666666665}}},"author":[]},"fields":{"readingTime":{"text":"17 min read"},"layout":"","slug":"/big-model2/"}}},{"node":{"excerpt":"Large Scale LM (1) Background 이 자료는 [해당 link…","frontmatter":{"title":"Large Scale LM (1) Background","excerpt":null,"tags":["nlp","parallelism","large-scale","lm"],"date":"2021-11-22T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/d6ed5ee3d7d66b561a10d5ef112636a6/31d4c/big-model1.png","srcSet":"/static/d6ed5ee3d7d66b561a10d5ef112636a6/4ec63/big-model1.png 750w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/2df19/big-model1.png 1080w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/e3ec4/big-model1.png 1366w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/31d4c/big-model1.png 1720w","sizes":"100vw"},"sources":[{"srcSet":"/static/d6ed5ee3d7d66b561a10d5ef112636a6/6c332/big-model1.webp 750w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/03806/big-model1.webp 1080w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/698e5/big-model1.webp 1366w,\n/static/d6ed5ee3d7d66b561a10d5ef112636a6/e01f3/big-model1.webp 1720w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.4872093023255814}}},"author":[]},"fields":{"readingTime":{"text":"5 min read"},"layout":"","slug":"/big-model1/"}}},{"node":{"excerpt":"DeepSpeed Usage…","frontmatter":{"title":"DeepSpeed Usage","excerpt":null,"tags":["nlp","parallelism","large-scale"],"date":"2021-10-30T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#081868","images":{"fallback":{"src":"/static/9b0c4df957330fe4209b1e9f8e708409/67a35/deepspeed.png","srcSet":"/static/9b0c4df957330fe4209b1e9f8e708409/67a35/deepspeed.png 224w","sizes":"100vw"},"sources":[{"srcSet":"/static/9b0c4df957330fe4209b1e9f8e708409/f42a0/deepspeed.webp 224w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}},"author":[]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/deepspeed/"}}},{"node":{"excerpt":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism ​ Mohammad Shoeybi et al. 2019. NVIDIA Corp. ​ Summary…","frontmatter":{"title":"Megatron LM Paper Review","excerpt":null,"tags":["nlp","parallelism","paper"],"date":"2020-12-03T10:00:00.000Z","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/a4dec0b9c36035e9191658ce9647ae73/a70e6/megatron.png","srcSet":"/static/a4dec0b9c36035e9191658ce9647ae73/37b55/megatron.png 750w,\n/static/a4dec0b9c36035e9191658ce9647ae73/a70e6/megatron.png 791w","sizes":"100vw"},"sources":[{"srcSet":"/static/a4dec0b9c36035e9191658ce9647ae73/0b2ce/megatron.webp 750w,\n/static/a4dec0b9c36035e9191658ce9647ae73/c471e/megatron.webp 791w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5170670037926676}}},"author":[]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/Megatron-lm/"}}}]}},"pageContext":{"tag":"parallelism"}},
    "staticQueryHashes": ["3170763342","3229353822"]}