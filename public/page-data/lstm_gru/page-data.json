{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/lstm_gru/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h2>LSTM &#x26; GRU</h2>\n<p>본 포스팅을 이해가기 위해서는 아래 글에 대한 이해가 선행되는 것이 좋습니다.</p>\n<ul>\n<li><a href=\"sooftware.io/rnn\">RNN (Recurrent Neural Network)</a></li>\n</ul>\n<h2>LSTM 등장 배경</h2>\n<p>RNN은 순환 경로를 포함하여 과거의 정보를 기억할 수 있었다.<br>\n구조가 단순하다는 장점이 있지만, 성능이 좋지 못하다는 단점도 존재한다.</p>\n<p>이러한 단점의 원인은 많은 경우에 시계열 데이터에서 시간적으로 많이 떨어진<br>\n장기 의존 관계(Long Term)를 잘 학습할 수 없다는 데 있다.</p>\n<p>여기서 “장기 의존 관계”가 무엇인지 짚고 넘어가자.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Sooft는 그의 방에서 TV를 보고 있었다. Ware는 그의 방으로 들어갔다. \n그리고 Ware는 ?에게 '안녕'이라고 인사를 했다.</code></pre></div>\n<p>위의 예시를 보자. ?에 들어갈 단어는 ‘Sooft’이다.<br>\n위의 문제에 올바르게 답하려면, 앞의 “Sooft는” 이라는 정보를 기억해둬야 한다.</p>\n<p>하지만 기본적인 RNN의 구조에서는 이러한 장기 의존 관계에 취약하다.</p>\n<h2>RNN의 문제점</h2>\n<p>그렇다면 왜 RNN은 이러한 장기 의존 관계에 대해서 약한 것일까?</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134299504-6704fb9d-2578-4c38-a5e1-0de5438bf546.png\" width=\"500\">  \n<p>이 점에 대해서는 RNN의 Backpropagation을 살펴보면 알 수 있다.<br>\n위의 그림과 같이 RNN에서의 Backpropagation은 RNN 계층이 과거 방향으로<br>\n시간을 거슬러 가면서 gradient를 전달하게 된다.</p>\n<p>하지만 이러한 기울기는 RNN 계층이 길어지게 되면 기울기가 작아지게 된다.<br>\n이를 Vanishing Gradient Problem(기울기 소실)이라고 한다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134299528-6663aac1-8417-4d13-b4e8-f55751962ab0.png\" width=\"500\">\n<p>그럼 RNN 계층에서 왜 Vanishing Gradient이 일어나는 원인을 살펴보자.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134299677-12bf24c3-0833-4c88-85e1-5b8d4da2db66.png\" width=\"500\">\n<p>위는 RNN 셀의 구조이다.</p>\n<p>위의 구조 중 ‘tanh’에만 주목해보자.<br>\ntanh와 dtanh의 그래프 모양은 다음과 같다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134299686-3c4d54ae-5571-4be9-a6d1-ded240259e7a.png\" width=\"500\">  \n<p>여기서 우리는 gradient에 관심이 있으므로, dtanh에 주목해보면<br>\ndtanh 값은 항상 0~1 사이의 값인 것을 알 수 있다.</p>\n<p>이는 역전파에서 gradient가 tanh 노드를 지날 때마다 값이 계속 작아진다는 의미이다.<br>\ntanh를 T번 통과하게 되면 gradient도 T번 반복해서 작아지게 된다.<br>\n그렇기 때문에 RNN 계층이 길어지게 되면 Vanishing Gradient Problem이 발생하게 된다.</p>\n<h2><strong>L</strong>ong <strong>S</strong>hort <strong>T</strong>erm <strong>M</strong>emory (LSTM)</h2>\n<p>이제 이러한 Vanishing Gradient를 일으키지 않는다는 LSTM의 구조에 대해 살펴보자.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134299866-d4c1a2b3-b35a-47b6-bd16-1b921459fdc6.png\" width=\"500\">  \n<p>위 그림은 RNN과 LSTM의 인터페이스를 비교한 그림이다.</p>\n<p>인터페이스만 보더라도 LSTM 계층에는 c라는 경로가 추가된 것을 알 수 있다.</p>\n<p>이 c를 기억 셀<sup>memory cell</sup>이라 하며, LSTM의 전용 기억 매커니즘이다.</p>\n<p>기억 셀의 특징은 데이터를 LSTM 계층 내에서만 주고 받고, 다른 계층으로는 출력하지 않는다는 것이다.<br>\n즉, 이는 LSTM도 내부적으로만 다르지 사용하는 입장에서는 RNN과 같은 인터페이스를 갖는다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134299986-0311e778-edd4-4fd6-86b2-4093ecbacd7d.png\" width=\"600\">  \n<p>이제 LSTM의 구조를 차분히 살펴보자.<br>\n앞서 이야기한 것처럼, LSTM에는 기억 셀 c<sub>t</sub>가 있다.<br>\n이 c<sub>t</sub>에는 시각 t에서의 LSTM의 메모리가 저장되어 있는데, 과거로부터 시각 t까지에 필요한<br>\n메모리가 저장되어 있다고 가정하자.</p>\n<p>그리고 필요한 정보를 모두 간직한 이 메모리를 바탕으로 외부 계층에 Hidden State h<sub>t</sub>를 출력한다.<br>\n이때 출력하는 h<sub>t</sub>는 다음 그림과 같이 기억 셀의 값은 <strong>tanh</strong> 함수로 변환한 값이다.<br>\n(여기서의 tanh는 각 요소에 tanh 함수를 적용한다는 뜻이다.)</p>\n<p>여기서의 핵심은 3개의 입력 (x<sub>t</sub>, h<sub>t-1</sub>, c<sub>t-1</sub>)를 이용하여 구한 c<sub>t</sub>를 사용해<br>\nHidden State h<sub>t</sub>를 계산한다는 것이다.</p>\n<p><strong>※ LSTM 구조의 핵심은 ht는 단기상태 (Short Term) ct는 장기 상태 (Long Term)라고 볼 수 있다. ※</strong></p>\n<h2>게이트<sup>gate</sup></h2>\n<p>여기서 뒷 내용을 이해하기 위해 게이트의 개념에 대해 이해하고 넘어가자.<br>\n게이트는 데이터의 흐름을 제어한다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134300359-a89a80a6-0d78-495f-b3c5-143818f45018.png\" width=\"600\">\n<p>위의 그림처럼 물의 흐름을 제어하는 것이 게이트의 역할이다.</p>\n<p>위의 그림처럼 LSTM에서의 게이트는 ‘열기/닫기’ 뿐 아니라, 어느 정도 열지를 조절할 수 있다.<br>\n그리고 이 열기 ~ 닫기 까지의 정도를 0.0 ~ 1.0의 실수로 표현할 수 있다.<br>\n그리고 이 0.0 ~ 1.0의 값이 다음으로 넘어갈 데이터의 양을 결정한다 !!</p>\n<p>여기서 중요한 점은 <strong>‘게이트를 얼마나 열지’라는 것도 데이터로부터 자동으로 학습된다는 점</strong>이다.</p>\n<p>그리고 여기서 0.0 ~ 1.0이라는 범위에 주목해보자.<br>\n우리는 이러한 범위를 가지는 매우 좋은 함수를 알고있다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134300507-0b0111d5-0917-4e12-8a41-919fc9fc6fad.png\" width=\"600\">\n<p>바로 Sigmoid 함수이다.</p>\n<p>Sigmoid 함수를 이용하면 어떤 값을 넣더라도 0.0 ~ 1.0의 값을 가질 수 있다.<br>\n그렇기 때문에 LSTM의 게이트에서는 tanh가 아닌 Sigmoid 함수를 사용한다.</p>\n<h2>LSTM의 Output 게이트</h2>\n<p>다시 LSTM 이야기로 돌아와보자.<br>\n게이트 얘기를 하기 전에 Hidden State h<sub>t</sub>는 기억 셀 c<sub>t</sub>에 단순히 tanh 함수를 적용한것 뿐이라고 했다.</p>\n<p>그럼 방금 배운 게이트의 개념을 tanh(c<sub>t</sub>)에 적용하는 것을 생각해보자.<br>\n즉, tanh(c<sub>t</sub>)의 각 원소가 ‘다음 시각의 Hidden State에 얼마나 중요한가’를 조정해보자.</p>\n<p>output 게이트의 열림 상태의 계산식은 다음과 같다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134300707-e5302087-99e9-4985-a1a5-79c44ba4aff0.png\" width=\"600\">  \n<p>위의 식을 보게되면 RNN 계층의 내부 계산에서 tanh가 아닌 Sigmoid를<br>\n사용했다는 점만 다르다는 것을 확인할 수 있다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134300788-173eac4b-0991-4a61-8c21-9795a9875f44.png\" width=\"600\">\n<p>다음은 tanh(c<sub>t</sub>)에 Output게이트를 추가한 모습이다.<br>\noutput 게이트에서 수행하는 식은 σ로 표시했다.</p>\n<p>그리고 이 σ의 출력을 o라고 한다면, h<sub>t</sub>는 다음과 같이 계산된다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134300906-386cbac8-02a4-442b-a2c8-41bcc1d4182e.png\" width=\"150\">  \n<p>여기서의 ⊙는 Hardmard Product (아다마르 곱) 이라고 하여, 행렬의 원소별 곱셈을 의미한다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134300914-cee72f4d-2a6d-4c68-a78e-c0d8c08740bc.png\" width=\"600\">  \n<h2>LSTM의 forget 게이트</h2>\n<img src=\"https://user-images.githubusercontent.com/42150335/134301001-e1c7f1e9-0060-4f76-ab07-5e7215e33114.png\" width=\"600\">  \n<p>다음으로 할 일은 잊을건 잊어버리는 것이다.<br>\n굳이 필요없는 정보를 계속 들고갈 필요는 없다.<br>\n그러므로 기억 셀에 ‘무엇을 잊을까’를 지시하는 것이다.<br>\n여기에도 앞의 게이트 개념을 사용한다.</p>\n<p>전 층에서 넘어온 기억셀 c<sub>t-1</sub>에 대해서 불필요한 개념을 잊게 해주는<br>\n게이트를 forget 게이트라고 한다.</p>\n<p>여기서도 위의 Output 게이트와 마찬가지로 다음 식을 수행한다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134301091-1e0c21d7-87cf-4e1b-9472-cd1d75ffb3c0.png\" width=\"250\">\n<p>Output게이트의 식과 동일한 것을 확인할 수 있다.<br>\n여기서도 마찬가지로, forget게이트의 출력인 f와 이전 기억 셀 c<sub>t-1</sub>의 아다마르 곱으로 계산한다.</p>\n<h2>LSTM의 새로운 기억 셀</h2>\n<img src=\"https://user-images.githubusercontent.com/42150335/134301235-a7d0a8f5-151c-4ed6-88a9-32057a50cd5a.png\" width=\"600\">  \n<p>forget 게이트를 거치면서 이전 시각의 기억 셀로부터 잊어야 할 기억이 삭제됐다.<br>\n이제는 새로 들어온 데이터로부터 정보를 추가해야한다.<br>\n그러기 위해서 위의 그림과 같이 tanh 노드를 추가한다.</p>\n<p>이 때, Sigmoid가 아닌 tanh 노드를 추가하는 이유는 이 ‘새로운 기억 셀’은 게이트가 아닌,<br>\n새로운 데이터를 기억 셀에 추가하는 것이기 때문이다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134301294-71bbd02a-39fe-4beb-8661-85b9f985c0cf.png\" width=\"400\">  \n<p>이제 이렇게해서 잊는 것 뿐만이 아닌, 새로운 정보까지 추가가 되었다.</p>\n<h2>LSTM의 input 게이트</h2>\n<img src=\"https://user-images.githubusercontent.com/42150335/134301396-8a91d2ad-6775-450f-acc5-10a1b86709cc.png\" width=\"600\">\n<p>마지막으로 새로운 정보가 들어있는 g에 게이트를 하나 추가할 생각이다.<br>\n앞에서 새로운 데이터에 대해서 추가를 했으니, 이 데이터를 얼마나 반영할지도 판단하는 것이다.<br>\n즉, 새로운 정보를 무비판적으로 수용하기보다는 적절히 취사선택하는 것이 이 게이트의 역할이다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134301434-0d113597-778b-4b45-b4d1-450b1566958c.png\" width=\"500\">  \n<p>Output 게이트, forget와 동일한 식을 계산한다.<br>\n이후 똑같이 아다마르 곱을 통해 기억 셀에 추가해준다.</p>\n<p>이상이 LSTM 계층 내에서 이뤄지는 처리이다.</p>\n<h2>LSTM의 계산그래프</h2>\n<img src=\"https://user-images.githubusercontent.com/42150335/134301543-75285f6b-5bfb-489b-b7a6-6432b39d1447.png\" width=\"600\">\n<p>앞에서까지 살펴본 LSTM의 계산그래프와 내부 연산은 위의 그림과 같다.<br>\nRNN에 비해 훨씬 복잡한 구조인 것을 볼 수 있다.</p>\n<p>그리고 위의 LSTM 계산그래프의 게이트를 구분해보면 다음과 같다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134301619-964918c6-bf4f-4c7b-8e77-384730bbcf7e.png\" width=\"600\">\n<p>위의 계산그래프를 이해하고 기억해둔다면,<br>\nLSTM에서 내부적으로 어떤 일이 일어나는지를 알 수 있을 것이다.</p>\n<h2>LSTM의 Gradient Flow</h2>\n<p>STM의 구조는 설명했지만, 이것이 어떤 원리로 Vanishing Gradient를 방지해주는 걸까?<br>\n그 원리는 기억 셀 c의 역전파에 주목하면 볼 수 있다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134301709-95fe8243-603b-403a-8a83-80c3b07cf10f.png\" width=\"600\">  \n<p>위의 그림은 기억 셀에만 집중하여, 그 역전파의 흐름을 그린 것이다.<br>\n이때 기억 셀의 역전파에서는 ’+‘와 ‘X’ 노드만을 지나게 된다.<br>\n’+’ 노드는 Gradient를 그대로 흘릴 뿐이므로 남는 것은 ‘X’ 노드이다.</p>\n<p>근데 여기서 중요한 점이 이 노드는 RNN과 같은 ‘행렬 곱’이 아닌 <strong>아다마르 곱</strong>을 계산한다.<br>\n그리고 RNN과 같이 똑같은 가중치 행렬을 사용하는게 아닌,<br>\n새로 들어온 데이터에 대해서 행렬곱을 수행하게 된다.</p>\n<p>그러므로 매번 새로운 값와 행렬 곱이 되므로 곱셈의 효과가 누적되지 않아<br>\nVanishing Gradient가 일어나기 어려운 구조인 것이다.</p>\n<p>또한 여기서 ‘X’ 노드에 주목해보자.<br>\n이 ‘X’ 노드의 계산은 <strong>forget 게이트</strong>가 제어한다.</p>\n<p>역전파 계산시 forget 게이트의 출력과 상류 gradient의 곱이 계산되므로,<br>\nforget 게이트가 ‘잊어야 한다’고 판단한 기억 셀의 원소에 대해서는<br>\n해당 기울기가 작아지고, ‘잊어서는 안 된다’고 판단한 원소에 대해서는<br>\n그 기울기가 약화되지 않은 채로 과거 방향으로 전해진다.<br>\n따라서 중요한 정보의 기울기는 소실 없이 전파된다.</p>\n<h2><strong>G</strong>ated <strong>R</strong>ecurrent <strong>U</strong>nit (GRU)</h2>\n<p>앞에서까지 LSTM에 대해 자세하게 설명했다.<br>\nLSTM은 아주 좋은 계층이지만, 매개변수가 많아서 계산이 오래 걸리는게 단점이다.</p>\n<p>그래서 최근에는 LSTM을 대신할 변형된 RNN이 많이 제안되고 있다.<br>\n그 중 유명하고 그 성능이 검증된 GRU<sup>Gated Recurrent Unit</sup>라는 RNN에 대해 알아보자.</p>\n<p>( 여담으로, 이 GRU는 우리나라 ‘조경현’ 박사님이 제안한 구조이다. )</p>\n<p>GRU는 LSTM의 게이트를 사용한다는 개념은 유지한 채, 매개변수를 줄여 계산시간을 줄여준다.<br>\nLSTM과 GRU의 인터페이스만 비교하더라도 둘의 차이점이 명확하게 드러난다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134301998-d0cfca1b-6c1e-49e1-ad28-654a7a3df27a.png\" width=\"600\">\n<p>그럼 GRU 내부에서 수행하는 계산을 살펴보자.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134302043-04439d8c-052b-49c0-8f07-846a5d024e05.png\" width=\"600\">  \n<p>GRU에서 수행하는 계산은 이 4개의 식으로 표현된다.<br>\n위의 식만 보더라도 6개였던 LSTM에 비해 간단해진 것을 확인할 수 있다.<br>\n그리고 GRU의 계산 그래프는 다음 슬라이드의 그림과 같다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134336742-a931fb8a-f8ca-4009-93a6-f6633ded89dc.png\" width=\"600\">  \n<p>앞의 LSTM에 비해 게이트의 수가 줄어든 것을 확인할 수 있다.<br>\n이처럼 GRU는 LSTM보다 계산 비용과 매개변수 수를 줄일 수 있다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134336822-3fb171a2-3286-4bfd-be87-52d0ba309adc.png\" width=\"600\">  \n<p>위의 그림처럼 GRU에는 기억 셀은 없고,<br>\n시간 방향으로 전파하는 것은 Hidden State인 h<sub>t</sub>뿐이다.</p>\n<p>그리고 r과 z라는 2개의 게이트를 사용한다.<br>\n여기서 r은 reset, z는 update 게이트이다.</p>\n<p>reset 게이트 r은 과거의 은닉 상태를 얼마나 ‘무시’할지를 결정한다.<br>\n만약 r이 0이면</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134336936-2caa1447-455c-4fdb-93f1-19600c73f887.png\" width=\"300\">  \n<p>위의 식으로부터, 새로운 Hidden State는 입력 x<sub>t</sub>만으로 결정된다.<br>\n과거의 Hidden State를 완전히 무시하는 것이다.</p>\n<p>한편, Update 게이트는 Hidden State를 갱신하는 게이트이다.<br>\nLSTM의 forget 게이트와 input 게이트의 2가지 역할을 혼자 담당하는 것이다.</p>\n<p>forget 게이트로써의 기능은 다음 식이 수행되는 부분이다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134337049-a9aac156-9e93-477f-a1d6-4634d50c195e.png\" width=\"200\">\n<p>과거의 Hidden State에서 잊어야 할 정보를 삭제한다.</p>\n<p>그리고 input 게이트로서의 기능은 다음 식이 수행되는 부분이다.</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/134337097-bf71b71b-d1e1-4cde-ba90-6d57ac8b33a0.png\" width=\"200\">  \n<p>새로 추가된 정보에 input 게이트의 가중치를 부여한다.</p>\n<h2>LSTM vs GRU</h2>\n<img src=\"https://user-images.githubusercontent.com/42150335/134337195-b609bb36-7d0d-44ee-9433-6331bde5be79.png\" width=\"600\">\n<p>LSTM과 GRU 중 어느 쪽을 사용해야 하는지를 묻는다면,<br>\n주어진 문제와 하이퍼파라미터 설정에 따라 승자가 달라진다고 대답할 수 있다.</p>\n<p>최근 연구에서는 LSTM이 많이 사용되지만, GRU도 꾸준히 인기를 끌고 있다.<br>\nGRU는 매개변수가 적고 계산량도 적기 때문에, 데이터셋이 작거나 모델 설계 시 반복 시도를 많이 해야 할 경우 특히 적합할 수 있다.</p>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"LSTM & GRU"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 포스팅을 이해가기 위해서는 아래 글에 대한 이해가 선행되는 것이 좋습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"sooftware.io/rnn"},"children":[{"type":"text","value":"RNN (Recurrent Neural Network)"}]}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"LSTM 등장 배경"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"RNN은 순환 경로를 포함하여 과거의 정보를 기억할 수 있었다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n구조가 단순하다는 장점이 있지만, 성능이 좋지 못하다는 단점도 존재한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이러한 단점의 원인은 많은 경우에 시계열 데이터에서 시간적으로 많이 떨어진"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n장기 의존 관계(Long Term)를 잘 학습할 수 없다는 데 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"여기서 “장기 의존 관계”가 무엇인지 짚고 넘어가자."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"className":["language-text"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"Sooft는 그의 방에서 TV를 보고 있었다. Ware는 그의 방으로 들어갔다. \n그리고 Ware는 ?에게 '안녕'이라고 인사를 했다."}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 예시를 보자. ?에 들어갈 단어는 ‘Sooft’이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n위의 문제에 올바르게 답하려면, 앞의 “Sooft는” 이라는 정보를 기억해둬야 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"하지만 기본적인 RNN의 구조에서는 이러한 장기 의존 관계에 취약하다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"RNN의 문제점"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그렇다면 왜 RNN은 이러한 장기 의존 관계에 대해서 약한 것일까?"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134299504-6704fb9d-2578-4c38-a5e1-0de5438bf546.png","width":500},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이 점에 대해서는 RNN의 Backpropagation을 살펴보면 알 수 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n위의 그림과 같이 RNN에서의 Backpropagation은 RNN 계층이 과거 방향으로"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n시간을 거슬러 가면서 gradient를 전달하게 된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"하지만 이러한 기울기는 RNN 계층이 길어지게 되면 기울기가 작아지게 된다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이를 Vanishing Gradient Problem(기울기 소실)이라고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134299528-6663aac1-8417-4d13-b4e8-f55751962ab0.png","width":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그럼 RNN 계층에서 왜 Vanishing Gradient이 일어나는 원인을 살펴보자."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134299677-12bf24c3-0833-4c88-85e1-5b8d4da2db66.png","width":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위는 RNN 셀의 구조이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 구조 중 ‘tanh’에만 주목해보자."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\ntanh와 dtanh의 그래프 모양은 다음과 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134299686-3c4d54ae-5571-4be9-a6d1-ded240259e7a.png","width":500},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"여기서 우리는 gradient에 관심이 있으므로, dtanh에 주목해보면"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\ndtanh 값은 항상 0~1 사이의 값인 것을 알 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이는 역전파에서 gradient가 tanh 노드를 지날 때마다 값이 계속 작아진다는 의미이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\ntanh를 T번 통과하게 되면 gradient도 T번 반복해서 작아지게 된다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그렇기 때문에 RNN 계층이 길어지게 되면 Vanishing Gradient Problem이 발생하게 된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"L"}]},{"type":"text","value":"ong "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"S"}]},{"type":"text","value":"hort "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"T"}]},{"type":"text","value":"erm "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"M"}]},{"type":"text","value":"emory (LSTM)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이제 이러한 Vanishing Gradient를 일으키지 않는다는 LSTM의 구조에 대해 살펴보자."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134299866-d4c1a2b3-b35a-47b6-bd16-1b921459fdc6.png","width":500},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위 그림은 RNN과 LSTM의 인터페이스를 비교한 그림이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"인터페이스만 보더라도 LSTM 계층에는 c라는 경로가 추가된 것을 알 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이 c를 기억 셀"},{"type":"element","tagName":"sup","properties":{},"children":[{"type":"text","value":"memory cell"}]},{"type":"text","value":"이라 하며, LSTM의 전용 기억 매커니즘이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"기억 셀의 특징은 데이터를 LSTM 계층 내에서만 주고 받고, 다른 계층으로는 출력하지 않는다는 것이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n즉, 이는 LSTM도 내부적으로만 다르지 사용하는 입장에서는 RNN과 같은 인터페이스를 갖는다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134299986-0311e778-edd4-4fd6-86b2-4093ecbacd7d.png","width":600},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이제 LSTM의 구조를 차분히 살펴보자."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n앞서 이야기한 것처럼, LSTM에는 기억 셀 c"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"text","value":"가 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이 c"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"text","value":"에는 시각 t에서의 LSTM의 메모리가 저장되어 있는데, 과거로부터 시각 t까지에 필요한"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n메모리가 저장되어 있다고 가정하자."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 필요한 정보를 모두 간직한 이 메모리를 바탕으로 외부 계층에 Hidden State h"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"text","value":"를 출력한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이때 출력하는 h"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"text","value":"는 다음 그림과 같이 기억 셀의 값은 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"tanh"}]},{"type":"text","value":" 함수로 변환한 값이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n(여기서의 tanh는 각 요소에 tanh 함수를 적용한다는 뜻이다.)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"여기서의 핵심은 3개의 입력 (x"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"text","value":", h"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t-1"}]},{"type":"text","value":", c"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t-1"}]},{"type":"text","value":")를 이용하여 구한 c"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"text","value":"를 사용해"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nHidden State h"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"text","value":"를 계산한다는 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"※ LSTM 구조의 핵심은 ht는 단기상태 (Short Term) ct는 장기 상태 (Long Term)라고 볼 수 있다. ※"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"게이트"},{"type":"element","tagName":"sup","properties":{},"children":[{"type":"text","value":"gate"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"여기서 뒷 내용을 이해하기 위해 게이트의 개념에 대해 이해하고 넘어가자."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n게이트는 데이터의 흐름을 제어한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134300359-a89a80a6-0d78-495f-b3c5-143818f45018.png","width":600},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 그림처럼 물의 흐름을 제어하는 것이 게이트의 역할이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 그림처럼 LSTM에서의 게이트는 ‘열기/닫기’ 뿐 아니라, 어느 정도 열지를 조절할 수 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그리고 이 열기 ~ 닫기 까지의 정도를 0.0 ~ 1.0의 실수로 표현할 수 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그리고 이 0.0 ~ 1.0의 값이 다음으로 넘어갈 데이터의 양을 결정한다 !!"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"여기서 중요한 점은 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"‘게이트를 얼마나 열지’라는 것도 데이터로부터 자동으로 학습된다는 점"}]},{"type":"text","value":"이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 여기서 0.0 ~ 1.0이라는 범위에 주목해보자."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n우리는 이러한 범위를 가지는 매우 좋은 함수를 알고있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134300507-0b0111d5-0917-4e12-8a41-919fc9fc6fad.png","width":600},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"바로 Sigmoid 함수이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Sigmoid 함수를 이용하면 어떤 값을 넣더라도 0.0 ~ 1.0의 값을 가질 수 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그렇기 때문에 LSTM의 게이트에서는 tanh가 아닌 Sigmoid 함수를 사용한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"LSTM의 Output 게이트"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"다시 LSTM 이야기로 돌아와보자."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n게이트 얘기를 하기 전에 Hidden State h"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"text","value":"는 기억 셀 c"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"text","value":"에 단순히 tanh 함수를 적용한것 뿐이라고 했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그럼 방금 배운 게이트의 개념을 tanh(c"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"text","value":")에 적용하는 것을 생각해보자."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n즉, tanh(c"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"text","value":")의 각 원소가 ‘다음 시각의 Hidden State에 얼마나 중요한가’를 조정해보자."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"output 게이트의 열림 상태의 계산식은 다음과 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134300707-e5302087-99e9-4985-a1a5-79c44ba4aff0.png","width":600},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 식을 보게되면 RNN 계층의 내부 계산에서 tanh가 아닌 Sigmoid를"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n사용했다는 점만 다르다는 것을 확인할 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134300788-173eac4b-0991-4a61-8c21-9795a9875f44.png","width":600},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"다음은 tanh(c"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"text","value":")에 Output게이트를 추가한 모습이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\noutput 게이트에서 수행하는 식은 σ로 표시했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 이 σ의 출력을 o라고 한다면, h"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"text","value":"는 다음과 같이 계산된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134300906-386cbac8-02a4-442b-a2c8-41bcc1d4182e.png","width":150},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"여기서의 ⊙는 Hardmard Product (아다마르 곱) 이라고 하여, 행렬의 원소별 곱셈을 의미한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134300914-cee72f4d-2a6d-4c68-a78e-c0d8c08740bc.png","width":600},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"LSTM의 forget 게이트"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134301001-e1c7f1e9-0060-4f76-ab07-5e7215e33114.png","width":600},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"다음으로 할 일은 잊을건 잊어버리는 것이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n굳이 필요없는 정보를 계속 들고갈 필요는 없다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그러므로 기억 셀에 ‘무엇을 잊을까’를 지시하는 것이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n여기에도 앞의 게이트 개념을 사용한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"전 층에서 넘어온 기억셀 c"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t-1"}]},{"type":"text","value":"에 대해서 불필요한 개념을 잊게 해주는"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n게이트를 forget 게이트라고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"여기서도 위의 Output 게이트와 마찬가지로 다음 식을 수행한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134301091-1e0c21d7-87cf-4e1b-9472-cd1d75ffb3c0.png","width":250},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Output게이트의 식과 동일한 것을 확인할 수 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n여기서도 마찬가지로, forget게이트의 출력인 f와 이전 기억 셀 c"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t-1"}]},{"type":"text","value":"의 아다마르 곱으로 계산한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"LSTM의 새로운 기억 셀"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134301235-a7d0a8f5-151c-4ed6-88a9-32057a50cd5a.png","width":600},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"forget 게이트를 거치면서 이전 시각의 기억 셀로부터 잊어야 할 기억이 삭제됐다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이제는 새로 들어온 데이터로부터 정보를 추가해야한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그러기 위해서 위의 그림과 같이 tanh 노드를 추가한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이 때, Sigmoid가 아닌 tanh 노드를 추가하는 이유는 이 ‘새로운 기억 셀’은 게이트가 아닌,"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n새로운 데이터를 기억 셀에 추가하는 것이기 때문이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134301294-71bbd02a-39fe-4beb-8661-85b9f985c0cf.png","width":400},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이제 이렇게해서 잊는 것 뿐만이 아닌, 새로운 정보까지 추가가 되었다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"LSTM의 input 게이트"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134301396-8a91d2ad-6775-450f-acc5-10a1b86709cc.png","width":600},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"마지막으로 새로운 정보가 들어있는 g에 게이트를 하나 추가할 생각이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n앞에서 새로운 데이터에 대해서 추가를 했으니, 이 데이터를 얼마나 반영할지도 판단하는 것이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n즉, 새로운 정보를 무비판적으로 수용하기보다는 적절히 취사선택하는 것이 이 게이트의 역할이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134301434-0d113597-778b-4b45-b4d1-450b1566958c.png","width":500},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Output 게이트, forget와 동일한 식을 계산한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이후 똑같이 아다마르 곱을 통해 기억 셀에 추가해준다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이상이 LSTM 계층 내에서 이뤄지는 처리이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"LSTM의 계산그래프"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134301543-75285f6b-5bfb-489b-b7a6-6432b39d1447.png","width":600},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"앞에서까지 살펴본 LSTM의 계산그래프와 내부 연산은 위의 그림과 같다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nRNN에 비해 훨씬 복잡한 구조인 것을 볼 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 위의 LSTM 계산그래프의 게이트를 구분해보면 다음과 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134301619-964918c6-bf4f-4c7b-8e77-384730bbcf7e.png","width":600},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 계산그래프를 이해하고 기억해둔다면,"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nLSTM에서 내부적으로 어떤 일이 일어나는지를 알 수 있을 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"LSTM의 Gradient Flow"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"STM의 구조는 설명했지만, 이것이 어떤 원리로 Vanishing Gradient를 방지해주는 걸까?"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그 원리는 기억 셀 c의 역전파에 주목하면 볼 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134301709-95fe8243-603b-403a-8a83-80c3b07cf10f.png","width":600},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 그림은 기억 셀에만 집중하여, 그 역전파의 흐름을 그린 것이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이때 기억 셀의 역전파에서는 ’+‘와 ‘X’ 노드만을 지나게 된다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n’+’ 노드는 Gradient를 그대로 흘릴 뿐이므로 남는 것은 ‘X’ 노드이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"근데 여기서 중요한 점이 이 노드는 RNN과 같은 ‘행렬 곱’이 아닌 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"아다마르 곱"}]},{"type":"text","value":"을 계산한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그리고 RNN과 같이 똑같은 가중치 행렬을 사용하는게 아닌,"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n새로 들어온 데이터에 대해서 행렬곱을 수행하게 된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그러므로 매번 새로운 값와 행렬 곱이 되므로 곱셈의 효과가 누적되지 않아"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nVanishing Gradient가 일어나기 어려운 구조인 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"또한 여기서 ‘X’ 노드에 주목해보자."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이 ‘X’ 노드의 계산은 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"forget 게이트"}]},{"type":"text","value":"가 제어한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"역전파 계산시 forget 게이트의 출력과 상류 gradient의 곱이 계산되므로,"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nforget 게이트가 ‘잊어야 한다’고 판단한 기억 셀의 원소에 대해서는"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n해당 기울기가 작아지고, ‘잊어서는 안 된다’고 판단한 원소에 대해서는"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그 기울기가 약화되지 않은 채로 과거 방향으로 전해진다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n따라서 중요한 정보의 기울기는 소실 없이 전파된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"G"}]},{"type":"text","value":"ated "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"R"}]},{"type":"text","value":"ecurrent "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"U"}]},{"type":"text","value":"nit (GRU)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"앞에서까지 LSTM에 대해 자세하게 설명했다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nLSTM은 아주 좋은 계층이지만, 매개변수가 많아서 계산이 오래 걸리는게 단점이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그래서 최근에는 LSTM을 대신할 변형된 RNN이 많이 제안되고 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그 중 유명하고 그 성능이 검증된 GRU"},{"type":"element","tagName":"sup","properties":{},"children":[{"type":"text","value":"Gated Recurrent Unit"}]},{"type":"text","value":"라는 RNN에 대해 알아보자."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"( 여담으로, 이 GRU는 우리나라 ‘조경현’ 박사님이 제안한 구조이다. )"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"GRU는 LSTM의 게이트를 사용한다는 개념은 유지한 채, 매개변수를 줄여 계산시간을 줄여준다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nLSTM과 GRU의 인터페이스만 비교하더라도 둘의 차이점이 명확하게 드러난다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134301998-d0cfca1b-6c1e-49e1-ad28-654a7a3df27a.png","width":600},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그럼 GRU 내부에서 수행하는 계산을 살펴보자."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134302043-04439d8c-052b-49c0-8f07-846a5d024e05.png","width":600},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"GRU에서 수행하는 계산은 이 4개의 식으로 표현된다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n위의 식만 보더라도 6개였던 LSTM에 비해 간단해진 것을 확인할 수 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그리고 GRU의 계산 그래프는 다음 슬라이드의 그림과 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134336742-a931fb8a-f8ca-4009-93a6-f6633ded89dc.png","width":600},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"앞의 LSTM에 비해 게이트의 수가 줄어든 것을 확인할 수 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이처럼 GRU는 LSTM보다 계산 비용과 매개변수 수를 줄일 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134336822-3fb171a2-3286-4bfd-be87-52d0ba309adc.png","width":600},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 그림처럼 GRU에는 기억 셀은 없고,"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n시간 방향으로 전파하는 것은 Hidden State인 h"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"text","value":"뿐이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 r과 z라는 2개의 게이트를 사용한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n여기서 r은 reset, z는 update 게이트이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"reset 게이트 r은 과거의 은닉 상태를 얼마나 ‘무시’할지를 결정한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n만약 r이 0이면"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134336936-2caa1447-455c-4fdb-93f1-19600c73f887.png","width":300},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 식으로부터, 새로운 Hidden State는 입력 x"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"text","value":"만으로 결정된다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n과거의 Hidden State를 완전히 무시하는 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"한편, Update 게이트는 Hidden State를 갱신하는 게이트이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nLSTM의 forget 게이트와 input 게이트의 2가지 역할을 혼자 담당하는 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"forget 게이트로써의 기능은 다음 식이 수행되는 부분이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134337049-a9aac156-9e93-477f-a1d6-4634d50c195e.png","width":200},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"과거의 Hidden State에서 잊어야 할 정보를 삭제한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 input 게이트로서의 기능은 다음 식이 수행되는 부분이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134337097-bf71b71b-d1e1-4cde-ba90-6d57ac8b33a0.png","width":200},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"새로 추가된 정보에 input 게이트의 가중치를 부여한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"LSTM vs GRU"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/134337195-b609bb36-7d0d-44ee-9433-6331bde5be79.png","width":600},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"LSTM과 GRU 중 어느 쪽을 사용해야 하는지를 묻는다면,"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n주어진 문제와 하이퍼파라미터 설정에 따라 승자가 달라진다고 대답할 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"최근 연구에서는 LSTM이 많이 사용되지만, GRU도 꾸준히 인기를 끌고 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nGRU는 매개변수가 적고 계산량도 적기 때문에, 데이터셋이 작거나 모델 설계 시 반복 시도를 많이 해야 할 경우 특히 적합할 수 있다."}]}],"data":{"quirksMode":false}},"excerpt":"LSTM & GRU 본 포스팅을 이해가기 위해서는 아래 글에 대한 이해가 선행되는 것이 좋습니다. RNN (Recurrent Neural Network) LSTM 등장 배경 RNN…","fields":{"readingTime":{"text":"18 min read"}},"frontmatter":{"title":"LSTM & GRU","userDate":"24 January 2020","date":"2020-01-24T10:00:00.000Z","tags":["nlp"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/385b4880d470b853d530d77243bb4c52/b3525/lstm_gru.png","srcSet":"/static/385b4880d470b853d530d77243bb4c52/2107c/lstm_gru.png 750w,\n/static/385b4880d470b853d530d77243bb4c52/b3525/lstm_gru.png 773w","sizes":"100vw"},"sources":[{"srcSet":"/static/385b4880d470b853d530d77243bb4c52/28c13/lstm_gru.webp 750w,\n/static/385b4880d470b853d530d77243bb4c52/b6f78/lstm_gru.webp 773w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.31565329883570503}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}]}}]}},"relatedPosts":{"totalCount":30,"edges":[{"node":{"id":"06ac0e32-0688-50f0-810d-134ef8b168ab","excerpt":"Decoding Strategy (디코딩 전략) 이번 포스팅에서는 자연어처리 모델의 디코딩 전략에 관해서 다뤄보려고 합니다. 디코딩이란 말처럼 디코딩은 디코더에서\n수행하는 작업입니다. 즉, BERT와 같은 인코더 모델에서 사용하는게 아니라 GPT…","frontmatter":{"title":"Decoding Strategy (디코딩 전략)","date":"2022-01-15T10:00:00.000Z"},"fields":{"readingTime":{"text":"9 min read"},"slug":"/generate/"}}},{"node":{"id":"db36f120-4fb0-5bf7-af53-16447fe6cdd4","excerpt":"Generation with Retrieval 이번에 딥마인드에서 RETRO(Retrieval-Enhanced Transformer) 라는 모델을 내놓았습니다. 문서 retrieval + GPT 기반 모델인데,\n7B 모델임에도 불구하고 2…","frontmatter":{"title":"Generation with Retrieval","date":"2022-01-04T23:00:00.000Z"},"fields":{"readingTime":{"text":"6 min read"},"slug":"/fid_and_rag/"}}},{"node":{"id":"3b4040eb-d53d-5064-beec-cfbf7a7a0fe2","excerpt":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems Paper Review Paper: https://aclanthology.org/2021.naacl-main.12…","frontmatter":{"title":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems Paper Review","date":"2021-12-18T10:00:00.000Z"},"fields":{"readingTime":{"text":"2 min read"},"slug":"/bert_fp/"}}},{"node":{"id":"78976688-33d9-53c4-8489-5099082b9972","excerpt":"GPT (Generative Pre-trained Transformer) 1 gpt1 먼저 알아보고, gpt2에 대해 알아보겠습니다. GPT1 Improving Language Understanding by Generative Pre-Training…","frontmatter":{"title":"GPT (Generative Pre-trained Transformer)","date":"2021-11-23T11:00:00.000Z"},"fields":{"readingTime":{"text":"13 min read"},"slug":"/gpt/"}}},{"node":{"id":"ad5b0c9b-8199-5f10-bfc9-6bb05942e164","excerpt":"Large Scale LM (2) Distributed Programming (작성중) 이 자료는 [해당 link…","frontmatter":{"title":"Large Scale LM (2) Distributed Programming","date":"2021-11-22T11:00:00.000Z"},"fields":{"readingTime":{"text":"17 min read"},"slug":"/big-model2/"}}}]}},"pageContext":{"slug":"/lstm_gru/","prev":{"excerpt":"Attention-Based Models for Speech Recognition Paper Review title http://papers.nips.cc/paper/5847-attention-based-models-for-speech…","frontmatter":{"title":"Attention-Based Models for Speech Recognition Paper Review","tags":["speech","paper"],"date":"2020-01-20T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABgElEQVQoz1VSi3KCMBDk/7+rymgNWuRhKRIQUOhMGR0fECBu507BNjMhl8uy2b2LEQQBVqsVPM+D67o8KXYcZ4zHvePAc18494nxfZ85oiiCoZRCHMeYz2cQQuDDtjGfzZhkubRgmibeFwu4rgN7vYZlCf6ZsGvH4dg0p4ikRNu2MADgeDwijiUT5/keUkpkWYaiOHCc7HYoyxKHokAYhkjTFFmWcn673SKKtvipKqKCcbvdcL1eQUrphr7vcb/fobXmPeWbpkFd17z+PR/WrutAPDRZIQ06VKpBnmeQsWRl5/MZ+q4HCJOdTieE4ReiSGKz8bFLUzRKMTErpA8pqaqKLRP4MwhAzSJSskkKhtIURcGWiShJEt6X3yW7HAmJneQq1bItUjsMsny5XHh9ZUdf/7AUMyGxU/fstY2FEPB9D4Rr6pqfhRALWMslPw+6tOtajqfmFG+TCYRl4VbXL4VUG1KpdT8WWetHwYcm9M8mUZ4I8/2erT66f0DdNEz4C5zGoWnk38kqAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/1402657e6eeeaf54425d3d5cf34998c8/0f4e3/loc-attention.png","srcSet":"/static/1402657e6eeeaf54425d3d5cf34998c8/0f4e3/loc-attention.png 681w","sizes":"100vw"},"sources":[{"srcSet":"/static/1402657e6eeeaf54425d3d5cf34998c8/c0309/loc-attention.webp 681w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.4552129221732746}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"11 min read"},"layout":"","slug":"/loc-attention/"}},"next":{"excerpt":"Seq2seq (Sequence to sequence) 본 포스팅을 이해하기 위해서는 다음 글에 대한 이해가 선행되는 것이 좋습니다. RNN (Recurrent Neural Network) LSTM & GRU (Long Short Term Memory…","frontmatter":{"title":"Seq2seq (Sequence to sequence)","tags":["nlp"],"date":"2020-01-25T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAACYElEQVQoz6WTy0tUcRTHf/0BPZZB64iiXbQIXVUSLtxItgkqKmgTGElFFmWhiKGUEmkRKVhCgtEDF0oUmDO+8K3N3NFxnDvO6851nGacuXfmPj4xGmO1CKQvfDic7+LAOXyPsG2b7ZCXrmdZWvKxuOhFVaJEQiv4ZRlZlhEUZP/Fv5XL6ViGRiyVwaMkwcqSM7KInGEQXzeI6/zJukk8sc5qIo2eSmNnNLRUeqPP+6mUiaXD94DB5/kMZOFHMoeQoynmnV8JfmrE/6EJ+WMT8vtHyFOD+CNJgiGVl9MKt74F6ZqLEQ6ryKEE474xel1d9End9Etv6fN0M+aeRciqTrKtDKoE3N8NNXvgmoCuisJ6JxwZRO0g52a3TtEhX6J67gD1UjH17mLuuvbT421AyDENteUU1O7DbC/HfFWO9WAv6WclBGMJYsEARX1RRN0AZwbjZNQIfjnEk7nTNLhO8sZXRedSJXWuIl67ahCBWIb401JoPoLVcgy79Tg0HkZrLSGsJkgoYYr7FcTDL1Q41siuRpFXQjyXLtDmPcuLxfO0L12hebGMLncNYlnRiHZcxr6xg2z1TnJ3dpG9LjDfXS2sV+rMIB5PcnFCL3idvkpuThzk3vTRDW7PHKLH3YwIKkmmZjx4J0eQJkaQxkfwjA/h8fiQlmMs+CIMuCP0ulUcUmSj9/gUZrwLOKRhhqRRnNIow55RJlze33MIMyo4Qxb/I5EPv2maGDmdtWSaWCJFTs9gGsZm3G0by7Yxf9Wtr7GwbBNro26S98XmK+moqooSjRIOBQkEAmiaVhi4HX4CuGdQpVTBy/kAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/3f604bd7e174e888d547ccba99e0fccb/a6e9f/seq2seq.png","srcSet":"/static/3f604bd7e174e888d547ccba99e0fccb/4a1e8/seq2seq.png 750w,\n/static/3f604bd7e174e888d547ccba99e0fccb/a6e9f/seq2seq.png 773w","sizes":"100vw"},"sources":[{"srcSet":"/static/3f604bd7e174e888d547ccba99e0fccb/f9860/seq2seq.webp 750w,\n/static/3f604bd7e174e888d547ccba99e0fccb/c53c5/seq2seq.webp 773w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5808538163001293}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder/A.I. engineer at TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"10 min read"},"layout":"","slug":"/seq2seq/"}},"primaryTag":"nlp"}},
    "staticQueryHashes": ["3170763342","3229353822"]}